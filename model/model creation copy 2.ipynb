{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f36a6052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30fc110a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>...</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "      <th>Airport_fee</th>\n",
       "      <th>cbd_congestion_fee</th>\n",
       "      <th>source_file</th>\n",
       "      <th>taxi_type</th>\n",
       "      <th>trip_duration_minutes</th>\n",
       "      <th>pickup_hour</th>\n",
       "      <th>pickup_day</th>\n",
       "      <th>pickup_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-01-18 20:53:30</td>\n",
       "      <td>2025-01-18 21:00:47</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.97</td>\n",
       "      <td>238</td>\n",
       "      <td>166</td>\n",
       "      <td>1</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>13.32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>yellow_tripdata_2025-01.parquet</td>\n",
       "      <td>yellow</td>\n",
       "      <td>7.283333</td>\n",
       "      <td>20</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-01-25 11:12:51</td>\n",
       "      <td>2025-01-25 11:17:57</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>50</td>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "      <td>5.8</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>10.55</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>yellow_tripdata_2025-01.parquet</td>\n",
       "      <td>yellow</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>11</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-01-21 15:09:31</td>\n",
       "      <td>2025-01-21 15:19:02</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>236</td>\n",
       "      <td>237</td>\n",
       "      <td>1</td>\n",
       "      <td>9.3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>16.65</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>yellow_tripdata_2025-01.parquet</td>\n",
       "      <td>yellow</td>\n",
       "      <td>9.516667</td>\n",
       "      <td>15</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-01-04 23:37:07</td>\n",
       "      <td>2025-01-04 23:45:58</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.44</td>\n",
       "      <td>137</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>26.3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>35.12</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>yellow_tripdata_2025-01.parquet</td>\n",
       "      <td>yellow</td>\n",
       "      <td>8.850000</td>\n",
       "      <td>23</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-01-29 00:31:22</td>\n",
       "      <td>2025-01-29 00:38:27</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.10</td>\n",
       "      <td>140</td>\n",
       "      <td>263</td>\n",
       "      <td>1</td>\n",
       "      <td>7.9</td>\n",
       "      <td>3.50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>15.48</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>yellow_tripdata_2025-01.parquet</td>\n",
       "      <td>yellow</td>\n",
       "      <td>7.083333</td>\n",
       "      <td>0</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       pickup_datetime     dropoff_datetime  passenger_count  trip_distance  \\\n",
       "0  2025-01-18 20:53:30  2025-01-18 21:00:47              1.0           0.97   \n",
       "1  2025-01-25 11:12:51  2025-01-25 11:17:57              1.0           0.60   \n",
       "2  2025-01-21 15:09:31  2025-01-21 15:19:02              1.0           0.80   \n",
       "3  2025-01-04 23:37:07  2025-01-04 23:45:58              1.0           4.44   \n",
       "4  2025-01-29 00:31:22  2025-01-29 00:38:27              1.0           1.10   \n",
       "\n",
       "   PULocationID  DOLocationID  payment_type  fare_amount  extra  mta_tax  ...  \\\n",
       "0           238           166             1          8.6   1.00      0.5  ...   \n",
       "1            50            48             2          5.8   3.25      0.5  ...   \n",
       "2           236           237             1          9.3   2.50      0.5  ...   \n",
       "3           137            88             0         26.3   0.00      0.5  ...   \n",
       "4           140           263             1          7.9   3.50      0.5  ...   \n",
       "\n",
       "   total_amount  congestion_surcharge  Airport_fee  cbd_congestion_fee  \\\n",
       "0         13.32                   0.0          0.0                0.00   \n",
       "1         10.55                   2.5          0.0                0.75   \n",
       "2         16.65                   2.5          0.0                0.00   \n",
       "3         35.12                   2.5          0.0                0.00   \n",
       "4         15.48                   2.5          0.0                0.00   \n",
       "\n",
       "                       source_file  taxi_type trip_duration_minutes  \\\n",
       "0  yellow_tripdata_2025-01.parquet     yellow              7.283333   \n",
       "1  yellow_tripdata_2025-01.parquet     yellow              5.100000   \n",
       "2  yellow_tripdata_2025-01.parquet     yellow              9.516667   \n",
       "3  yellow_tripdata_2025-01.parquet     yellow              8.850000   \n",
       "4  yellow_tripdata_2025-01.parquet     yellow              7.083333   \n",
       "\n",
       "  pickup_hour  pickup_day  pickup_month  \n",
       "0          20    Saturday             1  \n",
       "1          11    Saturday             1  \n",
       "2          15     Tuesday             1  \n",
       "3          23    Saturday             1  \n",
       "4           0   Wednesday             1  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('./cleaned_data/cleaned_yellow_taxi.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd6255d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['pickup_datetime', 'dropoff_datetime', 'passenger_count',\n",
      "       'trip_distance', 'PULocationID', 'DOLocationID', 'payment_type',\n",
      "       'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount',\n",
      "       'total_amount', 'congestion_surcharge', 'Airport_fee',\n",
      "       'cbd_congestion_fee', 'source_file', 'taxi_type',\n",
      "       'trip_duration_minutes', 'pickup_hour', 'pickup_day', 'pickup_month'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc91fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns in dataset:\n",
      "['pickup_datetime', 'dropoff_datetime', 'passenger_count', 'trip_distance', 'PULocationID', 'DOLocationID', 'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'total_amount', 'congestion_surcharge', 'Airport_fee', 'cbd_congestion_fee', 'source_file', 'taxi_type', 'trip_duration_minutes', 'pickup_hour', 'pickup_day', 'pickup_month']\n",
      "\n",
      "Selected feature columns (17):\n",
      " 1. PULocationID\n",
      " 2. DOLocationID\n",
      " 3. passenger_count\n",
      " 4. trip_distance\n",
      " 5. extra\n",
      " 6. mta_tax\n",
      " 7. tip_amount\n",
      " 8. tolls_amount\n",
      " 9. total_amount\n",
      "10. payment_type\n",
      "11. congestion_surcharge\n",
      "12. Airport_fee\n",
      "13. cbd_congestion_fee\n",
      "14. trip_duration_minutes\n",
      "15. pickup_hour\n",
      "16. pickup_day\n",
      "17. pickup_month\n",
      "\n",
      "✅ All feature columns found!\n",
      "\n",
      "Feature matrix shape: (349747, 17)\n",
      "Target vector shape: (349747,)\n",
      "Number of features: 17\n"
     ]
    }
   ],
   "source": [
    "# Select features that actually exist in the dataset\n",
    "print(\"Available columns in dataset:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Select appropriate features for the model (excluding target and non-predictive columns)\n",
    "# Removed PULocationID, DOLocationID, and total_amount for simplified UI-based prediction\n",
    "feature_columns = [\n",
    "    'passenger_count', 'trip_distance',\n",
    "    'extra', 'mta_tax', 'tip_amount', 'tolls_amount',\n",
    "    'payment_type', 'congestion_surcharge', 'Airport_fee', 'cbd_congestion_fee',\n",
    "    'trip_duration_minutes', 'pickup_hour', 'pickup_day', 'pickup_month'\n",
    "]\n",
    "\n",
    "# Note: We're excluding 'pickup_datetime', 'dropoff_datetime', 'source_file', 'taxi_type'\n",
    "# as they are either non-predictive or already converted to useful features\n",
    "# Also excluding 'PULocationID', 'DOLocationID', 'total_amount' for simplified UI prediction\n",
    "\n",
    "print(f\"\\nSelected feature columns ({len(feature_columns)}):\")\n",
    "for i, col in enumerate(feature_columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")\n",
    "\n",
    "# Verify all columns exist\n",
    "missing_cols = [col for col in feature_columns if col not in df.columns]\n",
    "if missing_cols:\n",
    "    print(f\"\\n❌ Missing columns: {missing_cols}\")\n",
    "else:\n",
    "    print(f\"\\n✅ All feature columns found!\")\n",
    "\n",
    "X = df[feature_columns]\n",
    "y = df['fare_amount']\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")\n",
    "print(f\"Number of features: {len(feature_columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9a568e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA PREPARATION ===\n",
      "X shape: (349747, 17)\n",
      "y shape: (349747,)\n",
      "X columns: ['PULocationID', 'DOLocationID', 'passenger_count', 'trip_distance', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'total_amount', 'payment_type', 'congestion_surcharge', 'Airport_fee', 'cbd_congestion_fee', 'trip_duration_minutes', 'pickup_hour', 'pickup_day', 'pickup_month']\n",
      "Target variable: fare_amount\n",
      "\n",
      "Missing values in X: 0\n",
      "Missing values in y: 0\n",
      "\n",
      "Target statistics:\n",
      "y mean: 12.68\n",
      "y std: 6.96\n",
      "y min: -6.70\n",
      "y max: 32.40\n"
     ]
    }
   ],
   "source": [
    "# Check data shapes and prepare for PyTorch\n",
    "print(\"=== DATA PREPARATION ===\")\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"X columns: {list(X.columns)}\")\n",
    "print(f\"Target variable: fare_amount\")\n",
    "\n",
    "# Check for any missing values\n",
    "print(f\"\\nMissing values in X: {X.isnull().sum().sum()}\")\n",
    "print(f\"Missing values in y: {y.isnull().sum()}\")\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\nTarget statistics:\")\n",
    "print(f\"y mean: {y.mean():.2f}\")\n",
    "print(f\"y std: {y.std():.2f}\")\n",
    "print(f\"y min: {y.min():.2f}\")\n",
    "print(f\"y max: {y.max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b29111",
   "metadata": {},
   "source": [
    "### Custom Dataset Class for Taxi Fare Prediction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06102e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SPLITTING DATA ===\n",
      "Training set: 279797 samples\n",
      "Test set: 69950 samples\n",
      "\n",
      "Missing values check:\n",
      "X_train missing: 0\n",
      "X_test missing: 0\n",
      "y_train missing: 0\n",
      "y_test missing: 0\n",
      "Dataset created with 279797 samples\n",
      "Feature shape: torch.Size([279797, 17])\n",
      "Target shape: torch.Size([279797])\n",
      "Feature statistics:\n",
      "  Mean: tensor([-1.7533e-08,  6.7351e-09,  0.0000e+00, -9.7618e-09,  6.2825e-08])...\n",
      "  Std: tensor([1.0000, 1.0000, 0.0000, 1.0000, 1.0000])...\n",
      "Dataset created with 69950 samples\n",
      "Feature shape: torch.Size([69950, 17])\n",
      "Target shape: torch.Size([69950])\n",
      "Feature statistics:\n",
      "  Mean: tensor([-1.2870e-08,  1.9305e-08,  0.0000e+00, -1.3634e-08,  2.9803e-08])...\n",
      "  Std: tensor([1.0000, 1.0000, 0.0000, 1.0000, 1.0000])...\n",
      "\n",
      "DataLoaders created:\n",
      "Train batches: 4372\n",
      "Test batches: 1093\n",
      "Batch size: 64\n",
      "Input feature size: 17\n",
      "Dataset created with 279797 samples\n",
      "Feature shape: torch.Size([279797, 17])\n",
      "Target shape: torch.Size([279797])\n",
      "Feature statistics:\n",
      "  Mean: tensor([-1.7533e-08,  6.7351e-09,  0.0000e+00, -9.7618e-09,  6.2825e-08])...\n",
      "  Std: tensor([1.0000, 1.0000, 0.0000, 1.0000, 1.0000])...\n",
      "Dataset created with 69950 samples\n",
      "Feature shape: torch.Size([69950, 17])\n",
      "Target shape: torch.Size([69950])\n",
      "Feature statistics:\n",
      "  Mean: tensor([-1.2870e-08,  1.9305e-08,  0.0000e+00, -1.3634e-08,  2.9803e-08])...\n",
      "  Std: tensor([1.0000, 1.0000, 0.0000, 1.0000, 1.0000])...\n",
      "\n",
      "DataLoaders created:\n",
      "Train batches: 4372\n",
      "Test batches: 1093\n",
      "Batch size: 64\n",
      "Input feature size: 17\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class TaxiFareDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset class for taxi fare prediction\n",
    "    \"\"\"\n",
    "    def __init__(self, features, targets, transform=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            features (pandas.DataFrame): Input features (X)\n",
    "            targets (pandas.Series): Target values (y)\n",
    "            transform (bool): Whether to apply standardization\n",
    "        \"\"\"\n",
    "        self.features = features.copy()\n",
    "        self.targets = targets.copy()\n",
    "        \n",
    "        # Handle categorical columns (convert to numeric if needed)\n",
    "        if 'pickup_day' in self.features.columns:\n",
    "            # Convert day names to numbers if they're strings\n",
    "            if self.features['pickup_day'].dtype == 'object':\n",
    "                day_mapping = {\n",
    "                    'Monday': 0, 'Tuesday': 1, 'Wednesday': 2, 'Thursday': 3,\n",
    "                    'Friday': 4, 'Saturday': 5, 'Sunday': 6\n",
    "                }\n",
    "                self.features['pickup_day'] = self.features['pickup_day'].map(day_mapping)\n",
    "        \n",
    "        # Handle any remaining categorical columns\n",
    "        for col in self.features.columns:\n",
    "            if self.features[col].dtype == 'object':\n",
    "                print(f\"Converting categorical column {col} to numeric\")\n",
    "                self.features[col] = pd.Categorical(self.features[col]).codes\n",
    "        \n",
    "        # Convert to numpy arrays\n",
    "        self.features = self.features.values.astype(np.float32)\n",
    "        self.targets = self.targets.values.astype(np.float32)\n",
    "        \n",
    "        # Apply standardization if requested\n",
    "        if transform:\n",
    "            self.scaler = StandardScaler()\n",
    "            self.features = self.scaler.fit_transform(self.features)\n",
    "        else:\n",
    "            self.scaler = None\n",
    "        \n",
    "        # Convert to tensors\n",
    "        self.x_data = torch.tensor(self.features, dtype=torch.float32)\n",
    "        self.y_data = torch.tensor(self.targets, dtype=torch.float32)\n",
    "        \n",
    "        print(f\"Dataset created with {len(self)} samples\")\n",
    "        print(f\"Feature shape: {self.x_data.shape}\")\n",
    "        print(f\"Target shape: {self.y_data.shape}\")\n",
    "        print(f\"Feature statistics:\")\n",
    "        print(f\"  Mean: {self.x_data.mean(dim=0)[:5]}...\")  # Show first 5 features\n",
    "        print(f\"  Std: {self.x_data.std(dim=0)[:5]}...\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_data[idx], self.y_data[idx]\n",
    "\n",
    "# Create train-test split\n",
    "print(\"=== SPLITTING DATA ===\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Check for any missing values before creating datasets\n",
    "print(f\"\\nMissing values check:\")\n",
    "print(f\"X_train missing: {X_train.isnull().sum().sum()}\")\n",
    "print(f\"X_test missing: {X_test.isnull().sum().sum()}\")\n",
    "print(f\"y_train missing: {y_train.isnull().sum()}\")\n",
    "print(f\"y_test missing: {y_test.isnull().sum()}\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TaxiFareDataset(X_train, y_train, transform=True)\n",
    "test_dataset = TaxiFareDataset(X_test, y_test, transform=True)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"\\nDataLoaders created:\")\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Input feature size: {train_dataset.x_data.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f164cf",
   "metadata": {},
   "source": [
    "### Neural Network Model for Taxi Fare Prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed3e0c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MODEL ARCHITECTURE ===\n",
      "Input size: 17 features\n",
      "Model: TaxiFareModel(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=17, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): Dropout(p=0.2, inplace=False)\n",
      "    (8): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (9): ReLU()\n",
      "    (10): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): Dropout(p=0.2, inplace=False)\n",
      "    (12): Linear(in_features=32, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Total parameters: 13121\n",
      "\n",
      "Optimizer: Adam\n",
      "Loss function: MSE\n",
      "Learning rate: 0.001\n"
     ]
    }
   ],
   "source": [
    "class TaxiFareModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep Neural Network for taxi fare prediction\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_sizes=[128, 64, 32], dropout_rate=0.2):\n",
    "        super(TaxiFareModel, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        # Build hidden layers\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_size, hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(hidden_size),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        # Output layer (single neuron for regression)\n",
    "        layers.append(nn.Linear(prev_size, 1))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x).squeeze()\n",
    "\n",
    "# Initialize model\n",
    "input_size = X.shape[1]  # Number of features\n",
    "model = TaxiFareModel(input_size=input_size)\n",
    "\n",
    "print(f\"=== MODEL ARCHITECTURE ===\")\n",
    "print(f\"Input size: {input_size} features\")\n",
    "print(f\"Model: {model}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=10)\n",
    "\n",
    "print(f\"\\nOptimizer: Adam\")\n",
    "print(f\"Loss function: MSE\")\n",
    "print(f\"Learning rate: 0.001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae2fb83",
   "metadata": {},
   "source": [
    "### Training and Evaluation Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7adeb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluation functions defined!\n",
      "Ready to start training the model...\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, device='cpu'):\n",
    "    \"\"\"Train the model for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        # Print progress every 100 batches\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
    "    \n",
    "    return total_loss / num_batches\n",
    "\n",
    "def evaluate_model(model, test_loader, criterion, device='cpu'):\n",
    "    \"\"\"Evaluate the model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, targets in test_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            predictions.extend(outputs.cpu().numpy())\n",
    "            actuals.extend(targets.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    predictions = np.array(predictions)\n",
    "    actuals = np.array(actuals)\n",
    "    \n",
    "    mse = np.mean((predictions - actuals) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.mean(np.abs(predictions - actuals))\n",
    "    \n",
    "    # R² score\n",
    "    ss_res = np.sum((actuals - predictions) ** 2)\n",
    "    ss_tot = np.sum((actuals - np.mean(actuals)) ** 2)\n",
    "    r2 = 1 - (ss_res / ss_tot)\n",
    "    \n",
    "    return avg_loss, mse, rmse, mae, r2, predictions, actuals\n",
    "\n",
    "print(\"Training and evaluation functions defined!\")\n",
    "print(\"Ready to start training the model...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "574807aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training function ready!\n",
      "Call train_taxi_fare_model() to start training\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "def train_taxi_fare_model(num_epochs=50):\n",
    "    \"\"\"Main training loop\"\"\"\n",
    "    print(\"=== STARTING TRAINING ===\")\n",
    "    \n",
    "    # Track training history\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_rmses = []\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    patience = 15\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Train\n",
    "        train_loss = train_model(model, train_loader, criterion, optimizer)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Evaluate\n",
    "        val_loss, mse, rmse, mae, r2, _, _ = evaluate_model(model, test_loader, criterion)\n",
    "        val_losses.append(val_loss)\n",
    "        val_rmses.append(rmse)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            # Save best model\n",
    "            torch.save(model.state_dict(), './best_models/best_taxi_fare_model_copy.pth')\n",
    "            print(\"✓ New best model saved!\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(val_rmses, label='Validation RMSE', color='orange')\n",
    "    plt.title('Validation RMSE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    # Final evaluation\n",
    "    final_loss, final_mse, final_rmse, final_mae, final_r2, predictions, actuals = evaluate_model(model, test_loader, criterion)\n",
    "    \n",
    "    plt.scatter(actuals, predictions, alpha=0.5)\n",
    "    plt.plot([actuals.min(), actuals.max()], [actuals.min(), actuals.max()], 'r--', lw=2)\n",
    "    plt.xlabel('Actual Fare')\n",
    "    plt.ylabel('Predicted Fare')\n",
    "    plt.title(f'Predictions vs Actuals\\nR² = {final_r2:.4f}')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n=== FINAL RESULTS ===\")\n",
    "    print(f\"Final Test RMSE: ${final_rmse:.2f}\")\n",
    "    print(f\"Final Test MAE: ${final_mae:.2f}\")\n",
    "    print(f\"Final R² Score: {final_r2:.4f}\")\n",
    "    \n",
    "    return train_losses, val_losses, val_rmses\n",
    "\n",
    "print(\"Training function ready!\")\n",
    "print(\"Call train_taxi_fare_model() to start training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff0d7503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STARTING TRAINING ===\n",
      "\n",
      "Epoch 1/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 232.8241\n",
      "Batch 100/4372, Loss: 176.3265\n",
      "Batch 100/4372, Loss: 176.3265\n",
      "Batch 200/4372, Loss: 167.8770\n",
      "Batch 200/4372, Loss: 167.8770\n",
      "Batch 300/4372, Loss: 147.5316\n",
      "Batch 300/4372, Loss: 147.5316\n",
      "Batch 400/4372, Loss: 102.3880\n",
      "Batch 400/4372, Loss: 102.3880\n",
      "Batch 500/4372, Loss: 115.9119\n",
      "Batch 500/4372, Loss: 115.9119\n",
      "Batch 600/4372, Loss: 106.9653\n",
      "Batch 600/4372, Loss: 106.9653\n",
      "Batch 700/4372, Loss: 70.6390\n",
      "Batch 700/4372, Loss: 70.6390\n",
      "Batch 800/4372, Loss: 33.3387\n",
      "Batch 800/4372, Loss: 33.3387\n",
      "Batch 900/4372, Loss: 37.3902\n",
      "Batch 900/4372, Loss: 37.3902\n",
      "Batch 1000/4372, Loss: 10.6225\n",
      "Batch 1000/4372, Loss: 10.6225\n",
      "Batch 1100/4372, Loss: 10.2152\n",
      "Batch 1100/4372, Loss: 10.2152\n",
      "Batch 1200/4372, Loss: 6.6498\n",
      "Batch 1200/4372, Loss: 6.6498\n",
      "Batch 1300/4372, Loss: 5.7843\n",
      "Batch 1300/4372, Loss: 5.7843\n",
      "Batch 1400/4372, Loss: 8.0483\n",
      "Batch 1400/4372, Loss: 8.0483\n",
      "Batch 1500/4372, Loss: 7.2193\n",
      "Batch 1500/4372, Loss: 7.2193\n",
      "Batch 1600/4372, Loss: 3.6163\n",
      "Batch 1600/4372, Loss: 3.6163\n",
      "Batch 1700/4372, Loss: 5.3327\n",
      "Batch 1700/4372, Loss: 5.3327\n",
      "Batch 1800/4372, Loss: 6.7448\n",
      "Batch 1800/4372, Loss: 6.7448\n",
      "Batch 1900/4372, Loss: 4.3810\n",
      "Batch 1900/4372, Loss: 4.3810\n",
      "Batch 2000/4372, Loss: 6.5614\n",
      "Batch 2000/4372, Loss: 6.5614\n",
      "Batch 2100/4372, Loss: 5.2158\n",
      "Batch 2100/4372, Loss: 5.2158\n",
      "Batch 2200/4372, Loss: 3.6197\n",
      "Batch 2200/4372, Loss: 3.6197\n",
      "Batch 2300/4372, Loss: 7.0518\n",
      "Batch 2300/4372, Loss: 7.0518\n",
      "Batch 2400/4372, Loss: 3.6887\n",
      "Batch 2400/4372, Loss: 3.6887\n",
      "Batch 2500/4372, Loss: 5.3275\n",
      "Batch 2500/4372, Loss: 5.3275\n",
      "Batch 2600/4372, Loss: 4.2692\n",
      "Batch 2600/4372, Loss: 4.2692\n",
      "Batch 2700/4372, Loss: 2.4623\n",
      "Batch 2700/4372, Loss: 2.4623\n",
      "Batch 2800/4372, Loss: 3.1688\n",
      "Batch 2800/4372, Loss: 3.1688\n",
      "Batch 2900/4372, Loss: 6.9009\n",
      "Batch 2900/4372, Loss: 6.9009\n",
      "Batch 3000/4372, Loss: 3.9127\n",
      "Batch 3000/4372, Loss: 3.9127\n",
      "Batch 3100/4372, Loss: 4.0499\n",
      "Batch 3100/4372, Loss: 4.0499\n",
      "Batch 3200/4372, Loss: 3.9198\n",
      "Batch 3200/4372, Loss: 3.9198\n",
      "Batch 3300/4372, Loss: 4.0087\n",
      "Batch 3300/4372, Loss: 4.0087\n",
      "Batch 3400/4372, Loss: 3.4647\n",
      "Batch 3400/4372, Loss: 3.4647\n",
      "Batch 3500/4372, Loss: 9.2712\n",
      "Batch 3500/4372, Loss: 9.2712\n",
      "Batch 3600/4372, Loss: 4.7006\n",
      "Batch 3600/4372, Loss: 4.7006\n",
      "Batch 3700/4372, Loss: 5.2725\n",
      "Batch 3700/4372, Loss: 5.2725\n",
      "Batch 3800/4372, Loss: 3.3948\n",
      "Batch 3800/4372, Loss: 3.3948\n",
      "Batch 3900/4372, Loss: 3.0145\n",
      "Batch 3900/4372, Loss: 3.0145\n",
      "Batch 4000/4372, Loss: 6.3656\n",
      "Batch 4000/4372, Loss: 6.3656\n",
      "Batch 4100/4372, Loss: 3.2626\n",
      "Batch 4100/4372, Loss: 3.2626\n",
      "Batch 4200/4372, Loss: 2.4325\n",
      "Batch 4200/4372, Loss: 2.4325\n",
      "Batch 4300/4372, Loss: 3.4471\n",
      "Batch 4300/4372, Loss: 3.4471\n",
      "Train Loss: 26.4813\n",
      "Val Loss: 1.5438, RMSE: 1.2425, MAE: 0.7215, R²: 0.9683\n",
      "✓ New best model saved!\n",
      "\n",
      "Epoch 2/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 5.4792\n",
      "Train Loss: 26.4813\n",
      "Val Loss: 1.5438, RMSE: 1.2425, MAE: 0.7215, R²: 0.9683\n",
      "✓ New best model saved!\n",
      "\n",
      "Epoch 2/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 5.4792\n",
      "Batch 100/4372, Loss: 6.6313\n",
      "Batch 100/4372, Loss: 6.6313\n",
      "Batch 200/4372, Loss: 8.7829\n",
      "Batch 200/4372, Loss: 8.7829\n",
      "Batch 300/4372, Loss: 4.0132\n",
      "Batch 300/4372, Loss: 4.0132\n",
      "Batch 400/4372, Loss: 10.5528\n",
      "Batch 400/4372, Loss: 10.5528\n",
      "Batch 500/4372, Loss: 2.4920\n",
      "Batch 500/4372, Loss: 2.4920\n",
      "Batch 600/4372, Loss: 4.5503\n",
      "Batch 600/4372, Loss: 4.5503\n",
      "Batch 700/4372, Loss: 3.3217\n",
      "Batch 700/4372, Loss: 3.3217\n",
      "Batch 800/4372, Loss: 3.7822\n",
      "Batch 800/4372, Loss: 3.7822\n",
      "Batch 900/4372, Loss: 4.8723\n",
      "Batch 900/4372, Loss: 4.8723\n",
      "Batch 1000/4372, Loss: 6.7611\n",
      "Batch 1000/4372, Loss: 6.7611\n",
      "Batch 1100/4372, Loss: 3.6683\n",
      "Batch 1100/4372, Loss: 3.6683\n",
      "Batch 1200/4372, Loss: 3.8553\n",
      "Batch 1200/4372, Loss: 3.8553\n",
      "Batch 1300/4372, Loss: 4.2544\n",
      "Batch 1300/4372, Loss: 4.2544\n",
      "Batch 1400/4372, Loss: 6.3591\n",
      "Batch 1400/4372, Loss: 6.3591\n",
      "Batch 1500/4372, Loss: 2.0076\n",
      "Batch 1500/4372, Loss: 2.0076\n",
      "Batch 1600/4372, Loss: 3.7529\n",
      "Batch 1600/4372, Loss: 3.7529\n",
      "Batch 1700/4372, Loss: 5.2483\n",
      "Batch 1700/4372, Loss: 5.2483\n",
      "Batch 1800/4372, Loss: 2.3552\n",
      "Batch 1800/4372, Loss: 2.3552\n",
      "Batch 1900/4372, Loss: 3.7900\n",
      "Batch 1900/4372, Loss: 3.7900\n",
      "Batch 2000/4372, Loss: 3.0686\n",
      "Batch 2000/4372, Loss: 3.0686\n",
      "Batch 2100/4372, Loss: 2.8683\n",
      "Batch 2100/4372, Loss: 2.8683\n",
      "Batch 2200/4372, Loss: 3.2996\n",
      "Batch 2200/4372, Loss: 3.2996\n",
      "Batch 2300/4372, Loss: 3.0673\n",
      "Batch 2300/4372, Loss: 3.0673\n",
      "Batch 2400/4372, Loss: 2.9706\n",
      "Batch 2400/4372, Loss: 2.9706\n",
      "Batch 2500/4372, Loss: 3.1214\n",
      "Batch 2500/4372, Loss: 3.1214\n",
      "Batch 2600/4372, Loss: 3.8209\n",
      "Batch 2600/4372, Loss: 3.8209\n",
      "Batch 2700/4372, Loss: 3.7332\n",
      "Batch 2700/4372, Loss: 3.7332\n",
      "Batch 2800/4372, Loss: 6.1593\n",
      "Batch 2800/4372, Loss: 6.1593\n",
      "Batch 2900/4372, Loss: 3.7533\n",
      "Batch 2900/4372, Loss: 3.7533\n",
      "Batch 3000/4372, Loss: 4.4029\n",
      "Batch 3000/4372, Loss: 4.4029\n",
      "Batch 3100/4372, Loss: 6.8792\n",
      "Batch 3100/4372, Loss: 6.8792\n",
      "Batch 3200/4372, Loss: 4.4159\n",
      "Batch 3200/4372, Loss: 4.4159\n",
      "Batch 3300/4372, Loss: 4.5929\n",
      "Batch 3300/4372, Loss: 4.5929\n",
      "Batch 3400/4372, Loss: 1.9538\n",
      "Batch 3400/4372, Loss: 1.9538\n",
      "Batch 3500/4372, Loss: 3.4025\n",
      "Batch 3500/4372, Loss: 3.4025\n",
      "Batch 3600/4372, Loss: 5.4449\n",
      "Batch 3600/4372, Loss: 5.4449\n",
      "Batch 3700/4372, Loss: 5.3350\n",
      "Batch 3700/4372, Loss: 5.3350\n",
      "Batch 3800/4372, Loss: 7.2522\n",
      "Batch 3800/4372, Loss: 7.2522\n",
      "Batch 3900/4372, Loss: 3.9741\n",
      "Batch 3900/4372, Loss: 3.9741\n",
      "Batch 4000/4372, Loss: 4.0545\n",
      "Batch 4000/4372, Loss: 4.0545\n",
      "Batch 4100/4372, Loss: 4.3145\n",
      "Batch 4100/4372, Loss: 4.3145\n",
      "Batch 4200/4372, Loss: 3.0941\n",
      "Batch 4200/4372, Loss: 3.0941\n",
      "Batch 4300/4372, Loss: 4.7125\n",
      "Batch 4300/4372, Loss: 4.7125\n",
      "Train Loss: 4.1739\n",
      "Val Loss: 1.4474, RMSE: 1.2031, MAE: 0.6079, R²: 0.9702\n",
      "✓ New best model saved!\n",
      "\n",
      "Epoch 3/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 3.9940\n",
      "Train Loss: 4.1739\n",
      "Val Loss: 1.4474, RMSE: 1.2031, MAE: 0.6079, R²: 0.9702\n",
      "✓ New best model saved!\n",
      "\n",
      "Epoch 3/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 3.9940\n",
      "Batch 100/4372, Loss: 5.7527\n",
      "Batch 100/4372, Loss: 5.7527\n",
      "Batch 200/4372, Loss: 3.4022\n",
      "Batch 200/4372, Loss: 3.4022\n",
      "Batch 300/4372, Loss: 2.8280\n",
      "Batch 300/4372, Loss: 2.8280\n",
      "Batch 400/4372, Loss: 4.4469\n",
      "Batch 400/4372, Loss: 4.4469\n",
      "Batch 500/4372, Loss: 3.5065\n",
      "Batch 500/4372, Loss: 3.5065\n",
      "Batch 600/4372, Loss: 7.2911\n",
      "Batch 600/4372, Loss: 7.2911\n",
      "Batch 700/4372, Loss: 3.2772\n",
      "Batch 700/4372, Loss: 3.2772\n",
      "Batch 800/4372, Loss: 4.9349\n",
      "Batch 800/4372, Loss: 4.9349\n",
      "Batch 900/4372, Loss: 2.3290\n",
      "Batch 900/4372, Loss: 2.3290\n",
      "Batch 1000/4372, Loss: 5.6368\n",
      "Batch 1000/4372, Loss: 5.6368\n",
      "Batch 1100/4372, Loss: 4.6054\n",
      "Batch 1100/4372, Loss: 4.6054\n",
      "Batch 1200/4372, Loss: 3.4745\n",
      "Batch 1200/4372, Loss: 3.4745\n",
      "Batch 1300/4372, Loss: 7.1090\n",
      "Batch 1300/4372, Loss: 7.1090\n",
      "Batch 1400/4372, Loss: 2.4405\n",
      "Batch 1400/4372, Loss: 2.4405\n",
      "Batch 1500/4372, Loss: 2.4533\n",
      "Batch 1500/4372, Loss: 2.4533\n",
      "Batch 1600/4372, Loss: 2.4815\n",
      "Batch 1600/4372, Loss: 2.4815\n",
      "Batch 1700/4372, Loss: 2.6125\n",
      "Batch 1700/4372, Loss: 2.6125\n",
      "Batch 1800/4372, Loss: 2.9428\n",
      "Batch 1800/4372, Loss: 2.9428\n",
      "Batch 1900/4372, Loss: 3.2705\n",
      "Batch 1900/4372, Loss: 3.2705\n",
      "Batch 2000/4372, Loss: 2.7349\n",
      "Batch 2000/4372, Loss: 2.7349\n",
      "Batch 2100/4372, Loss: 3.6972\n",
      "Batch 2100/4372, Loss: 3.6972\n",
      "Batch 2200/4372, Loss: 1.7860\n",
      "Batch 2200/4372, Loss: 1.7860\n",
      "Batch 2300/4372, Loss: 4.0604\n",
      "Batch 2300/4372, Loss: 4.0604\n",
      "Batch 2400/4372, Loss: 2.0950\n",
      "Batch 2400/4372, Loss: 2.0950\n",
      "Batch 2500/4372, Loss: 2.4918\n",
      "Batch 2500/4372, Loss: 2.4918\n",
      "Batch 2600/4372, Loss: 5.4331\n",
      "Batch 2600/4372, Loss: 5.4331\n",
      "Batch 2700/4372, Loss: 5.0988\n",
      "Batch 2700/4372, Loss: 5.0988\n",
      "Batch 2800/4372, Loss: 2.8536\n",
      "Batch 2800/4372, Loss: 2.8536\n",
      "Batch 2900/4372, Loss: 4.2066\n",
      "Batch 2900/4372, Loss: 4.2066\n",
      "Batch 3000/4372, Loss: 3.1658\n",
      "Batch 3000/4372, Loss: 3.1658\n",
      "Batch 3100/4372, Loss: 4.3739\n",
      "Batch 3100/4372, Loss: 4.3739\n",
      "Batch 3200/4372, Loss: 3.1455\n",
      "Batch 3200/4372, Loss: 3.1455\n",
      "Batch 3300/4372, Loss: 3.0092\n",
      "Batch 3300/4372, Loss: 3.0092\n",
      "Batch 3400/4372, Loss: 2.0120\n",
      "Batch 3400/4372, Loss: 2.0120\n",
      "Batch 3500/4372, Loss: 3.1505\n",
      "Batch 3500/4372, Loss: 3.1505\n",
      "Batch 3600/4372, Loss: 2.3031\n",
      "Batch 3600/4372, Loss: 2.3031\n",
      "Batch 3700/4372, Loss: 2.9818\n",
      "Batch 3700/4372, Loss: 2.9818\n",
      "Batch 3800/4372, Loss: 3.1767\n",
      "Batch 3800/4372, Loss: 3.1767\n",
      "Batch 3900/4372, Loss: 2.2677\n",
      "Batch 3900/4372, Loss: 2.2677\n",
      "Batch 4000/4372, Loss: 2.7007\n",
      "Batch 4000/4372, Loss: 2.7007\n",
      "Batch 4100/4372, Loss: 3.8736\n",
      "Batch 4100/4372, Loss: 3.8736\n",
      "Batch 4200/4372, Loss: 4.9543\n",
      "Batch 4200/4372, Loss: 4.9543\n",
      "Batch 4300/4372, Loss: 3.1837\n",
      "Batch 4300/4372, Loss: 3.1837\n",
      "Train Loss: 3.8170\n",
      "Val Loss: 1.0370, RMSE: 1.0183, MAE: 0.6185, R²: 0.9787\n",
      "✓ New best model saved!\n",
      "\n",
      "Epoch 4/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 2.0668\n",
      "Train Loss: 3.8170\n",
      "Val Loss: 1.0370, RMSE: 1.0183, MAE: 0.6185, R²: 0.9787\n",
      "✓ New best model saved!\n",
      "\n",
      "Epoch 4/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 2.0668\n",
      "Batch 100/4372, Loss: 2.6642\n",
      "Batch 100/4372, Loss: 2.6642\n",
      "Batch 200/4372, Loss: 3.6433\n",
      "Batch 200/4372, Loss: 3.6433\n",
      "Batch 300/4372, Loss: 2.3704\n",
      "Batch 300/4372, Loss: 2.3704\n",
      "Batch 400/4372, Loss: 2.8449\n",
      "Batch 400/4372, Loss: 2.8449\n",
      "Batch 500/4372, Loss: 10.4472\n",
      "Batch 500/4372, Loss: 10.4472\n",
      "Batch 600/4372, Loss: 3.4256\n",
      "Batch 600/4372, Loss: 3.4256\n",
      "Batch 700/4372, Loss: 4.0368\n",
      "Batch 700/4372, Loss: 4.0368\n",
      "Batch 800/4372, Loss: 3.7183\n",
      "Batch 800/4372, Loss: 3.7183\n",
      "Batch 900/4372, Loss: 3.3152\n",
      "Batch 900/4372, Loss: 3.3152\n",
      "Batch 1000/4372, Loss: 6.5504\n",
      "Batch 1000/4372, Loss: 6.5504\n",
      "Batch 1100/4372, Loss: 3.0074\n",
      "Batch 1100/4372, Loss: 3.0074\n",
      "Batch 1200/4372, Loss: 3.4748\n",
      "Batch 1200/4372, Loss: 3.4748\n",
      "Batch 1300/4372, Loss: 4.7593\n",
      "Batch 1300/4372, Loss: 4.7593\n",
      "Batch 1400/4372, Loss: 3.3702\n",
      "Batch 1400/4372, Loss: 3.3702\n",
      "Batch 1500/4372, Loss: 4.2523\n",
      "Batch 1500/4372, Loss: 4.2523\n",
      "Batch 1600/4372, Loss: 3.6005\n",
      "Batch 1600/4372, Loss: 3.6005\n",
      "Batch 1700/4372, Loss: 3.0694\n",
      "Batch 1700/4372, Loss: 3.0694\n",
      "Batch 1800/4372, Loss: 3.6585\n",
      "Batch 1800/4372, Loss: 3.6585\n",
      "Batch 1900/4372, Loss: 3.1140\n",
      "Batch 1900/4372, Loss: 3.1140\n",
      "Batch 2000/4372, Loss: 3.7758\n",
      "Batch 2000/4372, Loss: 3.7758\n",
      "Batch 2100/4372, Loss: 5.0314\n",
      "Batch 2100/4372, Loss: 5.0314\n",
      "Batch 2200/4372, Loss: 2.7967\n",
      "Batch 2200/4372, Loss: 2.7967\n",
      "Batch 2300/4372, Loss: 3.0305\n",
      "Batch 2300/4372, Loss: 3.0305\n",
      "Batch 2400/4372, Loss: 2.2786\n",
      "Batch 2400/4372, Loss: 2.2786\n",
      "Batch 2500/4372, Loss: 3.5982\n",
      "Batch 2500/4372, Loss: 3.5982\n",
      "Batch 2600/4372, Loss: 1.8803\n",
      "Batch 2600/4372, Loss: 1.8803\n",
      "Batch 2700/4372, Loss: 2.8373\n",
      "Batch 2700/4372, Loss: 2.8373\n",
      "Batch 2800/4372, Loss: 3.6276\n",
      "Batch 2800/4372, Loss: 3.6276\n",
      "Batch 2900/4372, Loss: 3.3940\n",
      "Batch 2900/4372, Loss: 3.3940\n",
      "Batch 3000/4372, Loss: 2.5731\n",
      "Batch 3000/4372, Loss: 2.5731\n",
      "Batch 3100/4372, Loss: 7.3863\n",
      "Batch 3100/4372, Loss: 7.3863\n",
      "Batch 3200/4372, Loss: 2.2325\n",
      "Batch 3200/4372, Loss: 2.2325\n",
      "Batch 3300/4372, Loss: 6.6952\n",
      "Batch 3300/4372, Loss: 6.6952\n",
      "Batch 3400/4372, Loss: 1.9378\n",
      "Batch 3400/4372, Loss: 1.9378\n",
      "Batch 3500/4372, Loss: 1.8834\n",
      "Batch 3500/4372, Loss: 1.8834\n",
      "Batch 3600/4372, Loss: 3.1465\n",
      "Batch 3600/4372, Loss: 3.1465\n",
      "Batch 3700/4372, Loss: 2.8141\n",
      "Batch 3700/4372, Loss: 2.8141\n",
      "Batch 3800/4372, Loss: 3.5608\n",
      "Batch 3800/4372, Loss: 3.5608\n",
      "Batch 3900/4372, Loss: 2.8068\n",
      "Batch 3900/4372, Loss: 2.8068\n",
      "Batch 4000/4372, Loss: 2.7566\n",
      "Batch 4000/4372, Loss: 2.7566\n",
      "Batch 4100/4372, Loss: 2.4826\n",
      "Batch 4100/4372, Loss: 2.4826\n",
      "Batch 4200/4372, Loss: 3.2128\n",
      "Batch 4200/4372, Loss: 3.2128\n",
      "Batch 4300/4372, Loss: 2.6367\n",
      "Batch 4300/4372, Loss: 2.6367\n",
      "Train Loss: 3.6123\n",
      "Val Loss: 0.9115, RMSE: 0.9547, MAE: 0.4367, R²: 0.9813\n",
      "✓ New best model saved!\n",
      "\n",
      "Epoch 5/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 2.0499\n",
      "Train Loss: 3.6123\n",
      "Val Loss: 0.9115, RMSE: 0.9547, MAE: 0.4367, R²: 0.9813\n",
      "✓ New best model saved!\n",
      "\n",
      "Epoch 5/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 2.0499\n",
      "Batch 100/4372, Loss: 1.6208\n",
      "Batch 100/4372, Loss: 1.6208\n",
      "Batch 200/4372, Loss: 2.4710\n",
      "Batch 200/4372, Loss: 2.4710\n",
      "Batch 300/4372, Loss: 3.4761\n",
      "Batch 300/4372, Loss: 3.4761\n",
      "Batch 400/4372, Loss: 2.6691\n",
      "Batch 400/4372, Loss: 2.6691\n",
      "Batch 500/4372, Loss: 2.4752\n",
      "Batch 500/4372, Loss: 2.4752\n",
      "Batch 600/4372, Loss: 3.7543\n",
      "Batch 600/4372, Loss: 3.7543\n",
      "Batch 700/4372, Loss: 3.5742\n",
      "Batch 700/4372, Loss: 3.5742\n",
      "Batch 800/4372, Loss: 6.5192\n",
      "Batch 800/4372, Loss: 6.5192\n",
      "Batch 900/4372, Loss: 1.9340\n",
      "Batch 900/4372, Loss: 1.9340\n",
      "Batch 1000/4372, Loss: 3.0366\n",
      "Batch 1000/4372, Loss: 3.0366\n",
      "Batch 1100/4372, Loss: 5.1815\n",
      "Batch 1100/4372, Loss: 5.1815\n",
      "Batch 1200/4372, Loss: 2.9095\n",
      "Batch 1200/4372, Loss: 2.9095\n",
      "Batch 1300/4372, Loss: 3.1227\n",
      "Batch 1300/4372, Loss: 3.1227\n",
      "Batch 1400/4372, Loss: 2.9922\n",
      "Batch 1400/4372, Loss: 2.9922\n",
      "Batch 1500/4372, Loss: 3.8539\n",
      "Batch 1500/4372, Loss: 3.8539\n",
      "Batch 1600/4372, Loss: 4.6421\n",
      "Batch 1600/4372, Loss: 4.6421\n",
      "Batch 1700/4372, Loss: 2.8560\n",
      "Batch 1700/4372, Loss: 2.8560\n",
      "Batch 1800/4372, Loss: 3.1453\n",
      "Batch 1800/4372, Loss: 3.1453\n",
      "Batch 1900/4372, Loss: 6.3499\n",
      "Batch 1900/4372, Loss: 6.3499\n",
      "Batch 2000/4372, Loss: 2.4764\n",
      "Batch 2000/4372, Loss: 2.4764\n",
      "Batch 2100/4372, Loss: 2.2090\n",
      "Batch 2100/4372, Loss: 2.2090\n",
      "Batch 2200/4372, Loss: 2.1590\n",
      "Batch 2200/4372, Loss: 2.1590\n",
      "Batch 2300/4372, Loss: 4.1413\n",
      "Batch 2300/4372, Loss: 4.1413\n",
      "Batch 2400/4372, Loss: 2.4296\n",
      "Batch 2400/4372, Loss: 2.4296\n",
      "Batch 2500/4372, Loss: 6.6816\n",
      "Batch 2500/4372, Loss: 6.6816\n",
      "Batch 2600/4372, Loss: 2.1497\n",
      "Batch 2600/4372, Loss: 2.1497\n",
      "Batch 2700/4372, Loss: 2.9601\n",
      "Batch 2700/4372, Loss: 2.9601\n",
      "Batch 2800/4372, Loss: 1.5237\n",
      "Batch 2800/4372, Loss: 1.5237\n",
      "Batch 2900/4372, Loss: 2.6259\n",
      "Batch 2900/4372, Loss: 2.6259\n",
      "Batch 3000/4372, Loss: 4.6020\n",
      "Batch 3000/4372, Loss: 4.6020\n",
      "Batch 3100/4372, Loss: 4.6936\n",
      "Batch 3100/4372, Loss: 4.6936\n",
      "Batch 3200/4372, Loss: 2.7782\n",
      "Batch 3200/4372, Loss: 2.7782\n",
      "Batch 3300/4372, Loss: 3.2349\n",
      "Batch 3300/4372, Loss: 3.2349\n",
      "Batch 3400/4372, Loss: 4.9973\n",
      "Batch 3400/4372, Loss: 4.9973\n",
      "Batch 3500/4372, Loss: 1.8041\n",
      "Batch 3500/4372, Loss: 1.8041\n",
      "Batch 3600/4372, Loss: 2.8385\n",
      "Batch 3600/4372, Loss: 2.8385\n",
      "Batch 3700/4372, Loss: 3.5788\n",
      "Batch 3700/4372, Loss: 3.5788\n",
      "Batch 3800/4372, Loss: 17.9314\n",
      "Batch 3800/4372, Loss: 17.9314\n",
      "Batch 3900/4372, Loss: 2.0600\n",
      "Batch 3900/4372, Loss: 2.0600\n",
      "Batch 4000/4372, Loss: 3.2966\n",
      "Batch 4000/4372, Loss: 3.2966\n",
      "Batch 4100/4372, Loss: 2.6029\n",
      "Batch 4100/4372, Loss: 2.6029\n",
      "Batch 4200/4372, Loss: 4.0524\n",
      "Batch 4200/4372, Loss: 4.0524\n",
      "Batch 4300/4372, Loss: 3.6986\n",
      "Batch 4300/4372, Loss: 3.6986\n",
      "Train Loss: 3.4565\n",
      "Val Loss: 0.9396, RMSE: 0.9693, MAE: 0.6575, R²: 0.9807\n",
      "\n",
      "Epoch 6/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 2.0856\n",
      "Train Loss: 3.4565\n",
      "Val Loss: 0.9396, RMSE: 0.9693, MAE: 0.6575, R²: 0.9807\n",
      "\n",
      "Epoch 6/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 2.0856\n",
      "Batch 100/4372, Loss: 3.5082\n",
      "Batch 100/4372, Loss: 3.5082\n",
      "Batch 200/4372, Loss: 3.0745\n",
      "Batch 200/4372, Loss: 3.0745\n",
      "Batch 300/4372, Loss: 3.5007\n",
      "Batch 300/4372, Loss: 3.5007\n",
      "Batch 400/4372, Loss: 2.9892\n",
      "Batch 400/4372, Loss: 2.9892\n",
      "Batch 500/4372, Loss: 3.0522\n",
      "Batch 500/4372, Loss: 3.0522\n",
      "Batch 600/4372, Loss: 4.1146\n",
      "Batch 600/4372, Loss: 4.1146\n",
      "Batch 700/4372, Loss: 3.6722\n",
      "Batch 700/4372, Loss: 3.6722\n",
      "Batch 800/4372, Loss: 1.7933\n",
      "Batch 800/4372, Loss: 1.7933\n",
      "Batch 900/4372, Loss: 9.1947\n",
      "Batch 900/4372, Loss: 9.1947\n",
      "Batch 1000/4372, Loss: 2.4722\n",
      "Batch 1000/4372, Loss: 2.4722\n",
      "Batch 1100/4372, Loss: 3.1735\n",
      "Batch 1100/4372, Loss: 3.1735\n",
      "Batch 1200/4372, Loss: 3.0330\n",
      "Batch 1200/4372, Loss: 3.0330\n",
      "Batch 1300/4372, Loss: 2.3707\n",
      "Batch 1300/4372, Loss: 2.3707\n",
      "Batch 1400/4372, Loss: 3.4318\n",
      "Batch 1400/4372, Loss: 3.4318\n",
      "Batch 1500/4372, Loss: 2.9008\n",
      "Batch 1500/4372, Loss: 2.9008\n",
      "Batch 1600/4372, Loss: 2.8414\n",
      "Batch 1600/4372, Loss: 2.8414\n",
      "Batch 1700/4372, Loss: 3.8823\n",
      "Batch 1700/4372, Loss: 3.8823\n",
      "Batch 1800/4372, Loss: 2.3251\n",
      "Batch 1800/4372, Loss: 2.3251\n",
      "Batch 1900/4372, Loss: 2.8737\n",
      "Batch 1900/4372, Loss: 2.8737\n",
      "Batch 2000/4372, Loss: 5.2061\n",
      "Batch 2000/4372, Loss: 5.2061\n",
      "Batch 2100/4372, Loss: 1.8189\n",
      "Batch 2100/4372, Loss: 1.8189\n",
      "Batch 2200/4372, Loss: 2.8329\n",
      "Batch 2200/4372, Loss: 2.8329\n",
      "Batch 2300/4372, Loss: 5.7327\n",
      "Batch 2300/4372, Loss: 5.7327\n",
      "Batch 2400/4372, Loss: 2.8396\n",
      "Batch 2400/4372, Loss: 2.8396\n",
      "Batch 2500/4372, Loss: 3.1190\n",
      "Batch 2500/4372, Loss: 3.1190\n",
      "Batch 2600/4372, Loss: 3.7716\n",
      "Batch 2600/4372, Loss: 3.7716\n",
      "Batch 2700/4372, Loss: 2.9475\n",
      "Batch 2700/4372, Loss: 2.9475\n",
      "Batch 2800/4372, Loss: 2.3580\n",
      "Batch 2800/4372, Loss: 2.3580\n",
      "Batch 2900/4372, Loss: 1.9867\n",
      "Batch 2900/4372, Loss: 1.9867\n",
      "Batch 3000/4372, Loss: 3.0403\n",
      "Batch 3000/4372, Loss: 3.0403\n",
      "Batch 3100/4372, Loss: 1.7085\n",
      "Batch 3100/4372, Loss: 1.7085\n",
      "Batch 3200/4372, Loss: 2.9038\n",
      "Batch 3200/4372, Loss: 2.9038\n",
      "Batch 3300/4372, Loss: 3.3382\n",
      "Batch 3300/4372, Loss: 3.3382\n",
      "Batch 3400/4372, Loss: 3.0433\n",
      "Batch 3400/4372, Loss: 3.0433\n",
      "Batch 3500/4372, Loss: 4.0637\n",
      "Batch 3500/4372, Loss: 4.0637\n",
      "Batch 3600/4372, Loss: 2.9685\n",
      "Batch 3600/4372, Loss: 2.9685\n",
      "Batch 3700/4372, Loss: 2.5821\n",
      "Batch 3700/4372, Loss: 2.5821\n",
      "Batch 3800/4372, Loss: 3.0157\n",
      "Batch 3800/4372, Loss: 3.0157\n",
      "Batch 3900/4372, Loss: 2.5386\n",
      "Batch 3900/4372, Loss: 2.5386\n",
      "Batch 4000/4372, Loss: 7.8947\n",
      "Batch 4000/4372, Loss: 7.8947\n",
      "Batch 4100/4372, Loss: 3.0312\n",
      "Batch 4100/4372, Loss: 3.0312\n",
      "Batch 4200/4372, Loss: 2.6614\n",
      "Batch 4200/4372, Loss: 2.6614\n",
      "Batch 4300/4372, Loss: 3.3372\n",
      "Batch 4300/4372, Loss: 3.3372\n",
      "Train Loss: 3.3334\n",
      "Val Loss: 0.7096, RMSE: 0.8424, MAE: 0.5031, R²: 0.9854\n",
      "✓ New best model saved!\n",
      "\n",
      "Epoch 7/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 3.6390\n",
      "Train Loss: 3.3334\n",
      "Val Loss: 0.7096, RMSE: 0.8424, MAE: 0.5031, R²: 0.9854\n",
      "✓ New best model saved!\n",
      "\n",
      "Epoch 7/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 3.6390\n",
      "Batch 100/4372, Loss: 2.4910\n",
      "Batch 100/4372, Loss: 2.4910\n",
      "Batch 200/4372, Loss: 2.5858\n",
      "Batch 200/4372, Loss: 2.5858\n",
      "Batch 300/4372, Loss: 3.1505\n",
      "Batch 300/4372, Loss: 3.1505\n",
      "Batch 400/4372, Loss: 1.9583\n",
      "Batch 400/4372, Loss: 1.9583\n",
      "Batch 500/4372, Loss: 4.7988\n",
      "Batch 500/4372, Loss: 4.7988\n",
      "Batch 600/4372, Loss: 2.0300\n",
      "Batch 600/4372, Loss: 2.0300\n",
      "Batch 700/4372, Loss: 6.1435\n",
      "Batch 700/4372, Loss: 6.1435\n",
      "Batch 800/4372, Loss: 2.8344\n",
      "Batch 800/4372, Loss: 2.8344\n",
      "Batch 900/4372, Loss: 2.3720\n",
      "Batch 900/4372, Loss: 2.3720\n",
      "Batch 1000/4372, Loss: 3.1766\n",
      "Batch 1000/4372, Loss: 3.1766\n",
      "Batch 1100/4372, Loss: 3.4969\n",
      "Batch 1100/4372, Loss: 3.4969\n",
      "Batch 1200/4372, Loss: 2.1763\n",
      "Batch 1200/4372, Loss: 2.1763\n",
      "Batch 1300/4372, Loss: 1.2978\n",
      "Batch 1300/4372, Loss: 1.2978\n",
      "Batch 1400/4372, Loss: 7.2003\n",
      "Batch 1400/4372, Loss: 7.2003\n",
      "Batch 1500/4372, Loss: 2.7237\n",
      "Batch 1500/4372, Loss: 2.7237\n",
      "Batch 1600/4372, Loss: 6.2357\n",
      "Batch 1600/4372, Loss: 6.2357\n",
      "Batch 1700/4372, Loss: 1.8700\n",
      "Batch 1700/4372, Loss: 1.8700\n",
      "Batch 1800/4372, Loss: 3.1494\n",
      "Batch 1800/4372, Loss: 3.1494\n",
      "Batch 1900/4372, Loss: 2.4512\n",
      "Batch 1900/4372, Loss: 2.4512\n",
      "Batch 2000/4372, Loss: 2.9820\n",
      "Batch 2000/4372, Loss: 2.9820\n",
      "Batch 2100/4372, Loss: 2.5341\n",
      "Batch 2100/4372, Loss: 2.5341\n",
      "Batch 2200/4372, Loss: 2.3212\n",
      "Batch 2200/4372, Loss: 2.3212\n",
      "Batch 2300/4372, Loss: 2.6815\n",
      "Batch 2300/4372, Loss: 2.6815\n",
      "Batch 2400/4372, Loss: 2.3337\n",
      "Batch 2400/4372, Loss: 2.3337\n",
      "Batch 2500/4372, Loss: 2.5477\n",
      "Batch 2500/4372, Loss: 2.5477\n",
      "Batch 2600/4372, Loss: 5.6866\n",
      "Batch 2600/4372, Loss: 5.6866\n",
      "Batch 2700/4372, Loss: 3.0520\n",
      "Batch 2700/4372, Loss: 3.0520\n",
      "Batch 2800/4372, Loss: 2.1123\n",
      "Batch 2800/4372, Loss: 2.1123\n",
      "Batch 2900/4372, Loss: 2.6731\n",
      "Batch 2900/4372, Loss: 2.6731\n",
      "Batch 3000/4372, Loss: 2.7737\n",
      "Batch 3000/4372, Loss: 2.7737\n",
      "Batch 3100/4372, Loss: 2.3748\n",
      "Batch 3100/4372, Loss: 2.3748\n",
      "Batch 3200/4372, Loss: 2.1113\n",
      "Batch 3200/4372, Loss: 2.1113\n",
      "Batch 3300/4372, Loss: 5.0459\n",
      "Batch 3300/4372, Loss: 5.0459\n",
      "Batch 3400/4372, Loss: 1.4552\n",
      "Batch 3400/4372, Loss: 1.4552\n",
      "Batch 3500/4372, Loss: 3.1597\n",
      "Batch 3500/4372, Loss: 3.1597\n",
      "Batch 3600/4372, Loss: 2.2273\n",
      "Batch 3600/4372, Loss: 2.2273\n",
      "Batch 3700/4372, Loss: 2.4614\n",
      "Batch 3700/4372, Loss: 2.4614\n",
      "Batch 3800/4372, Loss: 2.0284\n",
      "Batch 3800/4372, Loss: 2.0284\n",
      "Batch 3900/4372, Loss: 2.0439\n",
      "Batch 3900/4372, Loss: 2.0439\n",
      "Batch 4000/4372, Loss: 2.2039\n",
      "Batch 4000/4372, Loss: 2.2039\n",
      "Batch 4100/4372, Loss: 4.2663\n",
      "Batch 4100/4372, Loss: 4.2663\n",
      "Batch 4200/4372, Loss: 2.7782\n",
      "Batch 4200/4372, Loss: 2.7782\n",
      "Batch 4300/4372, Loss: 2.4843\n",
      "Batch 4300/4372, Loss: 2.4843\n",
      "Train Loss: 3.2243\n",
      "Val Loss: 0.8130, RMSE: 0.9017, MAE: 0.6027, R²: 0.9833\n",
      "\n",
      "Epoch 8/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 2.4985\n",
      "Train Loss: 3.2243\n",
      "Val Loss: 0.8130, RMSE: 0.9017, MAE: 0.6027, R²: 0.9833\n",
      "\n",
      "Epoch 8/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 2.4985\n",
      "Batch 100/4372, Loss: 2.3815\n",
      "Batch 100/4372, Loss: 2.3815\n",
      "Batch 200/4372, Loss: 2.9102\n",
      "Batch 200/4372, Loss: 2.9102\n",
      "Batch 300/4372, Loss: 2.0844\n",
      "Batch 300/4372, Loss: 2.0844\n",
      "Batch 400/4372, Loss: 2.4999\n",
      "Batch 400/4372, Loss: 2.4999\n",
      "Batch 500/4372, Loss: 2.1623\n",
      "Batch 500/4372, Loss: 2.1623\n",
      "Batch 600/4372, Loss: 2.9520\n",
      "Batch 600/4372, Loss: 2.9520\n",
      "Batch 700/4372, Loss: 2.3622\n",
      "Batch 700/4372, Loss: 2.3622\n",
      "Batch 800/4372, Loss: 1.5327\n",
      "Batch 800/4372, Loss: 1.5327\n",
      "Batch 900/4372, Loss: 2.8218\n",
      "Batch 900/4372, Loss: 2.8218\n",
      "Batch 1000/4372, Loss: 3.7410\n",
      "Batch 1000/4372, Loss: 3.7410\n",
      "Batch 1100/4372, Loss: 2.7966\n",
      "Batch 1100/4372, Loss: 2.7966\n",
      "Batch 1200/4372, Loss: 2.0713\n",
      "Batch 1200/4372, Loss: 2.0713\n",
      "Batch 1300/4372, Loss: 1.8523\n",
      "Batch 1300/4372, Loss: 1.8523\n",
      "Batch 1400/4372, Loss: 2.1593\n",
      "Batch 1400/4372, Loss: 2.1593\n",
      "Batch 1500/4372, Loss: 3.2048\n",
      "Batch 1500/4372, Loss: 3.2048\n",
      "Batch 1600/4372, Loss: 1.9155\n",
      "Batch 1600/4372, Loss: 1.9155\n",
      "Batch 1700/4372, Loss: 1.3954\n",
      "Batch 1700/4372, Loss: 1.3954\n",
      "Batch 1800/4372, Loss: 5.3148\n",
      "Batch 1800/4372, Loss: 5.3148\n",
      "Batch 1900/4372, Loss: 2.0747\n",
      "Batch 1900/4372, Loss: 2.0747\n",
      "Batch 2000/4372, Loss: 4.1370\n",
      "Batch 2000/4372, Loss: 4.1370\n",
      "Batch 2100/4372, Loss: 2.9675\n",
      "Batch 2100/4372, Loss: 2.9675\n",
      "Batch 2200/4372, Loss: 3.1698\n",
      "Batch 2200/4372, Loss: 3.1698\n",
      "Batch 2300/4372, Loss: 2.2839\n",
      "Batch 2300/4372, Loss: 2.2839\n",
      "Batch 2400/4372, Loss: 2.8811\n",
      "Batch 2400/4372, Loss: 2.8811\n",
      "Batch 2500/4372, Loss: 2.9182\n",
      "Batch 2500/4372, Loss: 2.9182\n",
      "Batch 2600/4372, Loss: 3.3111\n",
      "Batch 2600/4372, Loss: 3.3111\n",
      "Batch 2700/4372, Loss: 6.9285\n",
      "Batch 2700/4372, Loss: 6.9285\n",
      "Batch 2800/4372, Loss: 3.3044\n",
      "Batch 2800/4372, Loss: 3.3044\n",
      "Batch 2900/4372, Loss: 1.9693\n",
      "Batch 2900/4372, Loss: 1.9693\n",
      "Batch 3000/4372, Loss: 3.0746\n",
      "Batch 3000/4372, Loss: 3.0746\n",
      "Batch 3100/4372, Loss: 1.6958\n",
      "Batch 3100/4372, Loss: 1.6958\n",
      "Batch 3200/4372, Loss: 3.9295\n",
      "Batch 3200/4372, Loss: 3.9295\n",
      "Batch 3300/4372, Loss: 2.5360\n",
      "Batch 3300/4372, Loss: 2.5360\n",
      "Batch 3400/4372, Loss: 4.0240\n",
      "Batch 3400/4372, Loss: 4.0240\n",
      "Batch 3500/4372, Loss: 2.1873\n",
      "Batch 3500/4372, Loss: 2.1873\n",
      "Batch 3600/4372, Loss: 2.5257\n",
      "Batch 3600/4372, Loss: 2.5257\n",
      "Batch 3700/4372, Loss: 1.6769\n",
      "Batch 3700/4372, Loss: 1.6769\n",
      "Batch 3800/4372, Loss: 2.8581\n",
      "Batch 3800/4372, Loss: 2.8581\n",
      "Batch 3900/4372, Loss: 2.1284\n",
      "Batch 3900/4372, Loss: 2.1284\n",
      "Batch 4000/4372, Loss: 1.9515\n",
      "Batch 4000/4372, Loss: 1.9515\n",
      "Batch 4100/4372, Loss: 1.5238\n",
      "Batch 4100/4372, Loss: 1.5238\n",
      "Batch 4200/4372, Loss: 2.8581\n",
      "Batch 4200/4372, Loss: 2.8581\n",
      "Batch 4300/4372, Loss: 2.1167\n",
      "Batch 4300/4372, Loss: 2.1167\n",
      "Train Loss: 3.1688\n",
      "Val Loss: 0.6215, RMSE: 0.7883, MAE: 0.4756, R²: 0.9872\n",
      "✓ New best model saved!\n",
      "\n",
      "Epoch 9/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 3.6925\n",
      "Train Loss: 3.1688\n",
      "Val Loss: 0.6215, RMSE: 0.7883, MAE: 0.4756, R²: 0.9872\n",
      "✓ New best model saved!\n",
      "\n",
      "Epoch 9/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 3.6925\n",
      "Batch 100/4372, Loss: 1.7306\n",
      "Batch 100/4372, Loss: 1.7306\n",
      "Batch 200/4372, Loss: 2.5661\n",
      "Batch 200/4372, Loss: 2.5661\n",
      "Batch 300/4372, Loss: 4.2663\n",
      "Batch 300/4372, Loss: 4.2663\n",
      "Batch 400/4372, Loss: 2.7697\n",
      "Batch 400/4372, Loss: 2.7697\n",
      "Batch 500/4372, Loss: 6.0996\n",
      "Batch 500/4372, Loss: 6.0996\n",
      "Batch 600/4372, Loss: 3.7167\n",
      "Batch 600/4372, Loss: 3.7167\n",
      "Batch 700/4372, Loss: 2.4243\n",
      "Batch 700/4372, Loss: 2.4243\n",
      "Batch 800/4372, Loss: 1.6130\n",
      "Batch 800/4372, Loss: 1.6130\n",
      "Batch 900/4372, Loss: 2.2791\n",
      "Batch 900/4372, Loss: 2.2791\n",
      "Batch 1000/4372, Loss: 2.3108\n",
      "Batch 1000/4372, Loss: 2.3108\n",
      "Batch 1100/4372, Loss: 6.2652\n",
      "Batch 1100/4372, Loss: 6.2652\n",
      "Batch 1200/4372, Loss: 2.6784\n",
      "Batch 1200/4372, Loss: 2.6784\n",
      "Batch 1300/4372, Loss: 2.5523\n",
      "Batch 1300/4372, Loss: 2.5523\n",
      "Batch 1400/4372, Loss: 2.6422\n",
      "Batch 1400/4372, Loss: 2.6422\n",
      "Batch 1500/4372, Loss: 1.8988\n",
      "Batch 1500/4372, Loss: 1.8988\n",
      "Batch 1600/4372, Loss: 1.3668\n",
      "Batch 1600/4372, Loss: 1.3668\n",
      "Batch 1700/4372, Loss: 2.4401\n",
      "Batch 1700/4372, Loss: 2.4401\n",
      "Batch 1800/4372, Loss: 2.0268\n",
      "Batch 1800/4372, Loss: 2.0268\n",
      "Batch 1900/4372, Loss: 2.6006\n",
      "Batch 1900/4372, Loss: 2.6006\n",
      "Batch 2000/4372, Loss: 3.8287\n",
      "Batch 2000/4372, Loss: 3.8287\n",
      "Batch 2100/4372, Loss: 2.6673\n",
      "Batch 2100/4372, Loss: 2.6673\n",
      "Batch 2200/4372, Loss: 4.9313\n",
      "Batch 2200/4372, Loss: 4.9313\n",
      "Batch 2300/4372, Loss: 2.7649\n",
      "Batch 2300/4372, Loss: 2.7649\n",
      "Batch 2400/4372, Loss: 1.9688\n",
      "Batch 2400/4372, Loss: 1.9688\n",
      "Batch 2500/4372, Loss: 3.1496\n",
      "Batch 2500/4372, Loss: 3.1496\n",
      "Batch 2600/4372, Loss: 2.8099\n",
      "Batch 2600/4372, Loss: 2.8099\n",
      "Batch 2700/4372, Loss: 1.7620\n",
      "Batch 2700/4372, Loss: 1.7620\n",
      "Batch 2800/4372, Loss: 2.1891\n",
      "Batch 2800/4372, Loss: 2.1891\n",
      "Batch 2900/4372, Loss: 2.5905\n",
      "Batch 2900/4372, Loss: 2.5905\n",
      "Batch 3000/4372, Loss: 2.4128\n",
      "Batch 3000/4372, Loss: 2.4128\n",
      "Batch 3100/4372, Loss: 1.7254\n",
      "Batch 3100/4372, Loss: 1.7254\n",
      "Batch 3200/4372, Loss: 2.4909\n",
      "Batch 3200/4372, Loss: 2.4909\n",
      "Batch 3300/4372, Loss: 3.5506\n",
      "Batch 3300/4372, Loss: 3.5506\n",
      "Batch 3400/4372, Loss: 2.4436\n",
      "Batch 3400/4372, Loss: 2.4436\n",
      "Batch 3500/4372, Loss: 3.9501\n",
      "Batch 3500/4372, Loss: 3.9501\n",
      "Batch 3600/4372, Loss: 2.4309\n",
      "Batch 3600/4372, Loss: 2.4309\n",
      "Batch 3700/4372, Loss: 1.2689\n",
      "Batch 3700/4372, Loss: 1.2689\n",
      "Batch 3800/4372, Loss: 2.9931\n",
      "Batch 3800/4372, Loss: 2.9931\n",
      "Batch 3900/4372, Loss: 2.9885\n",
      "Batch 3900/4372, Loss: 2.9885\n",
      "Batch 4000/4372, Loss: 2.8795\n",
      "Batch 4000/4372, Loss: 2.8795\n",
      "Batch 4100/4372, Loss: 2.2961\n",
      "Batch 4100/4372, Loss: 2.2961\n",
      "Batch 4200/4372, Loss: 3.2692\n",
      "Batch 4200/4372, Loss: 3.2692\n",
      "Batch 4300/4372, Loss: 3.3174\n",
      "Batch 4300/4372, Loss: 3.3174\n",
      "Train Loss: 3.0771\n",
      "Val Loss: 0.6163, RMSE: 0.7850, MAE: 0.4683, R²: 0.9873\n",
      "✓ New best model saved!\n",
      "\n",
      "Epoch 10/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 4.0853\n",
      "Train Loss: 3.0771\n",
      "Val Loss: 0.6163, RMSE: 0.7850, MAE: 0.4683, R²: 0.9873\n",
      "✓ New best model saved!\n",
      "\n",
      "Epoch 10/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 4.0853\n",
      "Batch 100/4372, Loss: 2.8576\n",
      "Batch 100/4372, Loss: 2.8576\n",
      "Batch 200/4372, Loss: 2.7553\n",
      "Batch 200/4372, Loss: 2.7553\n",
      "Batch 300/4372, Loss: 2.3039\n",
      "Batch 300/4372, Loss: 2.3039\n",
      "Batch 400/4372, Loss: 2.5513\n",
      "Batch 400/4372, Loss: 2.5513\n",
      "Batch 500/4372, Loss: 1.9596\n",
      "Batch 500/4372, Loss: 1.9596\n",
      "Batch 600/4372, Loss: 2.2483\n",
      "Batch 600/4372, Loss: 2.2483\n",
      "Batch 700/4372, Loss: 2.2320\n",
      "Batch 700/4372, Loss: 2.2320\n",
      "Batch 800/4372, Loss: 2.1686\n",
      "Batch 800/4372, Loss: 2.1686\n",
      "Batch 900/4372, Loss: 4.7369\n",
      "Batch 900/4372, Loss: 4.7369\n",
      "Batch 1000/4372, Loss: 1.2372\n",
      "Batch 1000/4372, Loss: 1.2372\n",
      "Batch 1100/4372, Loss: 5.0952\n",
      "Batch 1100/4372, Loss: 5.0952\n",
      "Batch 1200/4372, Loss: 3.8092\n",
      "Batch 1200/4372, Loss: 3.8092\n",
      "Batch 1300/4372, Loss: 2.7712\n",
      "Batch 1300/4372, Loss: 2.7712\n",
      "Batch 1400/4372, Loss: 1.6632\n",
      "Batch 1400/4372, Loss: 1.6632\n",
      "Batch 1500/4372, Loss: 2.0436\n",
      "Batch 1500/4372, Loss: 2.0436\n",
      "Batch 1600/4372, Loss: 3.5284\n",
      "Batch 1600/4372, Loss: 3.5284\n",
      "Batch 1700/4372, Loss: 4.8656\n",
      "Batch 1700/4372, Loss: 4.8656\n",
      "Batch 1800/4372, Loss: 1.6207\n",
      "Batch 1800/4372, Loss: 1.6207\n",
      "Batch 1900/4372, Loss: 5.2045\n",
      "Batch 1900/4372, Loss: 5.2045\n",
      "Batch 2000/4372, Loss: 3.7858\n",
      "Batch 2000/4372, Loss: 3.7858\n",
      "Batch 2100/4372, Loss: 3.3468\n",
      "Batch 2100/4372, Loss: 3.3468\n",
      "Batch 2200/4372, Loss: 4.0708\n",
      "Batch 2200/4372, Loss: 4.0708\n",
      "Batch 2300/4372, Loss: 1.9404\n",
      "Batch 2300/4372, Loss: 1.9404\n",
      "Batch 2400/4372, Loss: 2.1354\n",
      "Batch 2400/4372, Loss: 2.1354\n",
      "Batch 2500/4372, Loss: 2.2632\n",
      "Batch 2500/4372, Loss: 2.2632\n",
      "Batch 2600/4372, Loss: 4.5877\n",
      "Batch 2600/4372, Loss: 4.5877\n",
      "Batch 2700/4372, Loss: 5.0840\n",
      "Batch 2700/4372, Loss: 5.0840\n",
      "Batch 2800/4372, Loss: 4.3644\n",
      "Batch 2800/4372, Loss: 4.3644\n",
      "Batch 2900/4372, Loss: 2.6757\n",
      "Batch 2900/4372, Loss: 2.6757\n",
      "Batch 3000/4372, Loss: 1.7635\n",
      "Batch 3000/4372, Loss: 1.7635\n",
      "Batch 3100/4372, Loss: 4.5840\n",
      "Batch 3100/4372, Loss: 4.5840\n",
      "Batch 3200/4372, Loss: 2.8315\n",
      "Batch 3200/4372, Loss: 2.8315\n",
      "Batch 3300/4372, Loss: 2.6616\n",
      "Batch 3300/4372, Loss: 2.6616\n",
      "Batch 3400/4372, Loss: 3.1243\n",
      "Batch 3400/4372, Loss: 3.1243\n",
      "Batch 3500/4372, Loss: 3.1603\n",
      "Batch 3500/4372, Loss: 3.1603\n",
      "Batch 3600/4372, Loss: 2.2892\n",
      "Batch 3600/4372, Loss: 2.2892\n",
      "Batch 3700/4372, Loss: 1.3695\n",
      "Batch 3700/4372, Loss: 1.3695\n",
      "Batch 3800/4372, Loss: 5.1426\n",
      "Batch 3800/4372, Loss: 5.1426\n",
      "Batch 3900/4372, Loss: 6.6500\n",
      "Batch 3900/4372, Loss: 6.6500\n",
      "Batch 4000/4372, Loss: 2.5827\n",
      "Batch 4000/4372, Loss: 2.5827\n",
      "Batch 4100/4372, Loss: 2.9968\n",
      "Batch 4100/4372, Loss: 2.9968\n",
      "Batch 4200/4372, Loss: 2.0419\n",
      "Batch 4200/4372, Loss: 2.0419\n",
      "Batch 4300/4372, Loss: 1.6759\n",
      "Batch 4300/4372, Loss: 1.6759\n",
      "Train Loss: 3.0095\n",
      "Val Loss: 0.6188, RMSE: 0.7866, MAE: 0.4505, R²: 0.9873\n",
      "\n",
      "Epoch 11/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 2.5465\n",
      "Train Loss: 3.0095\n",
      "Val Loss: 0.6188, RMSE: 0.7866, MAE: 0.4505, R²: 0.9873\n",
      "\n",
      "Epoch 11/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 2.5465\n",
      "Batch 100/4372, Loss: 2.8021\n",
      "Batch 100/4372, Loss: 2.8021\n",
      "Batch 200/4372, Loss: 2.4212\n",
      "Batch 200/4372, Loss: 2.4212\n",
      "Batch 300/4372, Loss: 5.5238\n",
      "Batch 300/4372, Loss: 5.5238\n",
      "Batch 400/4372, Loss: 1.7576\n",
      "Batch 400/4372, Loss: 1.7576\n",
      "Batch 500/4372, Loss: 5.3051\n",
      "Batch 500/4372, Loss: 5.3051\n",
      "Batch 600/4372, Loss: 2.1865\n",
      "Batch 600/4372, Loss: 2.1865\n",
      "Batch 700/4372, Loss: 1.0207\n",
      "Batch 700/4372, Loss: 1.0207\n",
      "Batch 800/4372, Loss: 3.4509\n",
      "Batch 800/4372, Loss: 3.4509\n",
      "Batch 900/4372, Loss: 4.9969\n",
      "Batch 900/4372, Loss: 4.9969\n",
      "Batch 1000/4372, Loss: 2.7013\n",
      "Batch 1000/4372, Loss: 2.7013\n",
      "Batch 1100/4372, Loss: 1.9152\n",
      "Batch 1100/4372, Loss: 1.9152\n",
      "Batch 1200/4372, Loss: 1.9688\n",
      "Batch 1200/4372, Loss: 1.9688\n",
      "Batch 1300/4372, Loss: 2.2863\n",
      "Batch 1300/4372, Loss: 2.2863\n",
      "Batch 1400/4372, Loss: 1.4997\n",
      "Batch 1400/4372, Loss: 1.4997\n",
      "Batch 1500/4372, Loss: 4.7068\n",
      "Batch 1500/4372, Loss: 4.7068\n",
      "Batch 1600/4372, Loss: 3.5580\n",
      "Batch 1600/4372, Loss: 3.5580\n",
      "Batch 1700/4372, Loss: 1.1739\n",
      "Batch 1700/4372, Loss: 1.1739\n",
      "Batch 1800/4372, Loss: 3.2494\n",
      "Batch 1800/4372, Loss: 3.2494\n",
      "Batch 1900/4372, Loss: 3.4012\n",
      "Batch 1900/4372, Loss: 3.4012\n",
      "Batch 2000/4372, Loss: 1.7944\n",
      "Batch 2000/4372, Loss: 1.7944\n",
      "Batch 2100/4372, Loss: 2.0999\n",
      "Batch 2100/4372, Loss: 2.0999\n",
      "Batch 2200/4372, Loss: 1.7971\n",
      "Batch 2200/4372, Loss: 1.7971\n",
      "Batch 2300/4372, Loss: 3.5202\n",
      "Batch 2300/4372, Loss: 3.5202\n",
      "Batch 2400/4372, Loss: 2.1488\n",
      "Batch 2400/4372, Loss: 2.1488\n",
      "Batch 2500/4372, Loss: 3.7474\n",
      "Batch 2500/4372, Loss: 3.7474\n",
      "Batch 2600/4372, Loss: 3.3414\n",
      "Batch 2600/4372, Loss: 3.3414\n",
      "Batch 2700/4372, Loss: 2.3720\n",
      "Batch 2700/4372, Loss: 2.3720\n",
      "Batch 2800/4372, Loss: 1.9539\n",
      "Batch 2800/4372, Loss: 1.9539\n",
      "Batch 2900/4372, Loss: 1.7156\n",
      "Batch 2900/4372, Loss: 1.7156\n",
      "Batch 3000/4372, Loss: 4.5602\n",
      "Batch 3000/4372, Loss: 4.5602\n",
      "Batch 3100/4372, Loss: 3.1969\n",
      "Batch 3100/4372, Loss: 3.1969\n",
      "Batch 3200/4372, Loss: 2.0680\n",
      "Batch 3200/4372, Loss: 2.0680\n",
      "Batch 3300/4372, Loss: 1.9976\n",
      "Batch 3300/4372, Loss: 1.9976\n",
      "Batch 3400/4372, Loss: 5.4310\n",
      "Batch 3400/4372, Loss: 5.4310\n",
      "Batch 3500/4372, Loss: 4.1583\n",
      "Batch 3500/4372, Loss: 4.1583\n",
      "Batch 3600/4372, Loss: 5.4096\n",
      "Batch 3600/4372, Loss: 5.4096\n",
      "Batch 3700/4372, Loss: 4.0142\n",
      "Batch 3700/4372, Loss: 4.0142\n",
      "Batch 3800/4372, Loss: 3.7525\n",
      "Batch 3800/4372, Loss: 3.7525\n",
      "Batch 3900/4372, Loss: 1.3764\n",
      "Batch 3900/4372, Loss: 1.3764\n",
      "Batch 4000/4372, Loss: 2.1835\n",
      "Batch 4000/4372, Loss: 2.1835\n",
      "Batch 4100/4372, Loss: 2.5419\n",
      "Batch 4100/4372, Loss: 2.5419\n",
      "Batch 4200/4372, Loss: 2.0204\n",
      "Batch 4200/4372, Loss: 2.0204\n",
      "Batch 4300/4372, Loss: 4.5242\n",
      "Batch 4300/4372, Loss: 4.5242\n",
      "Train Loss: 2.9894\n",
      "Val Loss: 1.0537, RMSE: 1.0265, MAE: 0.7246, R²: 0.9783\n",
      "\n",
      "Epoch 12/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 1.8719\n",
      "Train Loss: 2.9894\n",
      "Val Loss: 1.0537, RMSE: 1.0265, MAE: 0.7246, R²: 0.9783\n",
      "\n",
      "Epoch 12/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 1.8719\n",
      "Batch 100/4372, Loss: 4.2786\n",
      "Batch 100/4372, Loss: 4.2786\n",
      "Batch 200/4372, Loss: 9.6735\n",
      "Batch 200/4372, Loss: 9.6735\n",
      "Batch 300/4372, Loss: 4.5082\n",
      "Batch 300/4372, Loss: 4.5082\n",
      "Batch 400/4372, Loss: 1.8750\n",
      "Batch 400/4372, Loss: 1.8750\n",
      "Batch 500/4372, Loss: 1.7559\n",
      "Batch 500/4372, Loss: 1.7559\n",
      "Batch 600/4372, Loss: 4.1872\n",
      "Batch 600/4372, Loss: 4.1872\n",
      "Batch 700/4372, Loss: 2.0310\n",
      "Batch 700/4372, Loss: 2.0310\n",
      "Batch 800/4372, Loss: 1.4105\n",
      "Batch 800/4372, Loss: 1.4105\n",
      "Batch 900/4372, Loss: 2.4202\n",
      "Batch 900/4372, Loss: 2.4202\n",
      "Batch 1000/4372, Loss: 2.5456\n",
      "Batch 1000/4372, Loss: 2.5456\n",
      "Batch 1100/4372, Loss: 1.8217\n",
      "Batch 1100/4372, Loss: 1.8217\n",
      "Batch 1200/4372, Loss: 3.3801\n",
      "Batch 1200/4372, Loss: 3.3801\n",
      "Batch 1300/4372, Loss: 2.1507\n",
      "Batch 1300/4372, Loss: 2.1507\n",
      "Batch 1400/4372, Loss: 1.8154\n",
      "Batch 1400/4372, Loss: 1.8154\n",
      "Batch 1500/4372, Loss: 2.8162\n",
      "Batch 1500/4372, Loss: 2.8162\n",
      "Batch 1600/4372, Loss: 6.0060\n",
      "Batch 1600/4372, Loss: 6.0060\n",
      "Batch 1700/4372, Loss: 2.3802\n",
      "Batch 1700/4372, Loss: 2.3802\n",
      "Batch 1800/4372, Loss: 2.3024\n",
      "Batch 1800/4372, Loss: 2.3024\n",
      "Batch 1900/4372, Loss: 2.9804\n",
      "Batch 1900/4372, Loss: 2.9804\n",
      "Batch 2000/4372, Loss: 5.8274\n",
      "Batch 2000/4372, Loss: 5.8274\n",
      "Batch 2100/4372, Loss: 3.2112\n",
      "Batch 2100/4372, Loss: 3.2112\n",
      "Batch 2200/4372, Loss: 3.2655\n",
      "Batch 2200/4372, Loss: 3.2655\n",
      "Batch 2300/4372, Loss: 4.4069\n",
      "Batch 2300/4372, Loss: 4.4069\n",
      "Batch 2400/4372, Loss: 2.4365\n",
      "Batch 2400/4372, Loss: 2.4365\n",
      "Batch 2500/4372, Loss: 1.5081\n",
      "Batch 2500/4372, Loss: 1.5081\n",
      "Batch 2600/4372, Loss: 7.7823\n",
      "Batch 2600/4372, Loss: 7.7823\n",
      "Batch 2700/4372, Loss: 2.3752\n",
      "Batch 2700/4372, Loss: 2.3752\n",
      "Batch 2800/4372, Loss: 4.2978\n",
      "Batch 2800/4372, Loss: 4.2978\n",
      "Batch 2900/4372, Loss: 4.5578\n",
      "Batch 2900/4372, Loss: 4.5578\n",
      "Batch 3000/4372, Loss: 4.0952\n",
      "Batch 3000/4372, Loss: 4.0952\n",
      "Batch 3100/4372, Loss: 2.3465\n",
      "Batch 3100/4372, Loss: 2.3465\n",
      "Batch 3200/4372, Loss: 5.2754\n",
      "Batch 3200/4372, Loss: 5.2754\n",
      "Batch 3300/4372, Loss: 4.1630\n",
      "Batch 3300/4372, Loss: 4.1630\n",
      "Batch 3400/4372, Loss: 3.3230\n",
      "Batch 3400/4372, Loss: 3.3230\n",
      "Batch 3500/4372, Loss: 2.7829\n",
      "Batch 3500/4372, Loss: 2.7829\n",
      "Batch 3600/4372, Loss: 3.0953\n",
      "Batch 3600/4372, Loss: 3.0953\n",
      "Batch 3700/4372, Loss: 2.3916\n",
      "Batch 3700/4372, Loss: 2.3916\n",
      "Batch 3800/4372, Loss: 5.7488\n",
      "Batch 3800/4372, Loss: 5.7488\n",
      "Batch 3900/4372, Loss: 1.9226\n",
      "Batch 3900/4372, Loss: 1.9226\n",
      "Batch 4000/4372, Loss: 2.8416\n",
      "Batch 4000/4372, Loss: 2.8416\n",
      "Batch 4100/4372, Loss: 2.6494\n",
      "Batch 4100/4372, Loss: 2.6494\n",
      "Batch 4200/4372, Loss: 2.4577\n",
      "Batch 4200/4372, Loss: 2.4577\n",
      "Batch 4300/4372, Loss: 3.8288\n",
      "Batch 4300/4372, Loss: 3.8288\n",
      "Train Loss: 2.9185\n",
      "Val Loss: 0.5553, RMSE: 0.7452, MAE: 0.4616, R²: 0.9886\n",
      "✓ New best model saved!\n",
      "\n",
      "Epoch 13/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 2.0300\n",
      "Train Loss: 2.9185\n",
      "Val Loss: 0.5553, RMSE: 0.7452, MAE: 0.4616, R²: 0.9886\n",
      "✓ New best model saved!\n",
      "\n",
      "Epoch 13/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 2.0300\n",
      "Batch 100/4372, Loss: 3.8676\n",
      "Batch 100/4372, Loss: 3.8676\n",
      "Batch 200/4372, Loss: 2.4475\n",
      "Batch 200/4372, Loss: 2.4475\n",
      "Batch 300/4372, Loss: 2.0667\n",
      "Batch 300/4372, Loss: 2.0667\n",
      "Batch 400/4372, Loss: 5.3788\n",
      "Batch 400/4372, Loss: 5.3788\n",
      "Batch 500/4372, Loss: 2.0411\n",
      "Batch 500/4372, Loss: 2.0411\n",
      "Batch 600/4372, Loss: 4.6369\n",
      "Batch 600/4372, Loss: 4.6369\n",
      "Batch 700/4372, Loss: 1.6589\n",
      "Batch 700/4372, Loss: 1.6589\n",
      "Batch 800/4372, Loss: 3.3638\n",
      "Batch 800/4372, Loss: 3.3638\n",
      "Batch 900/4372, Loss: 1.9721\n",
      "Batch 900/4372, Loss: 1.9721\n",
      "Batch 1000/4372, Loss: 2.1678\n",
      "Batch 1000/4372, Loss: 2.1678\n",
      "Batch 1100/4372, Loss: 5.0782\n",
      "Batch 1100/4372, Loss: 5.0782\n",
      "Batch 1200/4372, Loss: 2.6674\n",
      "Batch 1200/4372, Loss: 2.6674\n",
      "Batch 1300/4372, Loss: 3.7852\n",
      "Batch 1300/4372, Loss: 3.7852\n",
      "Batch 1400/4372, Loss: 2.0971\n",
      "Batch 1400/4372, Loss: 2.0971\n",
      "Batch 1500/4372, Loss: 2.0905\n",
      "Batch 1500/4372, Loss: 2.0905\n",
      "Batch 1600/4372, Loss: 1.5422\n",
      "Batch 1600/4372, Loss: 1.5422\n",
      "Batch 1700/4372, Loss: 3.6675\n",
      "Batch 1700/4372, Loss: 3.6675\n",
      "Batch 1800/4372, Loss: 1.4207\n",
      "Batch 1800/4372, Loss: 1.4207\n",
      "Batch 1900/4372, Loss: 5.9675\n",
      "Batch 1900/4372, Loss: 5.9675\n",
      "Batch 2000/4372, Loss: 2.0271\n",
      "Batch 2000/4372, Loss: 2.0271\n",
      "Batch 2100/4372, Loss: 3.9690\n",
      "Batch 2100/4372, Loss: 3.9690\n",
      "Batch 2200/4372, Loss: 3.1945\n",
      "Batch 2200/4372, Loss: 3.1945\n",
      "Batch 2300/4372, Loss: 2.5192\n",
      "Batch 2300/4372, Loss: 2.5192\n",
      "Batch 2400/4372, Loss: 1.0467\n",
      "Batch 2400/4372, Loss: 1.0467\n",
      "Batch 2500/4372, Loss: 4.2364\n",
      "Batch 2500/4372, Loss: 4.2364\n",
      "Batch 2600/4372, Loss: 1.7300\n",
      "Batch 2600/4372, Loss: 1.7300\n",
      "Batch 2700/4372, Loss: 2.9313\n",
      "Batch 2700/4372, Loss: 2.9313\n",
      "Batch 2800/4372, Loss: 2.4446\n",
      "Batch 2800/4372, Loss: 2.4446\n",
      "Batch 2900/4372, Loss: 3.6918\n",
      "Batch 2900/4372, Loss: 3.6918\n",
      "Batch 3000/4372, Loss: 1.4444\n",
      "Batch 3000/4372, Loss: 1.4444\n",
      "Batch 3100/4372, Loss: 6.4836\n",
      "Batch 3100/4372, Loss: 6.4836\n",
      "Batch 3200/4372, Loss: 1.8940\n",
      "Batch 3200/4372, Loss: 1.8940\n",
      "Batch 3300/4372, Loss: 1.5049\n",
      "Batch 3300/4372, Loss: 1.5049\n",
      "Batch 3400/4372, Loss: 3.4344\n",
      "Batch 3400/4372, Loss: 3.4344\n",
      "Batch 3500/4372, Loss: 2.0027\n",
      "Batch 3500/4372, Loss: 2.0027\n",
      "Batch 3600/4372, Loss: 2.5129\n",
      "Batch 3600/4372, Loss: 2.5129\n",
      "Batch 3700/4372, Loss: 2.3367\n",
      "Batch 3700/4372, Loss: 2.3367\n",
      "Batch 3800/4372, Loss: 5.5477\n",
      "Batch 3800/4372, Loss: 5.5477\n",
      "Batch 3900/4372, Loss: 2.2825\n",
      "Batch 3900/4372, Loss: 2.2825\n",
      "Batch 4000/4372, Loss: 2.1190\n",
      "Batch 4000/4372, Loss: 2.1190\n",
      "Batch 4100/4372, Loss: 3.1710\n",
      "Batch 4100/4372, Loss: 3.1710\n",
      "Batch 4200/4372, Loss: 1.5313\n",
      "Batch 4200/4372, Loss: 1.5313\n",
      "Batch 4300/4372, Loss: 2.4704\n",
      "Batch 4300/4372, Loss: 2.4704\n",
      "Train Loss: 2.8556\n",
      "Val Loss: 0.9797, RMSE: 0.9898, MAE: 0.7301, R²: 0.9799\n",
      "\n",
      "Epoch 14/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 2.0359\n",
      "Train Loss: 2.8556\n",
      "Val Loss: 0.9797, RMSE: 0.9898, MAE: 0.7301, R²: 0.9799\n",
      "\n",
      "Epoch 14/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 2.0359\n",
      "Batch 100/4372, Loss: 2.9447\n",
      "Batch 100/4372, Loss: 2.9447\n",
      "Batch 200/4372, Loss: 1.8870\n",
      "Batch 200/4372, Loss: 1.8870\n",
      "Batch 300/4372, Loss: 2.4821\n",
      "Batch 300/4372, Loss: 2.4821\n",
      "Batch 400/4372, Loss: 4.9578\n",
      "Batch 400/4372, Loss: 4.9578\n",
      "Batch 500/4372, Loss: 2.4736\n",
      "Batch 500/4372, Loss: 2.4736\n",
      "Batch 600/4372, Loss: 1.3635\n",
      "Batch 600/4372, Loss: 1.3635\n",
      "Batch 700/4372, Loss: 2.3114\n",
      "Batch 700/4372, Loss: 2.3114\n",
      "Batch 800/4372, Loss: 2.2894\n",
      "Batch 800/4372, Loss: 2.2894\n",
      "Batch 900/4372, Loss: 5.0005\n",
      "Batch 900/4372, Loss: 5.0005\n",
      "Batch 1000/4372, Loss: 1.7827\n",
      "Batch 1000/4372, Loss: 1.7827\n",
      "Batch 1100/4372, Loss: 2.4176\n",
      "Batch 1100/4372, Loss: 2.4176\n",
      "Batch 1200/4372, Loss: 2.3031\n",
      "Batch 1200/4372, Loss: 2.3031\n",
      "Batch 1300/4372, Loss: 3.5610\n",
      "Batch 1300/4372, Loss: 3.5610\n",
      "Batch 1400/4372, Loss: 2.2691\n",
      "Batch 1400/4372, Loss: 2.2691\n",
      "Batch 1500/4372, Loss: 2.6626\n",
      "Batch 1500/4372, Loss: 2.6626\n",
      "Batch 1600/4372, Loss: 1.7909\n",
      "Batch 1600/4372, Loss: 1.7909\n",
      "Batch 1700/4372, Loss: 6.0326\n",
      "Batch 1700/4372, Loss: 6.0326\n",
      "Batch 1800/4372, Loss: 1.6971\n",
      "Batch 1800/4372, Loss: 1.6971\n",
      "Batch 1900/4372, Loss: 3.4139\n",
      "Batch 1900/4372, Loss: 3.4139\n",
      "Batch 2000/4372, Loss: 5.5496\n",
      "Batch 2000/4372, Loss: 5.5496\n",
      "Batch 2100/4372, Loss: 6.8251\n",
      "Batch 2100/4372, Loss: 6.8251\n",
      "Batch 2200/4372, Loss: 3.3991\n",
      "Batch 2200/4372, Loss: 3.3991\n",
      "Batch 2300/4372, Loss: 2.1507\n",
      "Batch 2300/4372, Loss: 2.1507\n",
      "Batch 2400/4372, Loss: 2.3631\n",
      "Batch 2400/4372, Loss: 2.3631\n",
      "Batch 2500/4372, Loss: 3.1073\n",
      "Batch 2500/4372, Loss: 3.1073\n",
      "Batch 2600/4372, Loss: 2.9975\n",
      "Batch 2600/4372, Loss: 2.9975\n",
      "Batch 2700/4372, Loss: 2.8014\n",
      "Batch 2700/4372, Loss: 2.8014\n",
      "Batch 2800/4372, Loss: 1.6941\n",
      "Batch 2800/4372, Loss: 1.6941\n",
      "Batch 2900/4372, Loss: 2.7825\n",
      "Batch 2900/4372, Loss: 2.7825\n",
      "Batch 3000/4372, Loss: 2.9504\n",
      "Batch 3000/4372, Loss: 2.9504\n",
      "Batch 3100/4372, Loss: 2.8865\n",
      "Batch 3100/4372, Loss: 2.8865\n",
      "Batch 3200/4372, Loss: 2.7807\n",
      "Batch 3200/4372, Loss: 2.7807\n",
      "Batch 3300/4372, Loss: 1.8592\n",
      "Batch 3300/4372, Loss: 1.8592\n",
      "Batch 3400/4372, Loss: 1.0807\n",
      "Batch 3400/4372, Loss: 1.0807\n",
      "Batch 3500/4372, Loss: 1.9283\n",
      "Batch 3500/4372, Loss: 1.9283\n",
      "Batch 3600/4372, Loss: 1.3274\n",
      "Batch 3600/4372, Loss: 1.3274\n",
      "Batch 3700/4372, Loss: 4.3078\n",
      "Batch 3700/4372, Loss: 4.3078\n",
      "Batch 3800/4372, Loss: 2.2832\n",
      "Batch 3800/4372, Loss: 2.2832\n",
      "Batch 3900/4372, Loss: 2.6643\n",
      "Batch 3900/4372, Loss: 2.6643\n",
      "Batch 4000/4372, Loss: 3.8877\n",
      "Batch 4000/4372, Loss: 3.8877\n",
      "Batch 4100/4372, Loss: 1.9848\n",
      "Batch 4100/4372, Loss: 1.9848\n",
      "Batch 4200/4372, Loss: 2.2677\n",
      "Batch 4200/4372, Loss: 2.2677\n",
      "Batch 4300/4372, Loss: 1.5331\n",
      "Batch 4300/4372, Loss: 1.5331\n",
      "Train Loss: 2.7851\n",
      "Val Loss: 0.6042, RMSE: 0.7773, MAE: 0.4504, R²: 0.9876\n",
      "\n",
      "Epoch 15/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 2.7277\n",
      "Train Loss: 2.7851\n",
      "Val Loss: 0.6042, RMSE: 0.7773, MAE: 0.4504, R²: 0.9876\n",
      "\n",
      "Epoch 15/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 2.7277\n",
      "Batch 100/4372, Loss: 2.4457\n",
      "Batch 100/4372, Loss: 2.4457\n",
      "Batch 200/4372, Loss: 2.2643\n",
      "Batch 200/4372, Loss: 2.2643\n",
      "Batch 300/4372, Loss: 2.3537\n",
      "Batch 300/4372, Loss: 2.3537\n",
      "Batch 400/4372, Loss: 1.7500\n",
      "Batch 400/4372, Loss: 1.7500\n",
      "Batch 500/4372, Loss: 2.2782\n",
      "Batch 500/4372, Loss: 2.2782\n",
      "Batch 600/4372, Loss: 4.0664\n",
      "Batch 600/4372, Loss: 4.0664\n",
      "Batch 700/4372, Loss: 1.9218\n",
      "Batch 700/4372, Loss: 1.9218\n",
      "Batch 800/4372, Loss: 1.6249\n",
      "Batch 800/4372, Loss: 1.6249\n",
      "Batch 900/4372, Loss: 1.6422\n",
      "Batch 900/4372, Loss: 1.6422\n",
      "Batch 1000/4372, Loss: 1.3310\n",
      "Batch 1000/4372, Loss: 1.3310\n",
      "Batch 1100/4372, Loss: 2.5606\n",
      "Batch 1100/4372, Loss: 2.5606\n",
      "Batch 1200/4372, Loss: 1.3306\n",
      "Batch 1200/4372, Loss: 1.3306\n",
      "Batch 1300/4372, Loss: 6.0558\n",
      "Batch 1300/4372, Loss: 6.0558\n",
      "Batch 1400/4372, Loss: 1.9180\n",
      "Batch 1400/4372, Loss: 1.9180\n",
      "Batch 1500/4372, Loss: 2.1419\n",
      "Batch 1500/4372, Loss: 2.1419\n",
      "Batch 1600/4372, Loss: 1.5117\n",
      "Batch 1600/4372, Loss: 1.5117\n",
      "Batch 1700/4372, Loss: 1.5966\n",
      "Batch 1700/4372, Loss: 1.5966\n",
      "Batch 1800/4372, Loss: 1.2156\n",
      "Batch 1800/4372, Loss: 1.2156\n",
      "Batch 1900/4372, Loss: 5.1156\n",
      "Batch 1900/4372, Loss: 5.1156\n",
      "Batch 2000/4372, Loss: 2.4480\n",
      "Batch 2000/4372, Loss: 2.4480\n",
      "Batch 2100/4372, Loss: 3.0589\n",
      "Batch 2100/4372, Loss: 3.0589\n",
      "Batch 2200/4372, Loss: 1.8724\n",
      "Batch 2200/4372, Loss: 1.8724\n",
      "Batch 2300/4372, Loss: 5.7296\n",
      "Batch 2300/4372, Loss: 5.7296\n",
      "Batch 2400/4372, Loss: 11.4449\n",
      "Batch 2400/4372, Loss: 11.4449\n",
      "Batch 2500/4372, Loss: 3.5140\n",
      "Batch 2500/4372, Loss: 3.5140\n",
      "Batch 2600/4372, Loss: 1.7620\n",
      "Batch 2600/4372, Loss: 1.7620\n",
      "Batch 2700/4372, Loss: 1.7778\n",
      "Batch 2700/4372, Loss: 1.7778\n",
      "Batch 2800/4372, Loss: 2.1254\n",
      "Batch 2800/4372, Loss: 2.1254\n",
      "Batch 2900/4372, Loss: 2.7514\n",
      "Batch 2900/4372, Loss: 2.7514\n",
      "Batch 3000/4372, Loss: 3.0262\n",
      "Batch 3000/4372, Loss: 3.0262\n",
      "Batch 3100/4372, Loss: 5.4810\n",
      "Batch 3100/4372, Loss: 5.4810\n",
      "Batch 3200/4372, Loss: 2.9363\n",
      "Batch 3200/4372, Loss: 2.9363\n",
      "Batch 3300/4372, Loss: 2.5522\n",
      "Batch 3300/4372, Loss: 2.5522\n",
      "Batch 3400/4372, Loss: 2.7904\n",
      "Batch 3400/4372, Loss: 2.7904\n",
      "Batch 3500/4372, Loss: 2.4158\n",
      "Batch 3500/4372, Loss: 2.4158\n",
      "Batch 3600/4372, Loss: 1.8356\n",
      "Batch 3600/4372, Loss: 1.8356\n",
      "Batch 3700/4372, Loss: 4.9443\n",
      "Batch 3700/4372, Loss: 4.9443\n",
      "Batch 3800/4372, Loss: 2.7989\n",
      "Batch 3800/4372, Loss: 2.7989\n",
      "Batch 3900/4372, Loss: 2.2846\n",
      "Batch 3900/4372, Loss: 2.2846\n",
      "Batch 4000/4372, Loss: 2.2371\n",
      "Batch 4000/4372, Loss: 2.2371\n",
      "Batch 4100/4372, Loss: 1.3628\n",
      "Batch 4100/4372, Loss: 1.3628\n",
      "Batch 4200/4372, Loss: 2.7486\n",
      "Batch 4200/4372, Loss: 2.7486\n",
      "Batch 4300/4372, Loss: 1.5294\n",
      "Batch 4300/4372, Loss: 1.5294\n",
      "Train Loss: 2.7926\n",
      "Val Loss: 0.5970, RMSE: 0.7727, MAE: 0.4715, R²: 0.9877\n",
      "\n",
      "Epoch 16/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 1.1479\n",
      "Train Loss: 2.7926\n",
      "Val Loss: 0.5970, RMSE: 0.7727, MAE: 0.4715, R²: 0.9877\n",
      "\n",
      "Epoch 16/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 1.1479\n",
      "Batch 100/4372, Loss: 3.2571\n",
      "Batch 100/4372, Loss: 3.2571\n",
      "Batch 200/4372, Loss: 3.9112\n",
      "Batch 200/4372, Loss: 3.9112\n",
      "Batch 300/4372, Loss: 2.3746\n",
      "Batch 300/4372, Loss: 2.3746\n",
      "Batch 400/4372, Loss: 4.7939\n",
      "Batch 400/4372, Loss: 4.7939\n",
      "Batch 500/4372, Loss: 1.8146\n",
      "Batch 500/4372, Loss: 1.8146\n",
      "Batch 600/4372, Loss: 2.5079\n",
      "Batch 600/4372, Loss: 2.5079\n",
      "Batch 700/4372, Loss: 1.5206\n",
      "Batch 700/4372, Loss: 1.5206\n",
      "Batch 800/4372, Loss: 1.9374\n",
      "Batch 800/4372, Loss: 1.9374\n",
      "Batch 900/4372, Loss: 4.1592\n",
      "Batch 900/4372, Loss: 4.1592\n",
      "Batch 1000/4372, Loss: 2.8852\n",
      "Batch 1000/4372, Loss: 2.8852\n",
      "Batch 1100/4372, Loss: 2.2650\n",
      "Batch 1100/4372, Loss: 2.2650\n",
      "Batch 1200/4372, Loss: 2.7138\n",
      "Batch 1200/4372, Loss: 2.7138\n",
      "Batch 1300/4372, Loss: 1.9361\n",
      "Batch 1300/4372, Loss: 1.9361\n",
      "Batch 1400/4372, Loss: 2.1905\n",
      "Batch 1400/4372, Loss: 2.1905\n",
      "Batch 1500/4372, Loss: 5.4812\n",
      "Batch 1500/4372, Loss: 5.4812\n",
      "Batch 1600/4372, Loss: 1.8485\n",
      "Batch 1600/4372, Loss: 1.8485\n",
      "Batch 1700/4372, Loss: 1.5474\n",
      "Batch 1700/4372, Loss: 1.5474\n",
      "Batch 1800/4372, Loss: 2.1927\n",
      "Batch 1800/4372, Loss: 2.1927\n",
      "Batch 1900/4372, Loss: 1.5846\n",
      "Batch 1900/4372, Loss: 1.5846\n",
      "Batch 2000/4372, Loss: 1.9978\n",
      "Batch 2000/4372, Loss: 1.9978\n",
      "Batch 2100/4372, Loss: 2.9817\n",
      "Batch 2100/4372, Loss: 2.9817\n",
      "Batch 2200/4372, Loss: 1.4547\n",
      "Batch 2200/4372, Loss: 1.4547\n",
      "Batch 2300/4372, Loss: 2.0100\n",
      "Batch 2300/4372, Loss: 2.0100\n",
      "Batch 2400/4372, Loss: 3.5006\n",
      "Batch 2400/4372, Loss: 3.5006\n",
      "Batch 2500/4372, Loss: 1.8472\n",
      "Batch 2500/4372, Loss: 1.8472\n",
      "Batch 2600/4372, Loss: 3.0358\n",
      "Batch 2600/4372, Loss: 3.0358\n",
      "Batch 2700/4372, Loss: 2.0598\n",
      "Batch 2700/4372, Loss: 2.0598\n",
      "Batch 2800/4372, Loss: 7.7650\n",
      "Batch 2800/4372, Loss: 7.7650\n",
      "Batch 2900/4372, Loss: 1.8962\n",
      "Batch 2900/4372, Loss: 1.8962\n",
      "Batch 3000/4372, Loss: 3.2834\n",
      "Batch 3000/4372, Loss: 3.2834\n",
      "Batch 3100/4372, Loss: 3.3978\n",
      "Batch 3100/4372, Loss: 3.3978\n",
      "Batch 3200/4372, Loss: 3.5458\n",
      "Batch 3200/4372, Loss: 3.5458\n",
      "Batch 3300/4372, Loss: 2.0255\n",
      "Batch 3300/4372, Loss: 2.0255\n",
      "Batch 3400/4372, Loss: 1.8607\n",
      "Batch 3400/4372, Loss: 1.8607\n",
      "Batch 3500/4372, Loss: 2.3307\n",
      "Batch 3500/4372, Loss: 2.3307\n",
      "Batch 3600/4372, Loss: 2.1817\n",
      "Batch 3600/4372, Loss: 2.1817\n",
      "Batch 3700/4372, Loss: 4.0467\n",
      "Batch 3700/4372, Loss: 4.0467\n",
      "Batch 3800/4372, Loss: 1.9213\n",
      "Batch 3800/4372, Loss: 1.9213\n",
      "Batch 3900/4372, Loss: 5.7181\n",
      "Batch 3900/4372, Loss: 5.7181\n",
      "Batch 4000/4372, Loss: 2.0320\n",
      "Batch 4000/4372, Loss: 2.0320\n",
      "Batch 4100/4372, Loss: 2.6303\n",
      "Batch 4100/4372, Loss: 2.6303\n",
      "Batch 4200/4372, Loss: 3.3885\n",
      "Batch 4200/4372, Loss: 3.3885\n",
      "Batch 4300/4372, Loss: 2.0898\n",
      "Batch 4300/4372, Loss: 2.0898\n",
      "Train Loss: 2.7481\n",
      "Val Loss: 0.7440, RMSE: 0.8625, MAE: 0.5149, R²: 0.9847\n",
      "\n",
      "Epoch 17/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 2.3072\n",
      "Train Loss: 2.7481\n",
      "Val Loss: 0.7440, RMSE: 0.8625, MAE: 0.5149, R²: 0.9847\n",
      "\n",
      "Epoch 17/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 2.3072\n",
      "Batch 100/4372, Loss: 6.7795\n",
      "Batch 100/4372, Loss: 6.7795\n",
      "Batch 200/4372, Loss: 2.5047\n",
      "Batch 200/4372, Loss: 2.5047\n",
      "Batch 300/4372, Loss: 1.3509\n",
      "Batch 300/4372, Loss: 1.3509\n",
      "Batch 400/4372, Loss: 1.7280\n",
      "Batch 400/4372, Loss: 1.7280\n",
      "Batch 500/4372, Loss: 1.1749\n",
      "Batch 500/4372, Loss: 1.1749\n",
      "Batch 600/4372, Loss: 3.2225\n",
      "Batch 600/4372, Loss: 3.2225\n",
      "Batch 700/4372, Loss: 3.3430\n",
      "Batch 700/4372, Loss: 3.3430\n",
      "Batch 800/4372, Loss: 2.7211\n",
      "Batch 800/4372, Loss: 2.7211\n",
      "Batch 900/4372, Loss: 3.3907\n",
      "Batch 900/4372, Loss: 3.3907\n",
      "Batch 1000/4372, Loss: 1.5844\n",
      "Batch 1000/4372, Loss: 1.5844\n",
      "Batch 1100/4372, Loss: 2.1949\n",
      "Batch 1100/4372, Loss: 2.1949\n",
      "Batch 1200/4372, Loss: 2.8833\n",
      "Batch 1200/4372, Loss: 2.8833\n",
      "Batch 1300/4372, Loss: 2.9175\n",
      "Batch 1300/4372, Loss: 2.9175\n",
      "Batch 1400/4372, Loss: 2.4326\n",
      "Batch 1400/4372, Loss: 2.4326\n",
      "Batch 1500/4372, Loss: 3.1607\n",
      "Batch 1500/4372, Loss: 3.1607\n",
      "Batch 1600/4372, Loss: 1.8548\n",
      "Batch 1600/4372, Loss: 1.8548\n",
      "Batch 1700/4372, Loss: 2.2477\n",
      "Batch 1700/4372, Loss: 2.2477\n",
      "Batch 1800/4372, Loss: 1.2509\n",
      "Batch 1800/4372, Loss: 1.2509\n",
      "Batch 1900/4372, Loss: 2.4050\n",
      "Batch 1900/4372, Loss: 2.4050\n",
      "Batch 2000/4372, Loss: 3.3993\n",
      "Batch 2000/4372, Loss: 3.3993\n",
      "Batch 2100/4372, Loss: 0.9823\n",
      "Batch 2100/4372, Loss: 0.9823\n",
      "Batch 2200/4372, Loss: 4.6777\n",
      "Batch 2200/4372, Loss: 4.6777\n",
      "Batch 2300/4372, Loss: 2.1389\n",
      "Batch 2300/4372, Loss: 2.1389\n",
      "Batch 2400/4372, Loss: 2.0306\n",
      "Batch 2400/4372, Loss: 2.0306\n",
      "Batch 2500/4372, Loss: 1.9926\n",
      "Batch 2500/4372, Loss: 1.9926\n",
      "Batch 2600/4372, Loss: 1.3932\n",
      "Batch 2600/4372, Loss: 1.3932\n",
      "Batch 2700/4372, Loss: 2.6372\n",
      "Batch 2700/4372, Loss: 2.6372\n",
      "Batch 2800/4372, Loss: 4.4406\n",
      "Batch 2800/4372, Loss: 4.4406\n",
      "Batch 2900/4372, Loss: 2.4893\n",
      "Batch 2900/4372, Loss: 2.4893\n",
      "Batch 3000/4372, Loss: 2.1437\n",
      "Batch 3000/4372, Loss: 2.1437\n",
      "Batch 3100/4372, Loss: 2.4919\n",
      "Batch 3100/4372, Loss: 2.4919\n",
      "Batch 3200/4372, Loss: 4.9234\n",
      "Batch 3200/4372, Loss: 4.9234\n",
      "Batch 3300/4372, Loss: 2.4957\n",
      "Batch 3300/4372, Loss: 2.4957\n",
      "Batch 3400/4372, Loss: 2.3727\n",
      "Batch 3400/4372, Loss: 2.3727\n",
      "Batch 3500/4372, Loss: 1.2082\n",
      "Batch 3500/4372, Loss: 1.2082\n",
      "Batch 3600/4372, Loss: 1.6065\n",
      "Batch 3600/4372, Loss: 1.6065\n",
      "Batch 3700/4372, Loss: 1.7915\n",
      "Batch 3700/4372, Loss: 1.7915\n",
      "Batch 3800/4372, Loss: 2.1250\n",
      "Batch 3800/4372, Loss: 2.1250\n",
      "Batch 3900/4372, Loss: 2.8090\n",
      "Batch 3900/4372, Loss: 2.8090\n",
      "Batch 4000/4372, Loss: 2.7213\n",
      "Batch 4000/4372, Loss: 2.7213\n",
      "Batch 4100/4372, Loss: 2.7261\n",
      "Batch 4100/4372, Loss: 2.7261\n",
      "Batch 4200/4372, Loss: 1.5089\n",
      "Batch 4200/4372, Loss: 1.5089\n",
      "Batch 4300/4372, Loss: 2.9017\n",
      "Batch 4300/4372, Loss: 2.9017\n",
      "Train Loss: 2.7330\n",
      "Val Loss: 0.5268, RMSE: 0.7258, MAE: 0.3955, R²: 0.9892\n",
      "✓ New best model saved!\n",
      "\n",
      "Epoch 18/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 3.9784\n",
      "Train Loss: 2.7330\n",
      "Val Loss: 0.5268, RMSE: 0.7258, MAE: 0.3955, R²: 0.9892\n",
      "✓ New best model saved!\n",
      "\n",
      "Epoch 18/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 3.9784\n",
      "Batch 100/4372, Loss: 1.6965\n",
      "Batch 100/4372, Loss: 1.6965\n",
      "Batch 200/4372, Loss: 1.4784\n",
      "Batch 200/4372, Loss: 1.4784\n",
      "Batch 300/4372, Loss: 2.7658\n",
      "Batch 300/4372, Loss: 2.7658\n",
      "Batch 400/4372, Loss: 2.2977\n",
      "Batch 400/4372, Loss: 2.2977\n",
      "Batch 500/4372, Loss: 2.7235\n",
      "Batch 500/4372, Loss: 2.7235\n",
      "Batch 600/4372, Loss: 3.1535\n",
      "Batch 600/4372, Loss: 3.1535\n",
      "Batch 700/4372, Loss: 2.3534\n",
      "Batch 700/4372, Loss: 2.3534\n",
      "Batch 800/4372, Loss: 1.2543\n",
      "Batch 800/4372, Loss: 1.2543\n",
      "Batch 900/4372, Loss: 3.4702\n",
      "Batch 900/4372, Loss: 3.4702\n",
      "Batch 1000/4372, Loss: 2.3045\n",
      "Batch 1000/4372, Loss: 2.3045\n",
      "Batch 1100/4372, Loss: 1.7542\n",
      "Batch 1100/4372, Loss: 1.7542\n",
      "Batch 1200/4372, Loss: 2.9665\n",
      "Batch 1200/4372, Loss: 2.9665\n",
      "Batch 1300/4372, Loss: 1.6243\n",
      "Batch 1300/4372, Loss: 1.6243\n",
      "Batch 1400/4372, Loss: 2.9290\n",
      "Batch 1400/4372, Loss: 2.9290\n",
      "Batch 1500/4372, Loss: 1.6680\n",
      "Batch 1500/4372, Loss: 1.6680\n",
      "Batch 1600/4372, Loss: 2.0961\n",
      "Batch 1600/4372, Loss: 2.0961\n",
      "Batch 1700/4372, Loss: 2.5811\n",
      "Batch 1700/4372, Loss: 2.5811\n",
      "Batch 1800/4372, Loss: 2.9491\n",
      "Batch 1800/4372, Loss: 2.9491\n",
      "Batch 1900/4372, Loss: 2.4498\n",
      "Batch 1900/4372, Loss: 2.4498\n",
      "Batch 2000/4372, Loss: 2.1333\n",
      "Batch 2000/4372, Loss: 2.1333\n",
      "Batch 2100/4372, Loss: 1.5754\n",
      "Batch 2100/4372, Loss: 1.5754\n",
      "Batch 2200/4372, Loss: 2.1417\n",
      "Batch 2200/4372, Loss: 2.1417\n",
      "Batch 2300/4372, Loss: 1.6136\n",
      "Batch 2300/4372, Loss: 1.6136\n",
      "Batch 2400/4372, Loss: 3.0193\n",
      "Batch 2400/4372, Loss: 3.0193\n",
      "Batch 2500/4372, Loss: 1.5278\n",
      "Batch 2500/4372, Loss: 1.5278\n",
      "Batch 2600/4372, Loss: 2.9450\n",
      "Batch 2600/4372, Loss: 2.9450\n",
      "Batch 2700/4372, Loss: 2.0465\n",
      "Batch 2700/4372, Loss: 2.0465\n",
      "Batch 2800/4372, Loss: 2.7631\n",
      "Batch 2800/4372, Loss: 2.7631\n",
      "Batch 2900/4372, Loss: 1.5118\n",
      "Batch 2900/4372, Loss: 1.5118\n",
      "Batch 3000/4372, Loss: 1.9442\n",
      "Batch 3000/4372, Loss: 1.9442\n",
      "Batch 3100/4372, Loss: 1.8603\n",
      "Batch 3100/4372, Loss: 1.8603\n",
      "Batch 3200/4372, Loss: 1.8525\n",
      "Batch 3200/4372, Loss: 1.8525\n",
      "Batch 3300/4372, Loss: 1.4282\n",
      "Batch 3300/4372, Loss: 1.4282\n",
      "Batch 3400/4372, Loss: 1.6847\n",
      "Batch 3400/4372, Loss: 1.6847\n",
      "Batch 3500/4372, Loss: 1.6970\n",
      "Batch 3500/4372, Loss: 1.6970\n",
      "Batch 3600/4372, Loss: 1.8587\n",
      "Batch 3600/4372, Loss: 1.8587\n",
      "Batch 3700/4372, Loss: 1.6050\n",
      "Batch 3700/4372, Loss: 1.6050\n",
      "Batch 3800/4372, Loss: 3.5071\n",
      "Batch 3800/4372, Loss: 3.5071\n",
      "Batch 3900/4372, Loss: 6.1605\n",
      "Batch 3900/4372, Loss: 6.1605\n",
      "Batch 4000/4372, Loss: 2.2170\n",
      "Batch 4000/4372, Loss: 2.2170\n",
      "Batch 4100/4372, Loss: 3.2133\n",
      "Batch 4100/4372, Loss: 3.2133\n",
      "Batch 4200/4372, Loss: 3.3499\n",
      "Batch 4200/4372, Loss: 3.3499\n",
      "Batch 4300/4372, Loss: 1.7774\n",
      "Batch 4300/4372, Loss: 1.7774\n",
      "Train Loss: 2.6687\n",
      "Val Loss: 0.7002, RMSE: 0.8368, MAE: 0.5619, R²: 0.9856\n",
      "\n",
      "Epoch 19/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 1.9051\n",
      "Train Loss: 2.6687\n",
      "Val Loss: 0.7002, RMSE: 0.8368, MAE: 0.5619, R²: 0.9856\n",
      "\n",
      "Epoch 19/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 1.9051\n",
      "Batch 100/4372, Loss: 6.1303\n",
      "Batch 100/4372, Loss: 6.1303\n",
      "Batch 200/4372, Loss: 2.8083\n",
      "Batch 200/4372, Loss: 2.8083\n",
      "Batch 300/4372, Loss: 1.5722\n",
      "Batch 300/4372, Loss: 1.5722\n",
      "Batch 400/4372, Loss: 3.9354\n",
      "Batch 400/4372, Loss: 3.9354\n",
      "Batch 500/4372, Loss: 2.2770\n",
      "Batch 500/4372, Loss: 2.2770\n",
      "Batch 600/4372, Loss: 7.9483\n",
      "Batch 600/4372, Loss: 7.9483\n",
      "Batch 700/4372, Loss: 3.0300\n",
      "Batch 700/4372, Loss: 3.0300\n",
      "Batch 800/4372, Loss: 5.8863\n",
      "Batch 800/4372, Loss: 5.8863\n",
      "Batch 900/4372, Loss: 1.1868\n",
      "Batch 900/4372, Loss: 1.1868\n",
      "Batch 1000/4372, Loss: 1.9969\n",
      "Batch 1000/4372, Loss: 1.9969\n",
      "Batch 1100/4372, Loss: 1.8752\n",
      "Batch 1100/4372, Loss: 1.8752\n",
      "Batch 1200/4372, Loss: 1.6904\n",
      "Batch 1200/4372, Loss: 1.6904\n",
      "Batch 1300/4372, Loss: 4.9100\n",
      "Batch 1300/4372, Loss: 4.9100\n",
      "Batch 1400/4372, Loss: 2.7379\n",
      "Batch 1400/4372, Loss: 2.7379\n",
      "Batch 1500/4372, Loss: 4.5940\n",
      "Batch 1500/4372, Loss: 4.5940\n",
      "Batch 1600/4372, Loss: 2.4398\n",
      "Batch 1600/4372, Loss: 2.4398\n",
      "Batch 1700/4372, Loss: 2.2810\n",
      "Batch 1700/4372, Loss: 2.2810\n",
      "Batch 1800/4372, Loss: 1.9338\n",
      "Batch 1800/4372, Loss: 1.9338\n",
      "Batch 1900/4372, Loss: 1.8825\n",
      "Batch 1900/4372, Loss: 1.8825\n",
      "Batch 2000/4372, Loss: 3.3713\n",
      "Batch 2000/4372, Loss: 3.3713\n",
      "Batch 2100/4372, Loss: 2.4649\n",
      "Batch 2100/4372, Loss: 2.4649\n",
      "Batch 2200/4372, Loss: 1.8581\n",
      "Batch 2200/4372, Loss: 1.8581\n",
      "Batch 2300/4372, Loss: 2.3619\n",
      "Batch 2300/4372, Loss: 2.3619\n",
      "Batch 2400/4372, Loss: 1.7524\n",
      "Batch 2400/4372, Loss: 1.7524\n",
      "Batch 2500/4372, Loss: 1.7003\n",
      "Batch 2500/4372, Loss: 1.7003\n",
      "Batch 2600/4372, Loss: 1.8432\n",
      "Batch 2600/4372, Loss: 1.8432\n",
      "Batch 2700/4372, Loss: 2.3237\n",
      "Batch 2700/4372, Loss: 2.3237\n",
      "Batch 2800/4372, Loss: 2.4508\n",
      "Batch 2800/4372, Loss: 2.4508\n",
      "Batch 2900/4372, Loss: 3.8333\n",
      "Batch 2900/4372, Loss: 3.8333\n",
      "Batch 3000/4372, Loss: 1.1797\n",
      "Batch 3000/4372, Loss: 1.1797\n",
      "Batch 3100/4372, Loss: 1.5430\n",
      "Batch 3100/4372, Loss: 1.5430\n",
      "Batch 3200/4372, Loss: 2.2620\n",
      "Batch 3200/4372, Loss: 2.2620\n",
      "Batch 3300/4372, Loss: 3.0603\n",
      "Batch 3300/4372, Loss: 3.0603\n",
      "Batch 3400/4372, Loss: 2.0835\n",
      "Batch 3400/4372, Loss: 2.0835\n",
      "Batch 3500/4372, Loss: 2.2279\n",
      "Batch 3500/4372, Loss: 2.2279\n",
      "Batch 3600/4372, Loss: 2.3686\n",
      "Batch 3600/4372, Loss: 2.3686\n",
      "Batch 3700/4372, Loss: 2.2260\n",
      "Batch 3700/4372, Loss: 2.2260\n",
      "Batch 3800/4372, Loss: 3.1497\n",
      "Batch 3800/4372, Loss: 3.1497\n",
      "Batch 3900/4372, Loss: 2.5584\n",
      "Batch 3900/4372, Loss: 2.5584\n",
      "Batch 4000/4372, Loss: 3.8885\n",
      "Batch 4000/4372, Loss: 3.8885\n",
      "Batch 4100/4372, Loss: 1.9055\n",
      "Batch 4100/4372, Loss: 1.9055\n",
      "Batch 4200/4372, Loss: 1.7682\n",
      "Batch 4200/4372, Loss: 1.7682\n",
      "Batch 4300/4372, Loss: 3.0431\n",
      "Batch 4300/4372, Loss: 3.0431\n",
      "Train Loss: 2.6600\n",
      "Val Loss: 1.3626, RMSE: 1.1673, MAE: 0.8968, R²: 0.9720\n",
      "\n",
      "Epoch 20/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 3.1813\n",
      "Train Loss: 2.6600\n",
      "Val Loss: 1.3626, RMSE: 1.1673, MAE: 0.8968, R²: 0.9720\n",
      "\n",
      "Epoch 20/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 3.1813\n",
      "Batch 100/4372, Loss: 1.9868\n",
      "Batch 100/4372, Loss: 1.9868\n",
      "Batch 200/4372, Loss: 1.2983\n",
      "Batch 200/4372, Loss: 1.2983\n",
      "Batch 300/4372, Loss: 1.5310\n",
      "Batch 300/4372, Loss: 1.5310\n",
      "Batch 400/4372, Loss: 1.8054\n",
      "Batch 400/4372, Loss: 1.8054\n",
      "Batch 500/4372, Loss: 1.6430\n",
      "Batch 500/4372, Loss: 1.6430\n",
      "Batch 600/4372, Loss: 2.2390\n",
      "Batch 600/4372, Loss: 2.2390\n",
      "Batch 700/4372, Loss: 3.8090\n",
      "Batch 700/4372, Loss: 3.8090\n",
      "Batch 800/4372, Loss: 1.6041\n",
      "Batch 800/4372, Loss: 1.6041\n",
      "Batch 900/4372, Loss: 2.0521\n",
      "Batch 900/4372, Loss: 2.0521\n",
      "Batch 1000/4372, Loss: 1.2853\n",
      "Batch 1000/4372, Loss: 1.2853\n",
      "Batch 1100/4372, Loss: 3.0131\n",
      "Batch 1100/4372, Loss: 3.0131\n",
      "Batch 1200/4372, Loss: 1.0637\n",
      "Batch 1200/4372, Loss: 1.0637\n",
      "Batch 1300/4372, Loss: 3.0353\n",
      "Batch 1300/4372, Loss: 3.0353\n",
      "Batch 1400/4372, Loss: 5.8316\n",
      "Batch 1400/4372, Loss: 5.8316\n",
      "Batch 1500/4372, Loss: 1.3255\n",
      "Batch 1500/4372, Loss: 1.3255\n",
      "Batch 1600/4372, Loss: 3.0479\n",
      "Batch 1600/4372, Loss: 3.0479\n",
      "Batch 1700/4372, Loss: 2.5205\n",
      "Batch 1700/4372, Loss: 2.5205\n",
      "Batch 1800/4372, Loss: 2.2930\n",
      "Batch 1800/4372, Loss: 2.2930\n",
      "Batch 1900/4372, Loss: 2.4806\n",
      "Batch 1900/4372, Loss: 2.4806\n",
      "Batch 2000/4372, Loss: 1.6809\n",
      "Batch 2000/4372, Loss: 1.6809\n",
      "Batch 2100/4372, Loss: 1.5966\n",
      "Batch 2100/4372, Loss: 1.5966\n",
      "Batch 2200/4372, Loss: 3.6922\n",
      "Batch 2200/4372, Loss: 3.6922\n",
      "Batch 2300/4372, Loss: 4.7069\n",
      "Batch 2300/4372, Loss: 4.7069\n",
      "Batch 2400/4372, Loss: 1.8709\n",
      "Batch 2400/4372, Loss: 1.8709\n",
      "Batch 2500/4372, Loss: 1.4388\n",
      "Batch 2500/4372, Loss: 1.4388\n",
      "Batch 2600/4372, Loss: 1.0923\n",
      "Batch 2600/4372, Loss: 1.0923\n",
      "Batch 2700/4372, Loss: 2.1803\n",
      "Batch 2700/4372, Loss: 2.1803\n",
      "Batch 2800/4372, Loss: 0.9937\n",
      "Batch 2800/4372, Loss: 0.9937\n",
      "Batch 2900/4372, Loss: 3.8270\n",
      "Batch 2900/4372, Loss: 3.8270\n",
      "Batch 3000/4372, Loss: 8.9666\n",
      "Batch 3000/4372, Loss: 8.9666\n",
      "Batch 3100/4372, Loss: 2.5875\n",
      "Batch 3100/4372, Loss: 2.5875\n",
      "Batch 3200/4372, Loss: 2.7104\n",
      "Batch 3200/4372, Loss: 2.7104\n",
      "Batch 3300/4372, Loss: 2.2661\n",
      "Batch 3300/4372, Loss: 2.2661\n",
      "Batch 3400/4372, Loss: 2.4645\n",
      "Batch 3400/4372, Loss: 2.4645\n",
      "Batch 3500/4372, Loss: 2.0746\n",
      "Batch 3500/4372, Loss: 2.0746\n",
      "Batch 3600/4372, Loss: 4.0126\n",
      "Batch 3600/4372, Loss: 4.0126\n",
      "Batch 3700/4372, Loss: 3.8175\n",
      "Batch 3700/4372, Loss: 3.8175\n",
      "Batch 3800/4372, Loss: 1.9237\n",
      "Batch 3800/4372, Loss: 1.9237\n",
      "Batch 3900/4372, Loss: 1.6378\n",
      "Batch 3900/4372, Loss: 1.6378\n",
      "Batch 4000/4372, Loss: 5.1039\n",
      "Batch 4000/4372, Loss: 5.1039\n",
      "Batch 4100/4372, Loss: 6.9709\n",
      "Batch 4100/4372, Loss: 6.9709\n",
      "Batch 4200/4372, Loss: 1.5017\n",
      "Batch 4200/4372, Loss: 1.5017\n",
      "Batch 4300/4372, Loss: 1.9744\n",
      "Batch 4300/4372, Loss: 1.9744\n",
      "Train Loss: 2.6356\n",
      "Val Loss: 0.5638, RMSE: 0.7509, MAE: 0.4317, R²: 0.9884\n",
      "\n",
      "Epoch 21/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 1.5531\n",
      "Train Loss: 2.6356\n",
      "Val Loss: 0.5638, RMSE: 0.7509, MAE: 0.4317, R²: 0.9884\n",
      "\n",
      "Epoch 21/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 1.5531\n",
      "Batch 100/4372, Loss: 2.2503\n",
      "Batch 100/4372, Loss: 2.2503\n",
      "Batch 200/4372, Loss: 3.0987\n",
      "Batch 200/4372, Loss: 3.0987\n",
      "Batch 300/4372, Loss: 3.4529\n",
      "Batch 300/4372, Loss: 3.4529\n",
      "Batch 400/4372, Loss: 6.1120\n",
      "Batch 400/4372, Loss: 6.1120\n",
      "Batch 500/4372, Loss: 4.0401\n",
      "Batch 500/4372, Loss: 4.0401\n",
      "Batch 600/4372, Loss: 1.4049\n",
      "Batch 600/4372, Loss: 1.4049\n",
      "Batch 700/4372, Loss: 1.6227\n",
      "Batch 700/4372, Loss: 1.6227\n",
      "Batch 800/4372, Loss: 3.5245\n",
      "Batch 800/4372, Loss: 3.5245\n",
      "Batch 900/4372, Loss: 2.6907\n",
      "Batch 900/4372, Loss: 2.6907\n",
      "Batch 1000/4372, Loss: 2.3573\n",
      "Batch 1000/4372, Loss: 2.3573\n",
      "Batch 1100/4372, Loss: 1.5301\n",
      "Batch 1100/4372, Loss: 1.5301\n",
      "Batch 1200/4372, Loss: 3.2276\n",
      "Batch 1200/4372, Loss: 3.2276\n",
      "Batch 1300/4372, Loss: 2.7124\n",
      "Batch 1300/4372, Loss: 2.7124\n",
      "Batch 1400/4372, Loss: 1.3966\n",
      "Batch 1400/4372, Loss: 1.3966\n",
      "Batch 1500/4372, Loss: 2.4280\n",
      "Batch 1500/4372, Loss: 2.4280\n",
      "Batch 1600/4372, Loss: 3.8346\n",
      "Batch 1600/4372, Loss: 3.8346\n",
      "Batch 1700/4372, Loss: 2.6925\n",
      "Batch 1700/4372, Loss: 2.6925\n",
      "Batch 1800/4372, Loss: 2.6687\n",
      "Batch 1800/4372, Loss: 2.6687\n",
      "Batch 1900/4372, Loss: 6.0809\n",
      "Batch 1900/4372, Loss: 6.0809\n",
      "Batch 2000/4372, Loss: 3.2588\n",
      "Batch 2000/4372, Loss: 3.2588\n",
      "Batch 2100/4372, Loss: 1.3120\n",
      "Batch 2100/4372, Loss: 1.3120\n",
      "Batch 2200/4372, Loss: 1.9739\n",
      "Batch 2200/4372, Loss: 1.9739\n",
      "Batch 2300/4372, Loss: 3.9498\n",
      "Batch 2300/4372, Loss: 3.9498\n",
      "Batch 2400/4372, Loss: 1.1883\n",
      "Batch 2400/4372, Loss: 1.1883\n",
      "Batch 2500/4372, Loss: 2.2177\n",
      "Batch 2500/4372, Loss: 2.2177\n",
      "Batch 2600/4372, Loss: 1.3719\n",
      "Batch 2600/4372, Loss: 1.3719\n",
      "Batch 2700/4372, Loss: 2.5302\n",
      "Batch 2700/4372, Loss: 2.5302\n",
      "Batch 2800/4372, Loss: 2.1711\n",
      "Batch 2800/4372, Loss: 2.1711\n",
      "Batch 2900/4372, Loss: 1.8271\n",
      "Batch 2900/4372, Loss: 1.8271\n",
      "Batch 3000/4372, Loss: 2.2272\n",
      "Batch 3000/4372, Loss: 2.2272\n",
      "Batch 3100/4372, Loss: 7.8775\n",
      "Batch 3100/4372, Loss: 7.8775\n",
      "Batch 3200/4372, Loss: 1.4185\n",
      "Batch 3200/4372, Loss: 1.4185\n",
      "Batch 3300/4372, Loss: 3.0151\n",
      "Batch 3300/4372, Loss: 3.0151\n",
      "Batch 3400/4372, Loss: 2.3171\n",
      "Batch 3400/4372, Loss: 2.3171\n",
      "Batch 3500/4372, Loss: 2.5531\n",
      "Batch 3500/4372, Loss: 2.5531\n",
      "Batch 3600/4372, Loss: 2.0691\n",
      "Batch 3600/4372, Loss: 2.0691\n",
      "Batch 3700/4372, Loss: 2.1455\n",
      "Batch 3700/4372, Loss: 2.1455\n",
      "Batch 3800/4372, Loss: 1.6960\n",
      "Batch 3800/4372, Loss: 1.6960\n",
      "Batch 3900/4372, Loss: 2.1297\n",
      "Batch 3900/4372, Loss: 2.1297\n",
      "Batch 4000/4372, Loss: 2.4800\n",
      "Batch 4000/4372, Loss: 2.4800\n",
      "Batch 4100/4372, Loss: 2.2122\n",
      "Batch 4100/4372, Loss: 2.2122\n",
      "Batch 4200/4372, Loss: 1.7210\n",
      "Batch 4200/4372, Loss: 1.7210\n",
      "Batch 4300/4372, Loss: 1.8683\n",
      "Batch 4300/4372, Loss: 1.8683\n",
      "Train Loss: 2.5833\n",
      "Val Loss: 0.7621, RMSE: 0.8730, MAE: 0.5456, R²: 0.9843\n",
      "\n",
      "Epoch 22/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 1.9336\n",
      "Train Loss: 2.5833\n",
      "Val Loss: 0.7621, RMSE: 0.8730, MAE: 0.5456, R²: 0.9843\n",
      "\n",
      "Epoch 22/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 1.9336\n",
      "Batch 100/4372, Loss: 2.2193\n",
      "Batch 100/4372, Loss: 2.2193\n",
      "Batch 200/4372, Loss: 2.2793\n",
      "Batch 200/4372, Loss: 2.2793\n",
      "Batch 300/4372, Loss: 1.2426\n",
      "Batch 300/4372, Loss: 1.2426\n",
      "Batch 400/4372, Loss: 2.4547\n",
      "Batch 400/4372, Loss: 2.4547\n",
      "Batch 500/4372, Loss: 1.1513\n",
      "Batch 500/4372, Loss: 1.1513\n",
      "Batch 600/4372, Loss: 1.8227\n",
      "Batch 600/4372, Loss: 1.8227\n",
      "Batch 700/4372, Loss: 2.0364\n",
      "Batch 700/4372, Loss: 2.0364\n",
      "Batch 800/4372, Loss: 1.9136\n",
      "Batch 800/4372, Loss: 1.9136\n",
      "Batch 900/4372, Loss: 1.8025\n",
      "Batch 900/4372, Loss: 1.8025\n",
      "Batch 1000/4372, Loss: 1.8791\n",
      "Batch 1000/4372, Loss: 1.8791\n",
      "Batch 1100/4372, Loss: 2.3177\n",
      "Batch 1100/4372, Loss: 2.3177\n",
      "Batch 1200/4372, Loss: 1.5123\n",
      "Batch 1200/4372, Loss: 1.5123\n",
      "Batch 1300/4372, Loss: 2.5097\n",
      "Batch 1300/4372, Loss: 2.5097\n",
      "Batch 1400/4372, Loss: 1.8029\n",
      "Batch 1400/4372, Loss: 1.8029\n",
      "Batch 1500/4372, Loss: 1.3323\n",
      "Batch 1500/4372, Loss: 1.3323\n",
      "Batch 1600/4372, Loss: 1.2816\n",
      "Batch 1600/4372, Loss: 1.2816\n",
      "Batch 1700/4372, Loss: 2.2656\n",
      "Batch 1700/4372, Loss: 2.2656\n",
      "Batch 1800/4372, Loss: 8.2846\n",
      "Batch 1800/4372, Loss: 8.2846\n",
      "Batch 1900/4372, Loss: 1.7842\n",
      "Batch 1900/4372, Loss: 1.7842\n",
      "Batch 2000/4372, Loss: 4.7733\n",
      "Batch 2000/4372, Loss: 4.7733\n",
      "Batch 2100/4372, Loss: 1.9257\n",
      "Batch 2100/4372, Loss: 1.9257\n",
      "Batch 2200/4372, Loss: 3.1296\n",
      "Batch 2200/4372, Loss: 3.1296\n",
      "Batch 2300/4372, Loss: 2.8198\n",
      "Batch 2300/4372, Loss: 2.8198\n",
      "Batch 2400/4372, Loss: 1.5253\n",
      "Batch 2400/4372, Loss: 1.5253\n",
      "Batch 2500/4372, Loss: 2.7535\n",
      "Batch 2500/4372, Loss: 2.7535\n",
      "Batch 2600/4372, Loss: 5.5669\n",
      "Batch 2600/4372, Loss: 5.5669\n",
      "Batch 2700/4372, Loss: 2.0819\n",
      "Batch 2700/4372, Loss: 2.0819\n",
      "Batch 2800/4372, Loss: 3.1056\n",
      "Batch 2800/4372, Loss: 3.1056\n",
      "Batch 2900/4372, Loss: 2.8968\n",
      "Batch 2900/4372, Loss: 2.8968\n",
      "Batch 3000/4372, Loss: 5.9372\n",
      "Batch 3000/4372, Loss: 5.9372\n",
      "Batch 3100/4372, Loss: 1.3853\n",
      "Batch 3100/4372, Loss: 1.3853\n",
      "Batch 3200/4372, Loss: 1.6275\n",
      "Batch 3200/4372, Loss: 1.6275\n",
      "Batch 3300/4372, Loss: 1.8096\n",
      "Batch 3300/4372, Loss: 1.8096\n",
      "Batch 3400/4372, Loss: 1.6598\n",
      "Batch 3400/4372, Loss: 1.6598\n",
      "Batch 3500/4372, Loss: 3.6217\n",
      "Batch 3500/4372, Loss: 3.6217\n",
      "Batch 3600/4372, Loss: 1.0487\n",
      "Batch 3600/4372, Loss: 1.0487\n",
      "Batch 3700/4372, Loss: 1.7131\n",
      "Batch 3700/4372, Loss: 1.7131\n",
      "Batch 3800/4372, Loss: 1.9306\n",
      "Batch 3800/4372, Loss: 1.9306\n",
      "Batch 3900/4372, Loss: 4.7219\n",
      "Batch 3900/4372, Loss: 4.7219\n",
      "Batch 4000/4372, Loss: 2.7523\n",
      "Batch 4000/4372, Loss: 2.7523\n",
      "Batch 4100/4372, Loss: 2.0325\n",
      "Batch 4100/4372, Loss: 2.0325\n",
      "Batch 4200/4372, Loss: 1.7771\n",
      "Batch 4200/4372, Loss: 1.7771\n",
      "Batch 4300/4372, Loss: 1.8826\n",
      "Batch 4300/4372, Loss: 1.8826\n",
      "Train Loss: 2.5738\n",
      "Val Loss: 0.4875, RMSE: 0.6982, MAE: 0.3795, R²: 0.9900\n",
      "✓ New best model saved!\n",
      "\n",
      "Epoch 23/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 1.9892\n",
      "Train Loss: 2.5738\n",
      "Val Loss: 0.4875, RMSE: 0.6982, MAE: 0.3795, R²: 0.9900\n",
      "✓ New best model saved!\n",
      "\n",
      "Epoch 23/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 1.9892\n",
      "Batch 100/4372, Loss: 2.1723\n",
      "Batch 100/4372, Loss: 2.1723\n",
      "Batch 200/4372, Loss: 1.5087\n",
      "Batch 200/4372, Loss: 1.5087\n",
      "Batch 300/4372, Loss: 2.6372\n",
      "Batch 300/4372, Loss: 2.6372\n",
      "Batch 400/4372, Loss: 3.2010\n",
      "Batch 400/4372, Loss: 3.2010\n",
      "Batch 500/4372, Loss: 1.1401\n",
      "Batch 500/4372, Loss: 1.1401\n",
      "Batch 600/4372, Loss: 1.4603\n",
      "Batch 600/4372, Loss: 1.4603\n",
      "Batch 700/4372, Loss: 1.0491\n",
      "Batch 700/4372, Loss: 1.0491\n",
      "Batch 800/4372, Loss: 2.2045\n",
      "Batch 800/4372, Loss: 2.2045\n",
      "Batch 900/4372, Loss: 1.5667\n",
      "Batch 900/4372, Loss: 1.5667\n",
      "Batch 1000/4372, Loss: 1.8399\n",
      "Batch 1000/4372, Loss: 1.8399\n",
      "Batch 1100/4372, Loss: 2.0590\n",
      "Batch 1100/4372, Loss: 2.0590\n",
      "Batch 1200/4372, Loss: 1.5469\n",
      "Batch 1200/4372, Loss: 1.5469\n",
      "Batch 1300/4372, Loss: 6.7181\n",
      "Batch 1300/4372, Loss: 6.7181\n",
      "Batch 1400/4372, Loss: 3.7069\n",
      "Batch 1400/4372, Loss: 3.7069\n",
      "Batch 1500/4372, Loss: 6.3618\n",
      "Batch 1500/4372, Loss: 6.3618\n",
      "Batch 1600/4372, Loss: 1.5296\n",
      "Batch 1600/4372, Loss: 1.5296\n",
      "Batch 1700/4372, Loss: 1.2692\n",
      "Batch 1700/4372, Loss: 1.2692\n",
      "Batch 1800/4372, Loss: 4.0306\n",
      "Batch 1800/4372, Loss: 4.0306\n",
      "Batch 1900/4372, Loss: 2.1294\n",
      "Batch 1900/4372, Loss: 2.1294\n",
      "Batch 2000/4372, Loss: 2.3875\n",
      "Batch 2000/4372, Loss: 2.3875\n",
      "Batch 2100/4372, Loss: 3.3547\n",
      "Batch 2100/4372, Loss: 3.3547\n",
      "Batch 2200/4372, Loss: 1.5309\n",
      "Batch 2200/4372, Loss: 1.5309\n",
      "Batch 2300/4372, Loss: 1.4611\n",
      "Batch 2300/4372, Loss: 1.4611\n",
      "Batch 2400/4372, Loss: 2.7445\n",
      "Batch 2400/4372, Loss: 2.7445\n",
      "Batch 2500/4372, Loss: 1.8115\n",
      "Batch 2500/4372, Loss: 1.8115\n",
      "Batch 2600/4372, Loss: 4.3761\n",
      "Batch 2600/4372, Loss: 4.3761\n",
      "Batch 2700/4372, Loss: 1.5211\n",
      "Batch 2700/4372, Loss: 1.5211\n",
      "Batch 2800/4372, Loss: 4.5916\n",
      "Batch 2800/4372, Loss: 4.5916\n",
      "Batch 2900/4372, Loss: 2.2134\n",
      "Batch 2900/4372, Loss: 2.2134\n",
      "Batch 3000/4372, Loss: 4.1487\n",
      "Batch 3000/4372, Loss: 4.1487\n",
      "Batch 3100/4372, Loss: 2.3013\n",
      "Batch 3100/4372, Loss: 2.3013\n",
      "Batch 3200/4372, Loss: 2.4164\n",
      "Batch 3200/4372, Loss: 2.4164\n",
      "Batch 3300/4372, Loss: 1.9677\n",
      "Batch 3300/4372, Loss: 1.9677\n",
      "Batch 3400/4372, Loss: 1.6691\n",
      "Batch 3400/4372, Loss: 1.6691\n",
      "Batch 3500/4372, Loss: 2.8642\n",
      "Batch 3500/4372, Loss: 2.8642\n",
      "Batch 3600/4372, Loss: 4.3691\n",
      "Batch 3600/4372, Loss: 4.3691\n",
      "Batch 3700/4372, Loss: 2.6434\n",
      "Batch 3700/4372, Loss: 2.6434\n",
      "Batch 3800/4372, Loss: 3.2491\n",
      "Batch 3800/4372, Loss: 3.2491\n",
      "Batch 3900/4372, Loss: 3.1616\n",
      "Batch 3900/4372, Loss: 3.1616\n",
      "Batch 4000/4372, Loss: 3.0505\n",
      "Batch 4000/4372, Loss: 3.0505\n",
      "Batch 4100/4372, Loss: 2.4177\n",
      "Batch 4100/4372, Loss: 2.4177\n",
      "Batch 4200/4372, Loss: 3.2234\n",
      "Batch 4200/4372, Loss: 3.2234\n",
      "Batch 4300/4372, Loss: 1.4981\n",
      "Batch 4300/4372, Loss: 1.4981\n",
      "Train Loss: 2.5383\n",
      "Val Loss: 0.7446, RMSE: 0.8629, MAE: 0.5632, R²: 0.9847\n",
      "\n",
      "Epoch 24/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 1.2605\n",
      "Train Loss: 2.5383\n",
      "Val Loss: 0.7446, RMSE: 0.8629, MAE: 0.5632, R²: 0.9847\n",
      "\n",
      "Epoch 24/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 1.2605\n",
      "Batch 100/4372, Loss: 3.7526\n",
      "Batch 100/4372, Loss: 3.7526\n",
      "Batch 200/4372, Loss: 4.2856\n",
      "Batch 200/4372, Loss: 4.2856\n",
      "Batch 300/4372, Loss: 0.8935\n",
      "Batch 300/4372, Loss: 0.8935\n",
      "Batch 400/4372, Loss: 2.9928\n",
      "Batch 400/4372, Loss: 2.9928\n",
      "Batch 500/4372, Loss: 3.0190\n",
      "Batch 500/4372, Loss: 3.0190\n",
      "Batch 600/4372, Loss: 3.3147\n",
      "Batch 600/4372, Loss: 3.3147\n",
      "Batch 700/4372, Loss: 2.2937\n",
      "Batch 700/4372, Loss: 2.2937\n",
      "Batch 800/4372, Loss: 2.4204\n",
      "Batch 800/4372, Loss: 2.4204\n",
      "Batch 900/4372, Loss: 1.6464\n",
      "Batch 900/4372, Loss: 1.6464\n",
      "Batch 1000/4372, Loss: 2.3969\n",
      "Batch 1000/4372, Loss: 2.3969\n",
      "Batch 1100/4372, Loss: 2.4403\n",
      "Batch 1100/4372, Loss: 2.4403\n",
      "Batch 1200/4372, Loss: 1.5252\n",
      "Batch 1200/4372, Loss: 1.5252\n",
      "Batch 1300/4372, Loss: 2.8719\n",
      "Batch 1300/4372, Loss: 2.8719\n",
      "Batch 1400/4372, Loss: 2.0689\n",
      "Batch 1400/4372, Loss: 2.0689\n",
      "Batch 1500/4372, Loss: 1.8321\n",
      "Batch 1500/4372, Loss: 1.8321\n",
      "Batch 1600/4372, Loss: 2.1935\n",
      "Batch 1600/4372, Loss: 2.1935\n",
      "Batch 1700/4372, Loss: 1.8994\n",
      "Batch 1700/4372, Loss: 1.8994\n",
      "Batch 1800/4372, Loss: 1.3122\n",
      "Batch 1800/4372, Loss: 1.3122\n",
      "Batch 1900/4372, Loss: 2.7746\n",
      "Batch 1900/4372, Loss: 2.7746\n",
      "Batch 2000/4372, Loss: 1.8344\n",
      "Batch 2000/4372, Loss: 1.8344\n",
      "Batch 2100/4372, Loss: 5.0871\n",
      "Batch 2100/4372, Loss: 5.0871\n",
      "Batch 2200/4372, Loss: 3.1728\n",
      "Batch 2200/4372, Loss: 3.1728\n",
      "Batch 2300/4372, Loss: 3.6245\n",
      "Batch 2300/4372, Loss: 3.6245\n",
      "Batch 2400/4372, Loss: 3.5207\n",
      "Batch 2400/4372, Loss: 3.5207\n",
      "Batch 2500/4372, Loss: 2.5995\n",
      "Batch 2500/4372, Loss: 2.5995\n",
      "Batch 2600/4372, Loss: 1.6545\n",
      "Batch 2600/4372, Loss: 1.6545\n",
      "Batch 2700/4372, Loss: 3.5824\n",
      "Batch 2700/4372, Loss: 3.5824\n",
      "Batch 2800/4372, Loss: 1.7745\n",
      "Batch 2800/4372, Loss: 1.7745\n",
      "Batch 2900/4372, Loss: 2.9083\n",
      "Batch 2900/4372, Loss: 2.9083\n",
      "Batch 3000/4372, Loss: 1.3376\n",
      "Batch 3000/4372, Loss: 1.3376\n",
      "Batch 3100/4372, Loss: 1.7936\n",
      "Batch 3100/4372, Loss: 1.7936\n",
      "Batch 3200/4372, Loss: 1.5372\n",
      "Batch 3200/4372, Loss: 1.5372\n",
      "Batch 3300/4372, Loss: 2.1916\n",
      "Batch 3300/4372, Loss: 2.1916\n",
      "Batch 3400/4372, Loss: 1.7057\n",
      "Batch 3400/4372, Loss: 1.7057\n",
      "Batch 3500/4372, Loss: 1.8812\n",
      "Batch 3500/4372, Loss: 1.8812\n",
      "Batch 3600/4372, Loss: 3.0191\n",
      "Batch 3600/4372, Loss: 3.0191\n",
      "Batch 3700/4372, Loss: 3.4096\n",
      "Batch 3700/4372, Loss: 3.4096\n",
      "Batch 3800/4372, Loss: 4.3707\n",
      "Batch 3800/4372, Loss: 4.3707\n",
      "Batch 3900/4372, Loss: 1.4201\n",
      "Batch 3900/4372, Loss: 1.4201\n",
      "Batch 4000/4372, Loss: 2.3891\n",
      "Batch 4000/4372, Loss: 2.3891\n",
      "Batch 4100/4372, Loss: 1.5598\n",
      "Batch 4100/4372, Loss: 1.5598\n",
      "Batch 4200/4372, Loss: 1.4804\n",
      "Batch 4200/4372, Loss: 1.4804\n",
      "Batch 4300/4372, Loss: 1.9036\n",
      "Batch 4300/4372, Loss: 1.9036\n",
      "Train Loss: 2.5243\n",
      "Val Loss: 0.5784, RMSE: 0.7606, MAE: 0.4132, R²: 0.9881\n",
      "\n",
      "Epoch 25/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 3.0045\n",
      "Train Loss: 2.5243\n",
      "Val Loss: 0.5784, RMSE: 0.7606, MAE: 0.4132, R²: 0.9881\n",
      "\n",
      "Epoch 25/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 3.0045\n",
      "Batch 100/4372, Loss: 2.9846\n",
      "Batch 100/4372, Loss: 2.9846\n",
      "Batch 200/4372, Loss: 3.9249\n",
      "Batch 200/4372, Loss: 3.9249\n",
      "Batch 300/4372, Loss: 2.2795\n",
      "Batch 300/4372, Loss: 2.2795\n",
      "Batch 400/4372, Loss: 1.6833\n",
      "Batch 400/4372, Loss: 1.6833\n",
      "Batch 500/4372, Loss: 1.3531\n",
      "Batch 500/4372, Loss: 1.3531\n",
      "Batch 600/4372, Loss: 1.6473\n",
      "Batch 600/4372, Loss: 1.6473\n",
      "Batch 700/4372, Loss: 2.0816\n",
      "Batch 700/4372, Loss: 2.0816\n",
      "Batch 800/4372, Loss: 2.5397\n",
      "Batch 800/4372, Loss: 2.5397\n",
      "Batch 900/4372, Loss: 1.9006\n",
      "Batch 900/4372, Loss: 1.9006\n",
      "Batch 1000/4372, Loss: 1.7852\n",
      "Batch 1000/4372, Loss: 1.7852\n",
      "Batch 1100/4372, Loss: 3.5011\n",
      "Batch 1100/4372, Loss: 3.5011\n",
      "Batch 1200/4372, Loss: 1.6718\n",
      "Batch 1200/4372, Loss: 1.6718\n",
      "Batch 1300/4372, Loss: 1.8274\n",
      "Batch 1300/4372, Loss: 1.8274\n",
      "Batch 1400/4372, Loss: 4.3029\n",
      "Batch 1400/4372, Loss: 4.3029\n",
      "Batch 1500/4372, Loss: 2.1285\n",
      "Batch 1500/4372, Loss: 2.1285\n",
      "Batch 1600/4372, Loss: 2.1564\n",
      "Batch 1600/4372, Loss: 2.1564\n",
      "Batch 1700/4372, Loss: 1.1528\n",
      "Batch 1700/4372, Loss: 1.1528\n",
      "Batch 1800/4372, Loss: 1.3167\n",
      "Batch 1800/4372, Loss: 1.3167\n",
      "Batch 1900/4372, Loss: 2.5697\n",
      "Batch 1900/4372, Loss: 2.5697\n",
      "Batch 2000/4372, Loss: 2.2020\n",
      "Batch 2000/4372, Loss: 2.2020\n",
      "Batch 2100/4372, Loss: 2.2864\n",
      "Batch 2100/4372, Loss: 2.2864\n",
      "Batch 2200/4372, Loss: 2.3829\n",
      "Batch 2200/4372, Loss: 2.3829\n",
      "Batch 2300/4372, Loss: 2.0270\n",
      "Batch 2300/4372, Loss: 2.0270\n",
      "Batch 2400/4372, Loss: 1.6085\n",
      "Batch 2400/4372, Loss: 1.6085\n",
      "Batch 2500/4372, Loss: 1.8094\n",
      "Batch 2500/4372, Loss: 1.8094\n",
      "Batch 2600/4372, Loss: 1.1203\n",
      "Batch 2600/4372, Loss: 1.1203\n",
      "Batch 2700/4372, Loss: 2.2009\n",
      "Batch 2700/4372, Loss: 2.2009\n",
      "Batch 2800/4372, Loss: 1.0367\n",
      "Batch 2800/4372, Loss: 1.0367\n",
      "Batch 2900/4372, Loss: 2.0613\n",
      "Batch 2900/4372, Loss: 2.0613\n",
      "Batch 3000/4372, Loss: 1.7185\n",
      "Batch 3000/4372, Loss: 1.7185\n",
      "Batch 3100/4372, Loss: 1.8365\n",
      "Batch 3100/4372, Loss: 1.8365\n",
      "Batch 3200/4372, Loss: 1.7925\n",
      "Batch 3200/4372, Loss: 1.7925\n",
      "Batch 3300/4372, Loss: 1.4311\n",
      "Batch 3300/4372, Loss: 1.4311\n",
      "Batch 3400/4372, Loss: 2.6399\n",
      "Batch 3400/4372, Loss: 2.6399\n",
      "Batch 3500/4372, Loss: 1.1358\n",
      "Batch 3500/4372, Loss: 1.1358\n",
      "Batch 3600/4372, Loss: 1.6585\n",
      "Batch 3600/4372, Loss: 1.6585\n",
      "Batch 3700/4372, Loss: 2.3386\n",
      "Batch 3700/4372, Loss: 2.3386\n",
      "Batch 3800/4372, Loss: 3.2798\n",
      "Batch 3800/4372, Loss: 3.2798\n",
      "Batch 3900/4372, Loss: 1.5681\n",
      "Batch 3900/4372, Loss: 1.5681\n",
      "Batch 4000/4372, Loss: 2.0843\n",
      "Batch 4000/4372, Loss: 2.0843\n",
      "Batch 4100/4372, Loss: 1.6734\n",
      "Batch 4100/4372, Loss: 1.6734\n",
      "Batch 4200/4372, Loss: 1.8375\n",
      "Batch 4200/4372, Loss: 1.8375\n",
      "Batch 4300/4372, Loss: 2.6341\n",
      "Batch 4300/4372, Loss: 2.6341\n",
      "Train Loss: 2.5206\n",
      "Val Loss: 0.5336, RMSE: 0.7305, MAE: 0.3990, R²: 0.9890\n",
      "\n",
      "Epoch 26/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 4.9801\n",
      "Train Loss: 2.5206\n",
      "Val Loss: 0.5336, RMSE: 0.7305, MAE: 0.3990, R²: 0.9890\n",
      "\n",
      "Epoch 26/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 4.9801\n",
      "Batch 100/4372, Loss: 1.7487\n",
      "Batch 100/4372, Loss: 1.7487\n",
      "Batch 200/4372, Loss: 1.9453\n",
      "Batch 200/4372, Loss: 1.9453\n",
      "Batch 300/4372, Loss: 2.6449\n",
      "Batch 300/4372, Loss: 2.6449\n",
      "Batch 400/4372, Loss: 1.1511\n",
      "Batch 400/4372, Loss: 1.1511\n",
      "Batch 500/4372, Loss: 2.1571\n",
      "Batch 500/4372, Loss: 2.1571\n",
      "Batch 600/4372, Loss: 2.1128\n",
      "Batch 600/4372, Loss: 2.1128\n",
      "Batch 700/4372, Loss: 1.0339\n",
      "Batch 700/4372, Loss: 1.0339\n",
      "Batch 800/4372, Loss: 2.2395\n",
      "Batch 800/4372, Loss: 2.2395\n",
      "Batch 900/4372, Loss: 1.4457\n",
      "Batch 900/4372, Loss: 1.4457\n",
      "Batch 1000/4372, Loss: 1.9945\n",
      "Batch 1000/4372, Loss: 1.9945\n",
      "Batch 1100/4372, Loss: 3.6786\n",
      "Batch 1100/4372, Loss: 3.6786\n",
      "Batch 1200/4372, Loss: 1.6423\n",
      "Batch 1200/4372, Loss: 1.6423\n",
      "Batch 1300/4372, Loss: 2.1805\n",
      "Batch 1300/4372, Loss: 2.1805\n",
      "Batch 1400/4372, Loss: 1.5642\n",
      "Batch 1400/4372, Loss: 1.5642\n",
      "Batch 1500/4372, Loss: 3.0286\n",
      "Batch 1500/4372, Loss: 3.0286\n",
      "Batch 1600/4372, Loss: 2.0446\n",
      "Batch 1600/4372, Loss: 2.0446\n",
      "Batch 1700/4372, Loss: 1.5828\n",
      "Batch 1700/4372, Loss: 1.5828\n",
      "Batch 1800/4372, Loss: 2.6038\n",
      "Batch 1800/4372, Loss: 2.6038\n",
      "Batch 1900/4372, Loss: 1.3446\n",
      "Batch 1900/4372, Loss: 1.3446\n",
      "Batch 2000/4372, Loss: 1.8125\n",
      "Batch 2000/4372, Loss: 1.8125\n",
      "Batch 2100/4372, Loss: 1.4783\n",
      "Batch 2100/4372, Loss: 1.4783\n",
      "Batch 2200/4372, Loss: 2.1613\n",
      "Batch 2200/4372, Loss: 2.1613\n",
      "Batch 2300/4372, Loss: 1.6101\n",
      "Batch 2300/4372, Loss: 1.6101\n",
      "Batch 2400/4372, Loss: 2.0428\n",
      "Batch 2400/4372, Loss: 2.0428\n",
      "Batch 2500/4372, Loss: 2.9608\n",
      "Batch 2500/4372, Loss: 2.9608\n",
      "Batch 2600/4372, Loss: 2.3644\n",
      "Batch 2600/4372, Loss: 2.3644\n",
      "Batch 2700/4372, Loss: 5.1189\n",
      "Batch 2700/4372, Loss: 5.1189\n",
      "Batch 2800/4372, Loss: 2.2075\n",
      "Batch 2800/4372, Loss: 2.2075\n",
      "Batch 2900/4372, Loss: 2.9704\n",
      "Batch 2900/4372, Loss: 2.9704\n",
      "Batch 3000/4372, Loss: 1.4655\n",
      "Batch 3000/4372, Loss: 1.4655\n",
      "Batch 3100/4372, Loss: 1.4008\n",
      "Batch 3100/4372, Loss: 1.4008\n",
      "Batch 3200/4372, Loss: 2.1545\n",
      "Batch 3200/4372, Loss: 2.1545\n",
      "Batch 3300/4372, Loss: 1.8191\n",
      "Batch 3300/4372, Loss: 1.8191\n",
      "Batch 3400/4372, Loss: 2.3544\n",
      "Batch 3400/4372, Loss: 2.3544\n",
      "Batch 3500/4372, Loss: 1.4433\n",
      "Batch 3500/4372, Loss: 1.4433\n",
      "Batch 3600/4372, Loss: 8.3233\n",
      "Batch 3600/4372, Loss: 8.3233\n",
      "Batch 3700/4372, Loss: 2.1410\n",
      "Batch 3700/4372, Loss: 2.1410\n",
      "Batch 3800/4372, Loss: 1.1451\n",
      "Batch 3800/4372, Loss: 1.1451\n",
      "Batch 3900/4372, Loss: 3.1450\n",
      "Batch 3900/4372, Loss: 3.1450\n",
      "Batch 4000/4372, Loss: 2.2473\n",
      "Batch 4000/4372, Loss: 2.2473\n",
      "Batch 4100/4372, Loss: 2.5036\n",
      "Batch 4100/4372, Loss: 2.5036\n",
      "Batch 4200/4372, Loss: 2.5374\n",
      "Batch 4200/4372, Loss: 2.5374\n",
      "Batch 4300/4372, Loss: 2.8334\n",
      "Batch 4300/4372, Loss: 2.8334\n",
      "Train Loss: 2.4689\n",
      "Val Loss: 0.5822, RMSE: 0.7630, MAE: 0.4452, R²: 0.9880\n",
      "\n",
      "Epoch 27/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 5.1059\n",
      "Train Loss: 2.4689\n",
      "Val Loss: 0.5822, RMSE: 0.7630, MAE: 0.4452, R²: 0.9880\n",
      "\n",
      "Epoch 27/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 5.1059\n",
      "Batch 100/4372, Loss: 1.2579\n",
      "Batch 100/4372, Loss: 1.2579\n",
      "Batch 200/4372, Loss: 1.7308\n",
      "Batch 200/4372, Loss: 1.7308\n",
      "Batch 300/4372, Loss: 1.2562\n",
      "Batch 300/4372, Loss: 1.2562\n",
      "Batch 400/4372, Loss: 3.2043\n",
      "Batch 400/4372, Loss: 3.2043\n",
      "Batch 500/4372, Loss: 1.2263\n",
      "Batch 500/4372, Loss: 1.2263\n",
      "Batch 600/4372, Loss: 4.3670\n",
      "Batch 600/4372, Loss: 4.3670\n",
      "Batch 700/4372, Loss: 2.6446\n",
      "Batch 700/4372, Loss: 2.6446\n",
      "Batch 800/4372, Loss: 0.8743\n",
      "Batch 800/4372, Loss: 0.8743\n",
      "Batch 900/4372, Loss: 4.7699\n",
      "Batch 900/4372, Loss: 4.7699\n",
      "Batch 1000/4372, Loss: 2.5102\n",
      "Batch 1000/4372, Loss: 2.5102\n",
      "Batch 1100/4372, Loss: 3.3308\n",
      "Batch 1100/4372, Loss: 3.3308\n",
      "Batch 1200/4372, Loss: 1.7910\n",
      "Batch 1200/4372, Loss: 1.7910\n",
      "Batch 1300/4372, Loss: 1.7387\n",
      "Batch 1300/4372, Loss: 1.7387\n",
      "Batch 1400/4372, Loss: 3.6161\n",
      "Batch 1400/4372, Loss: 3.6161\n",
      "Batch 1500/4372, Loss: 1.6339\n",
      "Batch 1500/4372, Loss: 1.6339\n",
      "Batch 1600/4372, Loss: 4.5988\n",
      "Batch 1600/4372, Loss: 4.5988\n",
      "Batch 1700/4372, Loss: 2.0992\n",
      "Batch 1700/4372, Loss: 2.0992\n",
      "Batch 1800/4372, Loss: 2.0481\n",
      "Batch 1800/4372, Loss: 2.0481\n",
      "Batch 1900/4372, Loss: 2.2623\n",
      "Batch 1900/4372, Loss: 2.2623\n",
      "Batch 2000/4372, Loss: 1.3452\n",
      "Batch 2000/4372, Loss: 1.3452\n",
      "Batch 2100/4372, Loss: 1.4593\n",
      "Batch 2100/4372, Loss: 1.4593\n",
      "Batch 2200/4372, Loss: 2.5002\n",
      "Batch 2200/4372, Loss: 2.5002\n",
      "Batch 2300/4372, Loss: 2.6013\n",
      "Batch 2300/4372, Loss: 2.6013\n",
      "Batch 2400/4372, Loss: 2.2932\n",
      "Batch 2400/4372, Loss: 2.2932\n",
      "Batch 2500/4372, Loss: 3.2372\n",
      "Batch 2500/4372, Loss: 3.2372\n",
      "Batch 2600/4372, Loss: 3.1473\n",
      "Batch 2600/4372, Loss: 3.1473\n",
      "Batch 2700/4372, Loss: 2.1680\n",
      "Batch 2700/4372, Loss: 2.1680\n",
      "Batch 2800/4372, Loss: 1.7040\n",
      "Batch 2800/4372, Loss: 1.7040\n",
      "Batch 2900/4372, Loss: 1.5751\n",
      "Batch 2900/4372, Loss: 1.5751\n",
      "Batch 3000/4372, Loss: 1.4210\n",
      "Batch 3000/4372, Loss: 1.4210\n",
      "Batch 3100/4372, Loss: 2.7372\n",
      "Batch 3100/4372, Loss: 2.7372\n",
      "Batch 3200/4372, Loss: 1.2584\n",
      "Batch 3200/4372, Loss: 1.2584\n",
      "Batch 3300/4372, Loss: 4.6270\n",
      "Batch 3300/4372, Loss: 4.6270\n",
      "Batch 3400/4372, Loss: 1.1716\n",
      "Batch 3400/4372, Loss: 1.1716\n",
      "Batch 3500/4372, Loss: 2.9655\n",
      "Batch 3500/4372, Loss: 2.9655\n",
      "Batch 3600/4372, Loss: 1.8150\n",
      "Batch 3600/4372, Loss: 1.8150\n",
      "Batch 3700/4372, Loss: 2.1319\n",
      "Batch 3700/4372, Loss: 2.1319\n",
      "Batch 3800/4372, Loss: 4.6199\n",
      "Batch 3800/4372, Loss: 4.6199\n",
      "Batch 3900/4372, Loss: 1.8612\n",
      "Batch 3900/4372, Loss: 1.8612\n",
      "Batch 4000/4372, Loss: 3.4827\n",
      "Batch 4000/4372, Loss: 3.4827\n",
      "Batch 4100/4372, Loss: 2.1737\n",
      "Batch 4100/4372, Loss: 2.1737\n",
      "Batch 4200/4372, Loss: 3.0571\n",
      "Batch 4200/4372, Loss: 3.0571\n",
      "Batch 4300/4372, Loss: 1.8454\n",
      "Batch 4300/4372, Loss: 1.8454\n",
      "Train Loss: 2.4588\n",
      "Val Loss: 0.6739, RMSE: 0.8209, MAE: 0.5179, R²: 0.9861\n",
      "\n",
      "Epoch 28/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 4.7290\n",
      "Train Loss: 2.4588\n",
      "Val Loss: 0.6739, RMSE: 0.8209, MAE: 0.5179, R²: 0.9861\n",
      "\n",
      "Epoch 28/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 4.7290\n",
      "Batch 100/4372, Loss: 3.8382\n",
      "Batch 100/4372, Loss: 3.8382\n",
      "Batch 200/4372, Loss: 1.0986\n",
      "Batch 200/4372, Loss: 1.0986\n",
      "Batch 300/4372, Loss: 3.1716\n",
      "Batch 300/4372, Loss: 3.1716\n",
      "Batch 400/4372, Loss: 1.4395\n",
      "Batch 400/4372, Loss: 1.4395\n",
      "Batch 500/4372, Loss: 4.3209\n",
      "Batch 500/4372, Loss: 4.3209\n",
      "Batch 600/4372, Loss: 1.1088\n",
      "Batch 600/4372, Loss: 1.1088\n",
      "Batch 700/4372, Loss: 4.8898\n",
      "Batch 700/4372, Loss: 4.8898\n",
      "Batch 800/4372, Loss: 1.9036\n",
      "Batch 800/4372, Loss: 1.9036\n",
      "Batch 900/4372, Loss: 1.8721\n",
      "Batch 900/4372, Loss: 1.8721\n",
      "Batch 1000/4372, Loss: 3.3773\n",
      "Batch 1000/4372, Loss: 3.3773\n",
      "Batch 1100/4372, Loss: 1.7094\n",
      "Batch 1100/4372, Loss: 1.7094\n",
      "Batch 1200/4372, Loss: 5.9564\n",
      "Batch 1200/4372, Loss: 5.9564\n",
      "Batch 1300/4372, Loss: 1.7449\n",
      "Batch 1300/4372, Loss: 1.7449\n",
      "Batch 1400/4372, Loss: 3.3240\n",
      "Batch 1400/4372, Loss: 3.3240\n",
      "Batch 1500/4372, Loss: 2.0451\n",
      "Batch 1500/4372, Loss: 2.0451\n",
      "Batch 1600/4372, Loss: 2.1062\n",
      "Batch 1600/4372, Loss: 2.1062\n",
      "Batch 1700/4372, Loss: 2.2378\n",
      "Batch 1700/4372, Loss: 2.2378\n",
      "Batch 1800/4372, Loss: 2.7376\n",
      "Batch 1800/4372, Loss: 2.7376\n",
      "Batch 1900/4372, Loss: 3.3061\n",
      "Batch 1900/4372, Loss: 3.3061\n",
      "Batch 2000/4372, Loss: 1.2049\n",
      "Batch 2000/4372, Loss: 1.2049\n",
      "Batch 2100/4372, Loss: 1.8389\n",
      "Batch 2100/4372, Loss: 1.8389\n",
      "Batch 2200/4372, Loss: 2.1856\n",
      "Batch 2200/4372, Loss: 2.1856\n",
      "Batch 2300/4372, Loss: 2.2441\n",
      "Batch 2300/4372, Loss: 2.2441\n",
      "Batch 2400/4372, Loss: 1.2156\n",
      "Batch 2400/4372, Loss: 1.2156\n",
      "Batch 2500/4372, Loss: 1.6579\n",
      "Batch 2500/4372, Loss: 1.6579\n",
      "Batch 2600/4372, Loss: 3.9603\n",
      "Batch 2600/4372, Loss: 3.9603\n",
      "Batch 2700/4372, Loss: 1.4225\n",
      "Batch 2700/4372, Loss: 1.4225\n",
      "Batch 2800/4372, Loss: 3.1888\n",
      "Batch 2800/4372, Loss: 3.1888\n",
      "Batch 2900/4372, Loss: 4.5707\n",
      "Batch 2900/4372, Loss: 4.5707\n",
      "Batch 3000/4372, Loss: 1.2667\n",
      "Batch 3000/4372, Loss: 1.2667\n",
      "Batch 3100/4372, Loss: 1.2095\n",
      "Batch 3100/4372, Loss: 1.2095\n",
      "Batch 3200/4372, Loss: 1.2433\n",
      "Batch 3200/4372, Loss: 1.2433\n",
      "Batch 3300/4372, Loss: 1.1729\n",
      "Batch 3300/4372, Loss: 1.1729\n",
      "Batch 3400/4372, Loss: 2.1910\n",
      "Batch 3400/4372, Loss: 2.1910\n",
      "Batch 3500/4372, Loss: 1.4962\n",
      "Batch 3500/4372, Loss: 1.4962\n",
      "Batch 3600/4372, Loss: 2.9725\n",
      "Batch 3600/4372, Loss: 2.9725\n",
      "Batch 3700/4372, Loss: 1.7712\n",
      "Batch 3700/4372, Loss: 1.7712\n",
      "Batch 3800/4372, Loss: 1.9581\n",
      "Batch 3800/4372, Loss: 1.9581\n",
      "Batch 3900/4372, Loss: 4.9939\n",
      "Batch 3900/4372, Loss: 4.9939\n",
      "Batch 4000/4372, Loss: 1.4257\n",
      "Batch 4000/4372, Loss: 1.4257\n",
      "Batch 4100/4372, Loss: 1.5839\n",
      "Batch 4100/4372, Loss: 1.5839\n",
      "Batch 4200/4372, Loss: 2.3135\n",
      "Batch 4200/4372, Loss: 2.3135\n",
      "Batch 4300/4372, Loss: 2.3892\n",
      "Batch 4300/4372, Loss: 2.3892\n",
      "Train Loss: 2.3970\n",
      "Val Loss: 0.7409, RMSE: 0.8608, MAE: 0.5711, R²: 0.9848\n",
      "\n",
      "Epoch 29/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 1.7968\n",
      "Train Loss: 2.3970\n",
      "Val Loss: 0.7409, RMSE: 0.8608, MAE: 0.5711, R²: 0.9848\n",
      "\n",
      "Epoch 29/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 1.7968\n",
      "Batch 100/4372, Loss: 1.2747\n",
      "Batch 100/4372, Loss: 1.2747\n",
      "Batch 200/4372, Loss: 1.9762\n",
      "Batch 200/4372, Loss: 1.9762\n",
      "Batch 300/4372, Loss: 1.0114\n",
      "Batch 300/4372, Loss: 1.0114\n",
      "Batch 400/4372, Loss: 1.8188\n",
      "Batch 400/4372, Loss: 1.8188\n",
      "Batch 500/4372, Loss: 1.2113\n",
      "Batch 500/4372, Loss: 1.2113\n",
      "Batch 600/4372, Loss: 3.1222\n",
      "Batch 600/4372, Loss: 3.1222\n",
      "Batch 700/4372, Loss: 2.2267\n",
      "Batch 700/4372, Loss: 2.2267\n",
      "Batch 800/4372, Loss: 1.9022\n",
      "Batch 800/4372, Loss: 1.9022\n",
      "Batch 900/4372, Loss: 1.3705\n",
      "Batch 900/4372, Loss: 1.3705\n",
      "Batch 1000/4372, Loss: 1.1494\n",
      "Batch 1000/4372, Loss: 1.1494\n",
      "Batch 1100/4372, Loss: 1.7960\n",
      "Batch 1100/4372, Loss: 1.7960\n",
      "Batch 1200/4372, Loss: 1.0568\n",
      "Batch 1200/4372, Loss: 1.0568\n",
      "Batch 1300/4372, Loss: 2.3419\n",
      "Batch 1300/4372, Loss: 2.3419\n",
      "Batch 1400/4372, Loss: 1.1787\n",
      "Batch 1400/4372, Loss: 1.1787\n",
      "Batch 1500/4372, Loss: 1.5829\n",
      "Batch 1500/4372, Loss: 1.5829\n",
      "Batch 1600/4372, Loss: 3.6041\n",
      "Batch 1600/4372, Loss: 3.6041\n",
      "Batch 1700/4372, Loss: 1.5256\n",
      "Batch 1700/4372, Loss: 1.5256\n",
      "Batch 1800/4372, Loss: 1.2807\n",
      "Batch 1800/4372, Loss: 1.2807\n",
      "Batch 1900/4372, Loss: 4.8068\n",
      "Batch 1900/4372, Loss: 4.8068\n",
      "Batch 2000/4372, Loss: 2.7972\n",
      "Batch 2000/4372, Loss: 2.7972\n",
      "Batch 2100/4372, Loss: 1.3365\n",
      "Batch 2100/4372, Loss: 1.3365\n",
      "Batch 2200/4372, Loss: 1.4976\n",
      "Batch 2200/4372, Loss: 1.4976\n",
      "Batch 2300/4372, Loss: 1.5482\n",
      "Batch 2300/4372, Loss: 1.5482\n",
      "Batch 2400/4372, Loss: 2.4199\n",
      "Batch 2400/4372, Loss: 2.4199\n",
      "Batch 2500/4372, Loss: 2.0035\n",
      "Batch 2500/4372, Loss: 2.0035\n",
      "Batch 2600/4372, Loss: 4.8596\n",
      "Batch 2600/4372, Loss: 4.8596\n",
      "Batch 2700/4372, Loss: 1.4252\n",
      "Batch 2700/4372, Loss: 1.4252\n",
      "Batch 2800/4372, Loss: 2.0536\n",
      "Batch 2800/4372, Loss: 2.0536\n",
      "Batch 2900/4372, Loss: 3.3985\n",
      "Batch 2900/4372, Loss: 3.3985\n",
      "Batch 3000/4372, Loss: 1.6254\n",
      "Batch 3000/4372, Loss: 1.6254\n",
      "Batch 3100/4372, Loss: 1.9451\n",
      "Batch 3100/4372, Loss: 1.9451\n",
      "Batch 3200/4372, Loss: 1.5404\n",
      "Batch 3200/4372, Loss: 1.5404\n",
      "Batch 3300/4372, Loss: 2.2663\n",
      "Batch 3300/4372, Loss: 2.2663\n",
      "Batch 3400/4372, Loss: 1.2894\n",
      "Batch 3400/4372, Loss: 1.2894\n",
      "Batch 3500/4372, Loss: 3.6584\n",
      "Batch 3500/4372, Loss: 3.6584\n",
      "Batch 3600/4372, Loss: 1.9424\n",
      "Batch 3600/4372, Loss: 1.9424\n",
      "Batch 3700/4372, Loss: 2.0700\n",
      "Batch 3700/4372, Loss: 2.0700\n",
      "Batch 3800/4372, Loss: 2.8956\n",
      "Batch 3800/4372, Loss: 2.8956\n",
      "Batch 3900/4372, Loss: 1.1567\n",
      "Batch 3900/4372, Loss: 1.1567\n",
      "Batch 4000/4372, Loss: 2.1391\n",
      "Batch 4000/4372, Loss: 2.1391\n",
      "Batch 4100/4372, Loss: 1.4825\n",
      "Batch 4100/4372, Loss: 1.4825\n",
      "Batch 4200/4372, Loss: 0.9577\n",
      "Batch 4200/4372, Loss: 0.9577\n",
      "Batch 4300/4372, Loss: 1.9955\n",
      "Batch 4300/4372, Loss: 1.9955\n",
      "Train Loss: 2.3997\n",
      "Val Loss: 0.7253, RMSE: 0.8516, MAE: 0.5432, R²: 0.9851\n",
      "\n",
      "Epoch 30/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 1.5506\n",
      "Train Loss: 2.3997\n",
      "Val Loss: 0.7253, RMSE: 0.8516, MAE: 0.5432, R²: 0.9851\n",
      "\n",
      "Epoch 30/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 1.5506\n",
      "Batch 100/4372, Loss: 4.6947\n",
      "Batch 100/4372, Loss: 4.6947\n",
      "Batch 200/4372, Loss: 1.2710\n",
      "Batch 200/4372, Loss: 1.2710\n",
      "Batch 300/4372, Loss: 1.8095\n",
      "Batch 300/4372, Loss: 1.8095\n",
      "Batch 400/4372, Loss: 4.3927\n",
      "Batch 400/4372, Loss: 4.3927\n",
      "Batch 500/4372, Loss: 3.8178\n",
      "Batch 500/4372, Loss: 3.8178\n",
      "Batch 600/4372, Loss: 2.4289\n",
      "Batch 600/4372, Loss: 2.4289\n",
      "Batch 700/4372, Loss: 1.1730\n",
      "Batch 700/4372, Loss: 1.1730\n",
      "Batch 800/4372, Loss: 5.3756\n",
      "Batch 800/4372, Loss: 5.3756\n",
      "Batch 900/4372, Loss: 1.7071\n",
      "Batch 900/4372, Loss: 1.7071\n",
      "Batch 1000/4372, Loss: 4.9949\n",
      "Batch 1000/4372, Loss: 4.9949\n",
      "Batch 1100/4372, Loss: 2.1785\n",
      "Batch 1100/4372, Loss: 2.1785\n",
      "Batch 1200/4372, Loss: 3.1347\n",
      "Batch 1200/4372, Loss: 3.1347\n",
      "Batch 1300/4372, Loss: 2.5209\n",
      "Batch 1300/4372, Loss: 2.5209\n",
      "Batch 1400/4372, Loss: 1.4578\n",
      "Batch 1400/4372, Loss: 1.4578\n",
      "Batch 1500/4372, Loss: 2.9433\n",
      "Batch 1500/4372, Loss: 2.9433\n",
      "Batch 1600/4372, Loss: 5.3909\n",
      "Batch 1600/4372, Loss: 5.3909\n",
      "Batch 1700/4372, Loss: 1.5439\n",
      "Batch 1700/4372, Loss: 1.5439\n",
      "Batch 1800/4372, Loss: 1.6514\n",
      "Batch 1800/4372, Loss: 1.6514\n",
      "Batch 1900/4372, Loss: 1.3676\n",
      "Batch 1900/4372, Loss: 1.3676\n",
      "Batch 2000/4372, Loss: 2.8345\n",
      "Batch 2000/4372, Loss: 2.8345\n",
      "Batch 2100/4372, Loss: 1.8394\n",
      "Batch 2100/4372, Loss: 1.8394\n",
      "Batch 2200/4372, Loss: 3.9615\n",
      "Batch 2200/4372, Loss: 3.9615\n",
      "Batch 2300/4372, Loss: 0.9402\n",
      "Batch 2300/4372, Loss: 0.9402\n",
      "Batch 2400/4372, Loss: 2.0879\n",
      "Batch 2400/4372, Loss: 2.0879\n",
      "Batch 2500/4372, Loss: 1.6084\n",
      "Batch 2500/4372, Loss: 1.6084\n",
      "Batch 2600/4372, Loss: 2.5815\n",
      "Batch 2600/4372, Loss: 2.5815\n",
      "Batch 2700/4372, Loss: 1.2244\n",
      "Batch 2700/4372, Loss: 1.2244\n",
      "Batch 2800/4372, Loss: 3.4700\n",
      "Batch 2800/4372, Loss: 3.4700\n",
      "Batch 2900/4372, Loss: 1.3396\n",
      "Batch 2900/4372, Loss: 1.3396\n",
      "Batch 3000/4372, Loss: 1.9402\n",
      "Batch 3000/4372, Loss: 1.9402\n",
      "Batch 3100/4372, Loss: 2.3676\n",
      "Batch 3100/4372, Loss: 2.3676\n",
      "Batch 3200/4372, Loss: 2.1960\n",
      "Batch 3200/4372, Loss: 2.1960\n",
      "Batch 3300/4372, Loss: 5.3834\n",
      "Batch 3300/4372, Loss: 5.3834\n",
      "Batch 3400/4372, Loss: 2.4481\n",
      "Batch 3400/4372, Loss: 2.4481\n",
      "Batch 3500/4372, Loss: 2.7356\n",
      "Batch 3500/4372, Loss: 2.7356\n",
      "Batch 3600/4372, Loss: 2.6527\n",
      "Batch 3600/4372, Loss: 2.6527\n",
      "Batch 3700/4372, Loss: 1.1984\n",
      "Batch 3700/4372, Loss: 1.1984\n",
      "Batch 3800/4372, Loss: 2.8873\n",
      "Batch 3800/4372, Loss: 2.8873\n",
      "Batch 3900/4372, Loss: 1.8474\n",
      "Batch 3900/4372, Loss: 1.8474\n",
      "Batch 4000/4372, Loss: 1.3179\n",
      "Batch 4000/4372, Loss: 1.3179\n",
      "Batch 4100/4372, Loss: 4.4524\n",
      "Batch 4100/4372, Loss: 4.4524\n",
      "Batch 4200/4372, Loss: 1.4320\n",
      "Batch 4200/4372, Loss: 1.4320\n",
      "Batch 4300/4372, Loss: 2.0126\n",
      "Batch 4300/4372, Loss: 2.0126\n",
      "Train Loss: 2.3479\n",
      "Val Loss: 0.7127, RMSE: 0.8442, MAE: 0.5531, R²: 0.9853\n",
      "\n",
      "Epoch 31/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 1.9442\n",
      "Train Loss: 2.3479\n",
      "Val Loss: 0.7127, RMSE: 0.8442, MAE: 0.5531, R²: 0.9853\n",
      "\n",
      "Epoch 31/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 1.9442\n",
      "Batch 100/4372, Loss: 3.7111\n",
      "Batch 100/4372, Loss: 3.7111\n",
      "Batch 200/4372, Loss: 1.7594\n",
      "Batch 200/4372, Loss: 1.7594\n",
      "Batch 300/4372, Loss: 4.3920\n",
      "Batch 300/4372, Loss: 4.3920\n",
      "Batch 400/4372, Loss: 2.3152\n",
      "Batch 400/4372, Loss: 2.3152\n",
      "Batch 500/4372, Loss: 2.3245\n",
      "Batch 500/4372, Loss: 2.3245\n",
      "Batch 600/4372, Loss: 2.3113\n",
      "Batch 600/4372, Loss: 2.3113\n",
      "Batch 700/4372, Loss: 1.4974\n",
      "Batch 700/4372, Loss: 1.4974\n",
      "Batch 800/4372, Loss: 3.7482\n",
      "Batch 800/4372, Loss: 3.7482\n",
      "Batch 900/4372, Loss: 2.5570\n",
      "Batch 900/4372, Loss: 2.5570\n",
      "Batch 1000/4372, Loss: 1.4420\n",
      "Batch 1000/4372, Loss: 1.4420\n",
      "Batch 1100/4372, Loss: 1.8288\n",
      "Batch 1100/4372, Loss: 1.8288\n",
      "Batch 1200/4372, Loss: 3.6586\n",
      "Batch 1200/4372, Loss: 3.6586\n",
      "Batch 1300/4372, Loss: 2.1232\n",
      "Batch 1300/4372, Loss: 2.1232\n",
      "Batch 1400/4372, Loss: 1.9435\n",
      "Batch 1400/4372, Loss: 1.9435\n",
      "Batch 1500/4372, Loss: 2.0704\n",
      "Batch 1500/4372, Loss: 2.0704\n",
      "Batch 1600/4372, Loss: 2.9313\n",
      "Batch 1600/4372, Loss: 2.9313\n",
      "Batch 1700/4372, Loss: 4.5452\n",
      "Batch 1700/4372, Loss: 4.5452\n",
      "Batch 1800/4372, Loss: 1.9963\n",
      "Batch 1800/4372, Loss: 1.9963\n",
      "Batch 1900/4372, Loss: 1.7205\n",
      "Batch 1900/4372, Loss: 1.7205\n",
      "Batch 2000/4372, Loss: 3.4060\n",
      "Batch 2000/4372, Loss: 3.4060\n",
      "Batch 2100/4372, Loss: 7.7272\n",
      "Batch 2100/4372, Loss: 7.7272\n",
      "Batch 2200/4372, Loss: 1.5865\n",
      "Batch 2200/4372, Loss: 1.5865\n",
      "Batch 2300/4372, Loss: 1.5090\n",
      "Batch 2300/4372, Loss: 1.5090\n",
      "Batch 2400/4372, Loss: 1.9745\n",
      "Batch 2400/4372, Loss: 1.9745\n",
      "Batch 2500/4372, Loss: 3.2153\n",
      "Batch 2500/4372, Loss: 3.2153\n",
      "Batch 2600/4372, Loss: 2.0922\n",
      "Batch 2600/4372, Loss: 2.0922\n",
      "Batch 2700/4372, Loss: 5.4758\n",
      "Batch 2700/4372, Loss: 5.4758\n",
      "Batch 2800/4372, Loss: 1.8112\n",
      "Batch 2800/4372, Loss: 1.8112\n",
      "Batch 2900/4372, Loss: 7.4969\n",
      "Batch 2900/4372, Loss: 7.4969\n",
      "Batch 3000/4372, Loss: 3.2465\n",
      "Batch 3000/4372, Loss: 3.2465\n",
      "Batch 3100/4372, Loss: 3.7011\n",
      "Batch 3100/4372, Loss: 3.7011\n",
      "Batch 3200/4372, Loss: 2.8549\n",
      "Batch 3200/4372, Loss: 2.8549\n",
      "Batch 3300/4372, Loss: 1.0433\n",
      "Batch 3300/4372, Loss: 1.0433\n",
      "Batch 3400/4372, Loss: 1.2932\n",
      "Batch 3400/4372, Loss: 1.2932\n",
      "Batch 3500/4372, Loss: 2.5745\n",
      "Batch 3500/4372, Loss: 2.5745\n",
      "Batch 3600/4372, Loss: 1.4997\n",
      "Batch 3600/4372, Loss: 1.4997\n",
      "Batch 3700/4372, Loss: 2.3170\n",
      "Batch 3700/4372, Loss: 2.3170\n",
      "Batch 3800/4372, Loss: 2.4333\n",
      "Batch 3800/4372, Loss: 2.4333\n",
      "Batch 3900/4372, Loss: 1.7207\n",
      "Batch 3900/4372, Loss: 1.7207\n",
      "Batch 4000/4372, Loss: 2.6923\n",
      "Batch 4000/4372, Loss: 2.6923\n",
      "Batch 4100/4372, Loss: 2.7166\n",
      "Batch 4100/4372, Loss: 2.7166\n",
      "Batch 4200/4372, Loss: 3.7868\n",
      "Batch 4200/4372, Loss: 3.7868\n",
      "Batch 4300/4372, Loss: 8.7293\n",
      "Batch 4300/4372, Loss: 8.7293\n",
      "Train Loss: 2.3959\n",
      "Val Loss: 1.0844, RMSE: 1.0413, MAE: 0.7785, R²: 0.9777\n",
      "\n",
      "Epoch 32/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 2.2084\n",
      "Train Loss: 2.3959\n",
      "Val Loss: 1.0844, RMSE: 1.0413, MAE: 0.7785, R²: 0.9777\n",
      "\n",
      "Epoch 32/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 2.2084\n",
      "Batch 100/4372, Loss: 2.7138\n",
      "Batch 100/4372, Loss: 2.7138\n",
      "Batch 200/4372, Loss: 1.8558\n",
      "Batch 200/4372, Loss: 1.8558\n",
      "Batch 300/4372, Loss: 2.1100\n",
      "Batch 300/4372, Loss: 2.1100\n",
      "Batch 400/4372, Loss: 0.9005\n",
      "Batch 400/4372, Loss: 0.9005\n",
      "Batch 500/4372, Loss: 1.7808\n",
      "Batch 500/4372, Loss: 1.7808\n",
      "Batch 600/4372, Loss: 4.1383\n",
      "Batch 600/4372, Loss: 4.1383\n",
      "Batch 700/4372, Loss: 1.9461\n",
      "Batch 700/4372, Loss: 1.9461\n",
      "Batch 800/4372, Loss: 2.2877\n",
      "Batch 800/4372, Loss: 2.2877\n",
      "Batch 900/4372, Loss: 1.7231\n",
      "Batch 900/4372, Loss: 1.7231\n",
      "Batch 1000/4372, Loss: 1.0189\n",
      "Batch 1000/4372, Loss: 1.0189\n",
      "Batch 1100/4372, Loss: 1.6207\n",
      "Batch 1100/4372, Loss: 1.6207\n",
      "Batch 1200/4372, Loss: 2.6046\n",
      "Batch 1200/4372, Loss: 2.6046\n",
      "Batch 1300/4372, Loss: 2.6631\n",
      "Batch 1300/4372, Loss: 2.6631\n",
      "Batch 1400/4372, Loss: 2.3283\n",
      "Batch 1400/4372, Loss: 2.3283\n",
      "Batch 1500/4372, Loss: 1.7598\n",
      "Batch 1500/4372, Loss: 1.7598\n",
      "Batch 1600/4372, Loss: 1.2625\n",
      "Batch 1600/4372, Loss: 1.2625\n",
      "Batch 1700/4372, Loss: 1.1831\n",
      "Batch 1700/4372, Loss: 1.1831\n",
      "Batch 1800/4372, Loss: 1.3832\n",
      "Batch 1800/4372, Loss: 1.3832\n",
      "Batch 1900/4372, Loss: 1.4359\n",
      "Batch 1900/4372, Loss: 1.4359\n",
      "Batch 2000/4372, Loss: 4.0745\n",
      "Batch 2000/4372, Loss: 4.0745\n",
      "Batch 2100/4372, Loss: 1.4852\n",
      "Batch 2100/4372, Loss: 1.4852\n",
      "Batch 2200/4372, Loss: 1.4383\n",
      "Batch 2200/4372, Loss: 1.4383\n",
      "Batch 2300/4372, Loss: 1.6025\n",
      "Batch 2300/4372, Loss: 1.6025\n",
      "Batch 2400/4372, Loss: 1.2184\n",
      "Batch 2400/4372, Loss: 1.2184\n",
      "Batch 2500/4372, Loss: 1.1046\n",
      "Batch 2500/4372, Loss: 1.1046\n",
      "Batch 2600/4372, Loss: 1.9430\n",
      "Batch 2600/4372, Loss: 1.9430\n",
      "Batch 2700/4372, Loss: 2.5031\n",
      "Batch 2700/4372, Loss: 2.5031\n",
      "Batch 2800/4372, Loss: 2.2572\n",
      "Batch 2800/4372, Loss: 2.2572\n",
      "Batch 2900/4372, Loss: 4.0871\n",
      "Batch 2900/4372, Loss: 4.0871\n",
      "Batch 3000/4372, Loss: 2.3870\n",
      "Batch 3000/4372, Loss: 2.3870\n",
      "Batch 3100/4372, Loss: 2.1175\n",
      "Batch 3100/4372, Loss: 2.1175\n",
      "Batch 3200/4372, Loss: 1.4614\n",
      "Batch 3200/4372, Loss: 1.4614\n",
      "Batch 3300/4372, Loss: 5.7592\n",
      "Batch 3300/4372, Loss: 5.7592\n",
      "Batch 3400/4372, Loss: 1.6510\n",
      "Batch 3400/4372, Loss: 1.6510\n",
      "Batch 3500/4372, Loss: 0.9016\n",
      "Batch 3500/4372, Loss: 0.9016\n",
      "Batch 3600/4372, Loss: 0.8294\n",
      "Batch 3600/4372, Loss: 0.8294\n",
      "Batch 3700/4372, Loss: 2.8547\n",
      "Batch 3700/4372, Loss: 2.8547\n",
      "Batch 3800/4372, Loss: 1.3562\n",
      "Batch 3800/4372, Loss: 1.3562\n",
      "Batch 3900/4372, Loss: 4.0144\n",
      "Batch 3900/4372, Loss: 4.0144\n",
      "Batch 4000/4372, Loss: 2.9028\n",
      "Batch 4000/4372, Loss: 2.9028\n",
      "Batch 4100/4372, Loss: 2.3311\n",
      "Batch 4100/4372, Loss: 2.3311\n",
      "Batch 4200/4372, Loss: 1.9311\n",
      "Batch 4200/4372, Loss: 1.9311\n",
      "Batch 4300/4372, Loss: 2.6226\n",
      "Batch 4300/4372, Loss: 2.6226\n",
      "Train Loss: 2.3255\n",
      "Val Loss: 0.5351, RMSE: 0.7315, MAE: 0.3979, R²: 0.9890\n",
      "\n",
      "Epoch 33/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 4.1416\n",
      "Train Loss: 2.3255\n",
      "Val Loss: 0.5351, RMSE: 0.7315, MAE: 0.3979, R²: 0.9890\n",
      "\n",
      "Epoch 33/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 4.1416\n",
      "Batch 100/4372, Loss: 0.9647\n",
      "Batch 100/4372, Loss: 0.9647\n",
      "Batch 200/4372, Loss: 2.0197\n",
      "Batch 200/4372, Loss: 2.0197\n",
      "Batch 300/4372, Loss: 2.4092\n",
      "Batch 300/4372, Loss: 2.4092\n",
      "Batch 400/4372, Loss: 1.8820\n",
      "Batch 400/4372, Loss: 1.8820\n",
      "Batch 500/4372, Loss: 1.5618\n",
      "Batch 500/4372, Loss: 1.5618\n",
      "Batch 600/4372, Loss: 1.6740\n",
      "Batch 600/4372, Loss: 1.6740\n",
      "Batch 700/4372, Loss: 1.3613\n",
      "Batch 700/4372, Loss: 1.3613\n",
      "Batch 800/4372, Loss: 1.5641\n",
      "Batch 800/4372, Loss: 1.5641\n",
      "Batch 900/4372, Loss: 2.0816\n",
      "Batch 900/4372, Loss: 2.0816\n",
      "Batch 1000/4372, Loss: 2.4730\n",
      "Batch 1000/4372, Loss: 2.4730\n",
      "Batch 1100/4372, Loss: 1.7069\n",
      "Batch 1100/4372, Loss: 1.7069\n",
      "Batch 1200/4372, Loss: 1.0150\n",
      "Batch 1200/4372, Loss: 1.0150\n",
      "Batch 1300/4372, Loss: 2.9230\n",
      "Batch 1300/4372, Loss: 2.9230\n",
      "Batch 1400/4372, Loss: 1.0811\n",
      "Batch 1400/4372, Loss: 1.0811\n",
      "Batch 1500/4372, Loss: 1.9501\n",
      "Batch 1500/4372, Loss: 1.9501\n",
      "Batch 1600/4372, Loss: 1.2904\n",
      "Batch 1600/4372, Loss: 1.2904\n",
      "Batch 1700/4372, Loss: 1.3227\n",
      "Batch 1700/4372, Loss: 1.3227\n",
      "Batch 1800/4372, Loss: 3.0681\n",
      "Batch 1800/4372, Loss: 3.0681\n",
      "Batch 1900/4372, Loss: 4.8503\n",
      "Batch 1900/4372, Loss: 4.8503\n",
      "Batch 2000/4372, Loss: 4.4299\n",
      "Batch 2000/4372, Loss: 4.4299\n",
      "Batch 2100/4372, Loss: 2.0145\n",
      "Batch 2100/4372, Loss: 2.0145\n",
      "Batch 2200/4372, Loss: 1.6563\n",
      "Batch 2200/4372, Loss: 1.6563\n",
      "Batch 2300/4372, Loss: 2.1741\n",
      "Batch 2300/4372, Loss: 2.1741\n",
      "Batch 2400/4372, Loss: 1.4400\n",
      "Batch 2400/4372, Loss: 1.4400\n",
      "Batch 2500/4372, Loss: 2.8439\n",
      "Batch 2500/4372, Loss: 2.8439\n",
      "Batch 2600/4372, Loss: 4.1221\n",
      "Batch 2600/4372, Loss: 4.1221\n",
      "Batch 2700/4372, Loss: 1.4330\n",
      "Batch 2700/4372, Loss: 1.4330\n",
      "Batch 2800/4372, Loss: 1.9513\n",
      "Batch 2800/4372, Loss: 1.9513\n",
      "Batch 2900/4372, Loss: 5.1741\n",
      "Batch 2900/4372, Loss: 5.1741\n",
      "Batch 3000/4372, Loss: 1.6177\n",
      "Batch 3000/4372, Loss: 1.6177\n",
      "Batch 3100/4372, Loss: 1.5414\n",
      "Batch 3100/4372, Loss: 1.5414\n",
      "Batch 3200/4372, Loss: 1.9965\n",
      "Batch 3200/4372, Loss: 1.9965\n",
      "Batch 3300/4372, Loss: 4.3059\n",
      "Batch 3300/4372, Loss: 4.3059\n",
      "Batch 3400/4372, Loss: 2.1826\n",
      "Batch 3400/4372, Loss: 2.1826\n",
      "Batch 3500/4372, Loss: 1.4771\n",
      "Batch 3500/4372, Loss: 1.4771\n",
      "Batch 3600/4372, Loss: 1.5397\n",
      "Batch 3600/4372, Loss: 1.5397\n",
      "Batch 3700/4372, Loss: 1.1901\n",
      "Batch 3700/4372, Loss: 1.1901\n",
      "Batch 3800/4372, Loss: 1.5554\n",
      "Batch 3800/4372, Loss: 1.5554\n",
      "Batch 3900/4372, Loss: 1.2609\n",
      "Batch 3900/4372, Loss: 1.2609\n",
      "Batch 4000/4372, Loss: 1.0993\n",
      "Batch 4000/4372, Loss: 1.0993\n",
      "Batch 4100/4372, Loss: 3.3698\n",
      "Batch 4100/4372, Loss: 3.3698\n",
      "Batch 4200/4372, Loss: 1.5170\n",
      "Batch 4200/4372, Loss: 1.5170\n",
      "Batch 4300/4372, Loss: 3.0190\n",
      "Batch 4300/4372, Loss: 3.0190\n",
      "Train Loss: 2.3215\n",
      "Val Loss: 0.5021, RMSE: 0.7086, MAE: 0.4022, R²: 0.9897\n",
      "\n",
      "Epoch 34/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 2.5219\n",
      "Train Loss: 2.3215\n",
      "Val Loss: 0.5021, RMSE: 0.7086, MAE: 0.4022, R²: 0.9897\n",
      "\n",
      "Epoch 34/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 2.5219\n",
      "Batch 100/4372, Loss: 2.1361\n",
      "Batch 100/4372, Loss: 2.1361\n",
      "Batch 200/4372, Loss: 2.2438\n",
      "Batch 200/4372, Loss: 2.2438\n",
      "Batch 300/4372, Loss: 1.1792\n",
      "Batch 300/4372, Loss: 1.1792\n",
      "Batch 400/4372, Loss: 1.7319\n",
      "Batch 400/4372, Loss: 1.7319\n",
      "Batch 500/4372, Loss: 2.5419\n",
      "Batch 500/4372, Loss: 2.5419\n",
      "Batch 600/4372, Loss: 1.2796\n",
      "Batch 600/4372, Loss: 1.2796\n",
      "Batch 700/4372, Loss: 1.5635\n",
      "Batch 700/4372, Loss: 1.5635\n",
      "Batch 800/4372, Loss: 5.2627\n",
      "Batch 800/4372, Loss: 5.2627\n",
      "Batch 900/4372, Loss: 1.3144\n",
      "Batch 900/4372, Loss: 1.3144\n",
      "Batch 1000/4372, Loss: 2.6680\n",
      "Batch 1000/4372, Loss: 2.6680\n",
      "Batch 1100/4372, Loss: 2.4833\n",
      "Batch 1100/4372, Loss: 2.4833\n",
      "Batch 1200/4372, Loss: 1.1531\n",
      "Batch 1200/4372, Loss: 1.1531\n",
      "Batch 1300/4372, Loss: 1.2091\n",
      "Batch 1300/4372, Loss: 1.2091\n",
      "Batch 1400/4372, Loss: 1.6628\n",
      "Batch 1400/4372, Loss: 1.6628\n",
      "Batch 1500/4372, Loss: 2.1555\n",
      "Batch 1500/4372, Loss: 2.1555\n",
      "Batch 1600/4372, Loss: 6.2932\n",
      "Batch 1600/4372, Loss: 6.2932\n",
      "Batch 1700/4372, Loss: 4.1189\n",
      "Batch 1700/4372, Loss: 4.1189\n",
      "Batch 1800/4372, Loss: 1.5347\n",
      "Batch 1800/4372, Loss: 1.5347\n",
      "Batch 1900/4372, Loss: 2.6013\n",
      "Batch 1900/4372, Loss: 2.6013\n",
      "Batch 2000/4372, Loss: 3.9781\n",
      "Batch 2000/4372, Loss: 3.9781\n",
      "Batch 2100/4372, Loss: 1.9728\n",
      "Batch 2100/4372, Loss: 1.9728\n",
      "Batch 2200/4372, Loss: 1.3884\n",
      "Batch 2200/4372, Loss: 1.3884\n",
      "Batch 2300/4372, Loss: 1.0178\n",
      "Batch 2300/4372, Loss: 1.0178\n",
      "Batch 2400/4372, Loss: 1.2269\n",
      "Batch 2400/4372, Loss: 1.2269\n",
      "Batch 2500/4372, Loss: 6.3404\n",
      "Batch 2500/4372, Loss: 6.3404\n",
      "Batch 2600/4372, Loss: 1.6476\n",
      "Batch 2600/4372, Loss: 1.6476\n",
      "Batch 2700/4372, Loss: 1.4075\n",
      "Batch 2700/4372, Loss: 1.4075\n",
      "Batch 2800/4372, Loss: 2.2869\n",
      "Batch 2800/4372, Loss: 2.2869\n",
      "Batch 2900/4372, Loss: 1.4533\n",
      "Batch 2900/4372, Loss: 1.4533\n",
      "Batch 3000/4372, Loss: 1.7499\n",
      "Batch 3000/4372, Loss: 1.7499\n",
      "Batch 3100/4372, Loss: 1.5034\n",
      "Batch 3100/4372, Loss: 1.5034\n",
      "Batch 3200/4372, Loss: 4.1494\n",
      "Batch 3200/4372, Loss: 4.1494\n",
      "Batch 3300/4372, Loss: 1.4978\n",
      "Batch 3300/4372, Loss: 1.4978\n",
      "Batch 3400/4372, Loss: 3.8884\n",
      "Batch 3400/4372, Loss: 3.8884\n",
      "Batch 3500/4372, Loss: 2.4628\n",
      "Batch 3500/4372, Loss: 2.4628\n",
      "Batch 3600/4372, Loss: 1.3088\n",
      "Batch 3600/4372, Loss: 1.3088\n",
      "Batch 3700/4372, Loss: 2.1618\n",
      "Batch 3700/4372, Loss: 2.1618\n",
      "Batch 3800/4372, Loss: 2.9865\n",
      "Batch 3800/4372, Loss: 2.9865\n",
      "Batch 3900/4372, Loss: 4.7166\n",
      "Batch 3900/4372, Loss: 4.7166\n",
      "Batch 4000/4372, Loss: 1.1469\n",
      "Batch 4000/4372, Loss: 1.1469\n",
      "Batch 4100/4372, Loss: 1.6994\n",
      "Batch 4100/4372, Loss: 1.6994\n",
      "Batch 4200/4372, Loss: 2.3743\n",
      "Batch 4200/4372, Loss: 2.3743\n",
      "Batch 4300/4372, Loss: 1.6328\n",
      "Batch 4300/4372, Loss: 1.6328\n",
      "Train Loss: 2.2902\n",
      "Val Loss: 0.4927, RMSE: 0.7019, MAE: 0.3793, R²: 0.9899\n",
      "\n",
      "Epoch 35/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 2.6225\n",
      "Train Loss: 2.2902\n",
      "Val Loss: 0.4927, RMSE: 0.7019, MAE: 0.3793, R²: 0.9899\n",
      "\n",
      "Epoch 35/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 2.6225\n",
      "Batch 100/4372, Loss: 2.0966\n",
      "Batch 100/4372, Loss: 2.0966\n",
      "Batch 200/4372, Loss: 1.4092\n",
      "Batch 200/4372, Loss: 1.4092\n",
      "Batch 300/4372, Loss: 2.0185\n",
      "Batch 300/4372, Loss: 2.0185\n",
      "Batch 400/4372, Loss: 4.1266\n",
      "Batch 400/4372, Loss: 4.1266\n",
      "Batch 500/4372, Loss: 1.8981\n",
      "Batch 500/4372, Loss: 1.8981\n",
      "Batch 600/4372, Loss: 1.5041\n",
      "Batch 600/4372, Loss: 1.5041\n",
      "Batch 700/4372, Loss: 1.4965\n",
      "Batch 700/4372, Loss: 1.4965\n",
      "Batch 800/4372, Loss: 1.4200\n",
      "Batch 800/4372, Loss: 1.4200\n",
      "Batch 900/4372, Loss: 0.9722\n",
      "Batch 900/4372, Loss: 0.9722\n",
      "Batch 1000/4372, Loss: 2.8721\n",
      "Batch 1000/4372, Loss: 2.8721\n",
      "Batch 1100/4372, Loss: 8.1314\n",
      "Batch 1100/4372, Loss: 8.1314\n",
      "Batch 1200/4372, Loss: 2.5143\n",
      "Batch 1200/4372, Loss: 2.5143\n",
      "Batch 1300/4372, Loss: 3.0093\n",
      "Batch 1300/4372, Loss: 3.0093\n",
      "Batch 1400/4372, Loss: 3.1048\n",
      "Batch 1400/4372, Loss: 3.1048\n",
      "Batch 1500/4372, Loss: 3.2261\n",
      "Batch 1500/4372, Loss: 3.2261\n",
      "Batch 1600/4372, Loss: 2.0327\n",
      "Batch 1600/4372, Loss: 2.0327\n",
      "Batch 1700/4372, Loss: 1.5092\n",
      "Batch 1700/4372, Loss: 1.5092\n",
      "Batch 1800/4372, Loss: 2.3763\n",
      "Batch 1800/4372, Loss: 2.3763\n",
      "Batch 1900/4372, Loss: 1.6194\n",
      "Batch 1900/4372, Loss: 1.6194\n",
      "Batch 2000/4372, Loss: 1.4344\n",
      "Batch 2000/4372, Loss: 1.4344\n",
      "Batch 2100/4372, Loss: 1.7946\n",
      "Batch 2100/4372, Loss: 1.7946\n",
      "Batch 2200/4372, Loss: 1.0625\n",
      "Batch 2200/4372, Loss: 1.0625\n",
      "Batch 2300/4372, Loss: 3.1806\n",
      "Batch 2300/4372, Loss: 3.1806\n",
      "Batch 2400/4372, Loss: 1.4496\n",
      "Batch 2400/4372, Loss: 1.4496\n",
      "Batch 2500/4372, Loss: 0.9411\n",
      "Batch 2500/4372, Loss: 0.9411\n",
      "Batch 2600/4372, Loss: 2.6636\n",
      "Batch 2600/4372, Loss: 2.6636\n",
      "Batch 2700/4372, Loss: 3.7317\n",
      "Batch 2700/4372, Loss: 3.7317\n",
      "Batch 2800/4372, Loss: 0.8423\n",
      "Batch 2800/4372, Loss: 0.8423\n",
      "Batch 2900/4372, Loss: 1.1966\n",
      "Batch 2900/4372, Loss: 1.1966\n",
      "Batch 3000/4372, Loss: 1.8102\n",
      "Batch 3000/4372, Loss: 1.8102\n",
      "Batch 3100/4372, Loss: 1.9494\n",
      "Batch 3100/4372, Loss: 1.9494\n",
      "Batch 3200/4372, Loss: 2.7950\n",
      "Batch 3200/4372, Loss: 2.7950\n",
      "Batch 3300/4372, Loss: 1.1882\n",
      "Batch 3300/4372, Loss: 1.1882\n",
      "Batch 3400/4372, Loss: 1.9935\n",
      "Batch 3400/4372, Loss: 1.9935\n",
      "Batch 3500/4372, Loss: 1.3452\n",
      "Batch 3500/4372, Loss: 1.3452\n",
      "Batch 3600/4372, Loss: 1.4710\n",
      "Batch 3600/4372, Loss: 1.4710\n",
      "Batch 3700/4372, Loss: 1.8323\n",
      "Batch 3700/4372, Loss: 1.8323\n",
      "Batch 3800/4372, Loss: 2.6385\n",
      "Batch 3800/4372, Loss: 2.6385\n",
      "Batch 3900/4372, Loss: 3.1309\n",
      "Batch 3900/4372, Loss: 3.1309\n",
      "Batch 4000/4372, Loss: 2.1177\n",
      "Batch 4000/4372, Loss: 2.1177\n",
      "Batch 4100/4372, Loss: 1.9031\n",
      "Batch 4100/4372, Loss: 1.9031\n",
      "Batch 4200/4372, Loss: 1.3700\n",
      "Batch 4200/4372, Loss: 1.3700\n",
      "Batch 4300/4372, Loss: 11.5539\n",
      "Batch 4300/4372, Loss: 11.5539\n",
      "Train Loss: 2.2705\n",
      "Val Loss: 0.6521, RMSE: 0.8075, MAE: 0.5189, R²: 0.9866\n",
      "\n",
      "Epoch 36/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 2.0025\n",
      "Train Loss: 2.2705\n",
      "Val Loss: 0.6521, RMSE: 0.8075, MAE: 0.5189, R²: 0.9866\n",
      "\n",
      "Epoch 36/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 2.0025\n",
      "Batch 100/4372, Loss: 2.1266\n",
      "Batch 100/4372, Loss: 2.1266\n",
      "Batch 200/4372, Loss: 2.1800\n",
      "Batch 200/4372, Loss: 2.1800\n",
      "Batch 300/4372, Loss: 1.4948\n",
      "Batch 300/4372, Loss: 1.4948\n",
      "Batch 400/4372, Loss: 2.7477\n",
      "Batch 400/4372, Loss: 2.7477\n",
      "Batch 500/4372, Loss: 0.8472\n",
      "Batch 500/4372, Loss: 0.8472\n",
      "Batch 600/4372, Loss: 2.4703\n",
      "Batch 600/4372, Loss: 2.4703\n",
      "Batch 700/4372, Loss: 4.3568\n",
      "Batch 700/4372, Loss: 4.3568\n",
      "Batch 800/4372, Loss: 1.3217\n",
      "Batch 800/4372, Loss: 1.3217\n",
      "Batch 900/4372, Loss: 2.4466\n",
      "Batch 900/4372, Loss: 2.4466\n",
      "Batch 1000/4372, Loss: 1.8700\n",
      "Batch 1000/4372, Loss: 1.8700\n",
      "Batch 1100/4372, Loss: 1.2603\n",
      "Batch 1100/4372, Loss: 1.2603\n",
      "Batch 1200/4372, Loss: 0.9814\n",
      "Batch 1200/4372, Loss: 0.9814\n",
      "Batch 1300/4372, Loss: 0.9220\n",
      "Batch 1300/4372, Loss: 0.9220\n",
      "Batch 1400/4372, Loss: 10.9936\n",
      "Batch 1400/4372, Loss: 10.9936\n",
      "Batch 1500/4372, Loss: 2.1491\n",
      "Batch 1500/4372, Loss: 2.1491\n",
      "Batch 1600/4372, Loss: 4.2851\n",
      "Batch 1600/4372, Loss: 4.2851\n",
      "Batch 1700/4372, Loss: 1.1361\n",
      "Batch 1700/4372, Loss: 1.1361\n",
      "Batch 1800/4372, Loss: 5.3545\n",
      "Batch 1800/4372, Loss: 5.3545\n",
      "Batch 1900/4372, Loss: 6.7160\n",
      "Batch 1900/4372, Loss: 6.7160\n",
      "Batch 2000/4372, Loss: 5.2401\n",
      "Batch 2000/4372, Loss: 5.2401\n",
      "Batch 2100/4372, Loss: 1.6446\n",
      "Batch 2100/4372, Loss: 1.6446\n",
      "Batch 2200/4372, Loss: 1.5730\n",
      "Batch 2200/4372, Loss: 1.5730\n",
      "Batch 2300/4372, Loss: 1.2343\n",
      "Batch 2300/4372, Loss: 1.2343\n",
      "Batch 2400/4372, Loss: 1.8025\n",
      "Batch 2400/4372, Loss: 1.8025\n",
      "Batch 2500/4372, Loss: 5.2427\n",
      "Batch 2500/4372, Loss: 5.2427\n",
      "Batch 2600/4372, Loss: 1.8896\n",
      "Batch 2600/4372, Loss: 1.8896\n",
      "Batch 2700/4372, Loss: 1.4042\n",
      "Batch 2700/4372, Loss: 1.4042\n",
      "Batch 2800/4372, Loss: 1.5237\n",
      "Batch 2800/4372, Loss: 1.5237\n",
      "Batch 2900/4372, Loss: 2.7910\n",
      "Batch 2900/4372, Loss: 2.7910\n",
      "Batch 3000/4372, Loss: 1.6839\n",
      "Batch 3000/4372, Loss: 1.6839\n",
      "Batch 3100/4372, Loss: 4.0597\n",
      "Batch 3100/4372, Loss: 4.0597\n",
      "Batch 3200/4372, Loss: 2.2263\n",
      "Batch 3200/4372, Loss: 2.2263\n",
      "Batch 3300/4372, Loss: 2.0461\n",
      "Batch 3300/4372, Loss: 2.0461\n",
      "Batch 3400/4372, Loss: 1.2139\n",
      "Batch 3400/4372, Loss: 1.2139\n",
      "Batch 3500/4372, Loss: 2.4654\n",
      "Batch 3500/4372, Loss: 2.4654\n",
      "Batch 3600/4372, Loss: 1.9886\n",
      "Batch 3600/4372, Loss: 1.9886\n",
      "Batch 3700/4372, Loss: 1.5767\n",
      "Batch 3700/4372, Loss: 1.5767\n",
      "Batch 3800/4372, Loss: 2.3759\n",
      "Batch 3800/4372, Loss: 2.3759\n",
      "Batch 3900/4372, Loss: 3.1306\n",
      "Batch 3900/4372, Loss: 3.1306\n",
      "Batch 4000/4372, Loss: 2.8985\n",
      "Batch 4000/4372, Loss: 2.8985\n",
      "Batch 4100/4372, Loss: 1.2991\n",
      "Batch 4100/4372, Loss: 1.2991\n",
      "Batch 4200/4372, Loss: 1.4386\n",
      "Batch 4200/4372, Loss: 1.4386\n",
      "Batch 4300/4372, Loss: 1.7456\n",
      "Batch 4300/4372, Loss: 1.7456\n",
      "Train Loss: 2.2533\n",
      "Val Loss: 0.6325, RMSE: 0.7953, MAE: 0.5022, R²: 0.9870\n",
      "\n",
      "Epoch 37/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 3.4895\n",
      "Train Loss: 2.2533\n",
      "Val Loss: 0.6325, RMSE: 0.7953, MAE: 0.5022, R²: 0.9870\n",
      "\n",
      "Epoch 37/50\n",
      "--------------------------------------------------\n",
      "Batch 0/4372, Loss: 3.4895\n",
      "Batch 100/4372, Loss: 1.7365\n",
      "Batch 100/4372, Loss: 1.7365\n",
      "Batch 200/4372, Loss: 1.0210\n",
      "Batch 200/4372, Loss: 1.0210\n",
      "Batch 300/4372, Loss: 5.1279\n",
      "Batch 300/4372, Loss: 5.1279\n",
      "Batch 400/4372, Loss: 1.0749\n",
      "Batch 400/4372, Loss: 1.0749\n",
      "Batch 500/4372, Loss: 1.3082\n",
      "Batch 500/4372, Loss: 1.3082\n",
      "Batch 600/4372, Loss: 0.8837\n",
      "Batch 600/4372, Loss: 0.8837\n",
      "Batch 700/4372, Loss: 1.7619\n",
      "Batch 700/4372, Loss: 1.7619\n",
      "Batch 800/4372, Loss: 1.7700\n",
      "Batch 800/4372, Loss: 1.7700\n",
      "Batch 900/4372, Loss: 1.8995\n",
      "Batch 900/4372, Loss: 1.8995\n",
      "Batch 1000/4372, Loss: 2.3306\n",
      "Batch 1000/4372, Loss: 2.3306\n",
      "Batch 1100/4372, Loss: 1.1222\n",
      "Batch 1100/4372, Loss: 1.1222\n",
      "Batch 1200/4372, Loss: 0.9518\n",
      "Batch 1200/4372, Loss: 0.9518\n",
      "Batch 1300/4372, Loss: 0.9326\n",
      "Batch 1300/4372, Loss: 0.9326\n",
      "Batch 1400/4372, Loss: 2.1730\n",
      "Batch 1400/4372, Loss: 2.1730\n",
      "Batch 1500/4372, Loss: 2.1003\n",
      "Batch 1500/4372, Loss: 2.1003\n",
      "Batch 1600/4372, Loss: 3.3394\n",
      "Batch 1600/4372, Loss: 3.3394\n",
      "Batch 1700/4372, Loss: 2.0077\n",
      "Batch 1700/4372, Loss: 2.0077\n",
      "Batch 1800/4372, Loss: 1.9138\n",
      "Batch 1800/4372, Loss: 1.9138\n",
      "Batch 1900/4372, Loss: 5.3422\n",
      "Batch 1900/4372, Loss: 5.3422\n",
      "Batch 2000/4372, Loss: 2.3014\n",
      "Batch 2000/4372, Loss: 2.3014\n",
      "Batch 2100/4372, Loss: 1.1340\n",
      "Batch 2100/4372, Loss: 1.1340\n",
      "Batch 2200/4372, Loss: 1.5864\n",
      "Batch 2200/4372, Loss: 1.5864\n",
      "Batch 2300/4372, Loss: 2.4730\n",
      "Batch 2300/4372, Loss: 2.4730\n",
      "Batch 2400/4372, Loss: 1.8118\n",
      "Batch 2400/4372, Loss: 1.8118\n",
      "Batch 2500/4372, Loss: 1.4376\n",
      "Batch 2500/4372, Loss: 1.4376\n",
      "Batch 2600/4372, Loss: 1.7839\n",
      "Batch 2600/4372, Loss: 1.7839\n",
      "Batch 2700/4372, Loss: 2.6410\n",
      "Batch 2700/4372, Loss: 2.6410\n",
      "Batch 2800/4372, Loss: 1.4493\n",
      "Batch 2800/4372, Loss: 1.4493\n",
      "Batch 2900/4372, Loss: 2.2798\n",
      "Batch 2900/4372, Loss: 2.2798\n",
      "Batch 3000/4372, Loss: 2.0345\n",
      "Batch 3000/4372, Loss: 2.0345\n",
      "Batch 3100/4372, Loss: 1.2741\n",
      "Batch 3100/4372, Loss: 1.2741\n",
      "Batch 3200/4372, Loss: 3.4919\n",
      "Batch 3200/4372, Loss: 3.4919\n",
      "Batch 3300/4372, Loss: 1.0288\n",
      "Batch 3300/4372, Loss: 1.0288\n",
      "Batch 3400/4372, Loss: 1.7527\n",
      "Batch 3400/4372, Loss: 1.7527\n",
      "Batch 3500/4372, Loss: 2.0914\n",
      "Batch 3500/4372, Loss: 2.0914\n",
      "Batch 3600/4372, Loss: 1.7648\n",
      "Batch 3600/4372, Loss: 1.7648\n",
      "Batch 3700/4372, Loss: 2.1158\n",
      "Batch 3700/4372, Loss: 2.1158\n",
      "Batch 3800/4372, Loss: 4.1075\n",
      "Batch 3800/4372, Loss: 4.1075\n",
      "Batch 3900/4372, Loss: 1.9248\n",
      "Batch 3900/4372, Loss: 1.9248\n",
      "Batch 4000/4372, Loss: 1.9541\n",
      "Batch 4000/4372, Loss: 1.9541\n",
      "Batch 4100/4372, Loss: 5.7624\n",
      "Batch 4100/4372, Loss: 5.7624\n",
      "Batch 4200/4372, Loss: 1.5631\n",
      "Batch 4200/4372, Loss: 1.5631\n",
      "Batch 4300/4372, Loss: 1.9850\n",
      "Batch 4300/4372, Loss: 1.9850\n",
      "Train Loss: 2.2196\n",
      "Val Loss: 0.7444, RMSE: 0.8628, MAE: 0.5569, R²: 0.9847\n",
      "Early stopping triggered after 37 epochs\n",
      "Train Loss: 2.2196\n",
      "Val Loss: 0.7444, RMSE: 0.8628, MAE: 0.5569, R²: 0.9847\n",
      "Early stopping triggered after 37 epochs\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FINAL RESULTS ===\n",
      "Final Test RMSE: $0.86\n",
      "Final Test MAE: $0.56\n",
      "Final R² Score: 0.9847\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([26.481273267919168,\n",
       "  4.173852661118416,\n",
       "  3.816999087013025,\n",
       "  3.6123313406739666,\n",
       "  3.4565009500384876,\n",
       "  3.3334281340936727,\n",
       "  3.2243431736762003,\n",
       "  3.1688422869349004,\n",
       "  3.0771188378497643,\n",
       "  3.009476683454457,\n",
       "  2.9894326604895927,\n",
       "  2.918510609540717,\n",
       "  2.8555530285137243,\n",
       "  2.785121186558849,\n",
       "  2.7926100586955958,\n",
       "  2.748069652058832,\n",
       "  2.733043399202311,\n",
       "  2.6687471947589274,\n",
       "  2.659982958997703,\n",
       "  2.635615472428753,\n",
       "  2.5833090613412946,\n",
       "  2.573794671350516,\n",
       "  2.5382732446599423,\n",
       "  2.524334683605322,\n",
       "  2.5206148145619816,\n",
       "  2.4688877690470994,\n",
       "  2.458816375573185,\n",
       "  2.3970335601996853,\n",
       "  2.3997164504416797,\n",
       "  2.347898443903491,\n",
       "  2.395863272133816,\n",
       "  2.325463840599902,\n",
       "  2.32152935354702,\n",
       "  2.290202937822547,\n",
       "  2.270506171109146,\n",
       "  2.2533242298939418,\n",
       "  2.2195524946741303],\n",
       " [1.543769965941821,\n",
       "  1.4473668181449038,\n",
       "  1.03703515647424,\n",
       "  0.9114951059453987,\n",
       "  0.9396170050260242,\n",
       "  0.7095764827286554,\n",
       "  0.8130027817567335,\n",
       "  0.6214622169569448,\n",
       "  0.6162585473507779,\n",
       "  0.6187613922063081,\n",
       "  1.0537049814799677,\n",
       "  0.5553058129543573,\n",
       "  0.9796851026771926,\n",
       "  0.6042305038592626,\n",
       "  0.59702254815431,\n",
       "  0.743972949240585,\n",
       "  0.5267709316609108,\n",
       "  0.7001883387020282,\n",
       "  1.362562020165268,\n",
       "  0.5637946395102926,\n",
       "  0.7621482645603679,\n",
       "  0.4874860560086923,\n",
       "  0.744625917537869,\n",
       "  0.5784491327576755,\n",
       "  0.5335749879288543,\n",
       "  0.5822336203661514,\n",
       "  0.6739129593420334,\n",
       "  0.7409153201524424,\n",
       "  0.7252778062172581,\n",
       "  0.7127479890400624,\n",
       "  1.0843969656040335,\n",
       "  0.5351072714373483,\n",
       "  0.502114760663023,\n",
       "  0.49266789485474627,\n",
       "  0.6520987442623882,\n",
       "  0.6324749072947201,\n",
       "  0.7444238031226784],\n",
       " [1.2424855,\n",
       "  1.2030736,\n",
       "  1.0183434,\n",
       "  0.9547307,\n",
       "  0.96933734,\n",
       "  0.8423635,\n",
       "  0.9016654,\n",
       "  0.7883307,\n",
       "  0.7850244,\n",
       "  0.7866152,\n",
       "  1.0265013,\n",
       "  0.74519247,\n",
       "  0.98979205,\n",
       "  0.77732575,\n",
       "  0.7726741,\n",
       "  0.8625396,\n",
       "  0.72579426,\n",
       "  0.8367771,\n",
       "  1.1672878,\n",
       "  0.7508665,\n",
       "  0.8730119,\n",
       "  0.6982044,\n",
       "  0.8629171,\n",
       "  0.7605616,\n",
       "  0.730465,\n",
       "  0.76304686,\n",
       "  0.82092434,\n",
       "  0.8607654,\n",
       "  0.85163534,\n",
       "  0.8442464,\n",
       "  1.041346,\n",
       "  0.7315141,\n",
       "  0.708603,\n",
       "  0.7019047,\n",
       "  0.80752826,\n",
       "  0.7952863,\n",
       "  0.86280197])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_taxi_fare_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3b2fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SAMPLE PREDICTION EXAMPLES ===\n",
      "Sample trip details:\n",
      "  PULocationID: 161\n",
      "  DOLocationID: 230\n",
      "  passenger_count: 1\n",
      "  trip_distance: 8.5\n",
      "  extra: 0.5\n",
      "  mta_tax: 0.5\n",
      "  tip_amount: 3.0\n",
      "  tolls_amount: 0.0\n",
      "  total_amount: 25.0\n",
      "  payment_type: 1\n",
      "  congestion_surcharge: 2.5\n",
      "  Airport_fee: 0.0\n",
      "  cbd_congestion_fee: 0.75\n",
      "  trip_duration_minutes: 25\n",
      "  pickup_hour: 14\n",
      "  pickup_day: Friday\n",
      "  pickup_month: 1\n",
      "\n",
      "Expected feature order (17 features):\n",
      " 1. PULocationID\n",
      " 2. DOLocationID\n",
      " 3. passenger_count\n",
      " 4. trip_distance\n",
      " 5. extra\n",
      " 6. mta_tax\n",
      " 7. tip_amount\n",
      " 8. tolls_amount\n",
      " 9. total_amount\n",
      "10. payment_type\n",
      "11. congestion_surcharge\n",
      "12. Airport_fee\n",
      "13. cbd_congestion_fee\n",
      "14. trip_duration_minutes\n",
      "15. pickup_hour\n",
      "16. pickup_day\n",
      "17. pickup_month\n",
      "\n",
      "# To make a prediction, run:\n",
      "predicted_fare = predict_fare(model, sample_trip, train_dataset.scaler)\n",
      "print(f'Predicted fare: ${predicted_fare:.2f}')\n"
     ]
    }
   ],
   "source": [
    "# Sample Input and Prediction Function\n",
    "def predict_fare(model, sample_input, scaler=None):\n",
    "    \"\"\"\n",
    "    Make a prediction for a single trip or batch of trips\n",
    "    \n",
    "    Args:\n",
    "        model: Trained PyTorch model\n",
    "        sample_input: Dictionary with feature values or numpy array\n",
    "        scaler: StandardScaler used during training (if available)\n",
    "    \n",
    "    Returns:\n",
    "        Predicted fare amount\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # If input is a dictionary, convert to array\n",
    "        if isinstance(sample_input, dict):\n",
    "            # Define the expected feature order (same as training)\n",
    "            feature_order = [\n",
    "                'passenger_count', 'trip_distance',\n",
    "                'extra', 'mta_tax', 'tip_amount', 'tolls_amount',\n",
    "                'payment_type', 'congestion_surcharge', 'Airport_fee', 'cbd_congestion_fee',\n",
    "                'trip_duration_minutes', 'pickup_hour', 'pickup_day', 'pickup_month'\n",
    "            ]\n",
    "            \n",
    "            # Convert pickup_day to numeric if it's a string\n",
    "            if 'pickup_day' in sample_input and isinstance(sample_input['pickup_day'], str):\n",
    "                day_mapping = {\n",
    "                    'Monday': 0, 'Tuesday': 1, 'Wednesday': 2, 'Thursday': 3,\n",
    "                    'Friday': 4, 'Saturday': 5, 'Sunday': 6\n",
    "                }\n",
    "                sample_input['pickup_day'] = day_mapping.get(sample_input['pickup_day'], 0)\n",
    "            \n",
    "            # Create array in correct order\n",
    "            input_array = np.array([[sample_input[feature] for feature in feature_order]], dtype=np.float32)\n",
    "        else:\n",
    "            input_array = np.array(sample_input, dtype=np.float32)\n",
    "            if input_array.ndim == 1:\n",
    "                input_array = input_array.reshape(1, -1)\n",
    "        \n",
    "        # Apply scaling if scaler is provided\n",
    "        if scaler is not None:\n",
    "            input_array = scaler.transform(input_array)\n",
    "        \n",
    "        # Convert to tensor\n",
    "        input_tensor = torch.tensor(input_array, dtype=torch.float32)\n",
    "        \n",
    "        # Make prediction\n",
    "        prediction = model(input_tensor)\n",
    "        \n",
    "        # Return as float (remove tensor wrapper)\n",
    "        return prediction.item() if prediction.numel() == 1 else prediction.numpy()\n",
    "\n",
    "# Example 1: Using a dictionary input (most user-friendly)\n",
    "print(\"=== SAMPLE PREDICTION EXAMPLES ===\")\n",
    "\n",
    "# Updated sample trip data with correct features (excluding location IDs and total_amount)\n",
    "sample_trip = {\n",
    "    'passenger_count': 1,\n",
    "    'trip_distance': 8.5,     # miles\n",
    "    'extra': 0.5,\n",
    "    'mta_tax': 0.5,\n",
    "    'tip_amount': 3.0,\n",
    "    'tolls_amount': 0.0,\n",
    "    'payment_type': 1,        # Credit card\n",
    "    'congestion_surcharge': 2.5,\n",
    "    'Airport_fee': 0.0,\n",
    "    'cbd_congestion_fee': 0.75,\n",
    "    'trip_duration_minutes': 25,\n",
    "    'pickup_hour': 14,        # 2 PM\n",
    "    'pickup_day': 'Friday',   # Will be converted to 4\n",
    "    'pickup_month': 1         # January\n",
    "}\n",
    "\n",
    "print(\"Sample trip details:\")\n",
    "for key, value in sample_trip.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nExpected feature order ({len(feature_columns)} features):\")\n",
    "for i, feature in enumerate(feature_columns, 1):\n",
    "    print(f\"{i:2d}. {feature}\")\n",
    "\n",
    "# Make prediction (you'll need to run this after training the model)\n",
    "print(\"\\n# To make a prediction, run:\")\n",
    "print(\"predicted_fare = predict_fare(model, sample_trip, train_dataset.scaler)\")\n",
    "print(\"print(f'Predicted fare: ${predicted_fare:.2f}')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50c9be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction functions ready!\n",
      "\n",
      "Usage examples:\n",
      "1. predict_fare(model, sample_trip, train_dataset.scaler)\n",
      "2. sample_trips = create_sample_trips()\n",
      "   predict_multiple_fares(model, sample_trips, train_dataset.scaler)\n",
      "3. interactive_prediction(model, train_dataset.scaler)\n"
     ]
    }
   ],
   "source": [
    "# More Prediction Examples and Batch Processing\n",
    "def create_sample_trips():\n",
    "    \"\"\"Create multiple sample trips for testing\"\"\"\n",
    "    \n",
    "    # Short trip - Local trip\n",
    "    short_trip = {\n",
    "        'passenger_count': 1,\n",
    "        'trip_distance': 1.2,\n",
    "        'extra': 0.0,\n",
    "        'mta_tax': 0.5,\n",
    "        'tip_amount': 1.5,\n",
    "        'tolls_amount': 0.0,\n",
    "        'payment_type': 1,\n",
    "        'congestion_surcharge': 0.0,\n",
    "        'Airport_fee': 0.0,\n",
    "        'cbd_congestion_fee': 0.0,\n",
    "        'trip_duration_minutes': 8,\n",
    "        'pickup_hour': 10,        # 10 AM\n",
    "        'pickup_day': 'Tuesday',\n",
    "        'pickup_month': 1\n",
    "    }\n",
    "    \n",
    "    # Medium trip - Cross-borough trip\n",
    "    medium_trip = {\n",
    "        'passenger_count': 2,\n",
    "        'trip_distance': 5.8,\n",
    "        'extra': 0.5,\n",
    "        'mta_tax': 0.5,\n",
    "        'tip_amount': 4.0,\n",
    "        'tolls_amount': 0.0,\n",
    "        'payment_type': 1,\n",
    "        'congestion_surcharge': 2.5,\n",
    "        'Airport_fee': 0.0,\n",
    "        'cbd_congestion_fee': 0.75,\n",
    "        'trip_duration_minutes': 18,\n",
    "        'pickup_hour': 17,        # 5 PM rush hour\n",
    "        'pickup_day': 'Friday',\n",
    "        'pickup_month': 1\n",
    "    }\n",
    "    \n",
    "    # Long trip - Airport trip\n",
    "    long_trip = {\n",
    "        'passenger_count': 1,\n",
    "        'trip_distance': 12.3,\n",
    "        'extra': 1.0,\n",
    "        'mta_tax': 0.5,\n",
    "        'tip_amount': 6.0,\n",
    "        'tolls_amount': 5.76,     # Bridge toll\n",
    "        'payment_type': 1,\n",
    "        'congestion_surcharge': 2.5,\n",
    "        'Airport_fee': 5.0,       # Airport fee\n",
    "        'cbd_congestion_fee': 0.75,\n",
    "        'trip_duration_minutes': 35,\n",
    "        'pickup_hour': 6,         # 6 AM\n",
    "        'pickup_day': 'Monday',\n",
    "        'pickup_month': 1\n",
    "    }\n",
    "    \n",
    "    return [short_trip, medium_trip, long_trip]\n",
    "\n",
    "def predict_multiple_fares(model, trip_list, scaler=None):\n",
    "    \"\"\"\n",
    "    Predict fares for multiple trips\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    trip_descriptions = ['Short trip', 'Medium trip', 'Long trip']\n",
    "    \n",
    "    print(\"=== MULTIPLE TRIP PREDICTIONS ===\")\n",
    "    \n",
    "    for i, trip in enumerate(trip_list):\n",
    "        predicted_fare = predict_fare(model, trip, scaler)\n",
    "        predictions.append(predicted_fare)\n",
    "        \n",
    "        print(f\"\\n{trip_descriptions[i]}:\")\n",
    "        print(f\"  Distance: {trip['trip_distance']} miles\")\n",
    "        print(f\"  Duration: {trip['trip_duration_minutes']} minutes\") \n",
    "        print(f\"  Time: {trip['pickup_hour']}:00 on {trip['pickup_day']}\")\n",
    "        print(f\"  Passengers: {trip['passenger_count']}\")\n",
    "        print(f\"  🎯 Predicted Fare: ${predicted_fare:.2f}\")\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Interactive prediction function\n",
    "def interactive_prediction(model, scaler=None):\n",
    "    \"\"\"\n",
    "    Interactive function to input trip details and get prediction\n",
    "    \"\"\"\n",
    "    print(\"=== INTERACTIVE FARE PREDICTION ===\")\n",
    "    print(\"Enter trip details:\")\n",
    "    \n",
    "    try:\n",
    "        trip = {}\n",
    "        trip['passenger_count'] = float(input(\"Number of passengers (1-6): \") or 1)\n",
    "        trip['trip_distance'] = float(input(\"Trip distance in miles (e.g., 5.2): \") or 5.0)\n",
    "        trip['extra'] = float(input(\"Extra charges (e.g., 0.5): \") or 0.5)\n",
    "        trip['mta_tax'] = float(input(\"MTA tax (usually 0.5): \") or 0.5)\n",
    "        trip['tip_amount'] = float(input(\"Expected tip amount: \") or 2.0)\n",
    "        trip['tolls_amount'] = float(input(\"Toll charges: \") or 0.0)\n",
    "        trip['payment_type'] = float(input(\"Payment type (1=Credit, 2=Cash): \") or 1)\n",
    "        trip['congestion_surcharge'] = float(input(\"Congestion surcharge: \") or 2.5)\n",
    "        trip['Airport_fee'] = float(input(\"Airport fee: \") or 0.0)\n",
    "        trip['cbd_congestion_fee'] = float(input(\"CBD congestion fee: \") or 0.75)\n",
    "        trip['trip_duration_minutes'] = float(input(\"Trip duration in minutes: \") or 20)\n",
    "        trip['pickup_hour'] = float(input(\"Pickup hour (0-23): \") or 14)\n",
    "        trip['pickup_day'] = input(\"Pickup day (Monday-Sunday): \") or \"Friday\"\n",
    "        trip['pickup_month'] = float(input(\"Pickup month (1-12): \") or 1)\n",
    "        \n",
    "        predicted_fare = predict_fare(model, trip, scaler)\n",
    "        print(f\"\\n🚖 Predicted Fare: ${predicted_fare:.2f}\")\n",
    "        \n",
    "    except ValueError:\n",
    "        print(\"Invalid input. Please enter numeric values where required.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "print(\"Prediction functions ready!\")\n",
    "print(\"\\nUsage examples:\")\n",
    "print(\"1. predict_fare(model, sample_trip, train_dataset.scaler)\")\n",
    "print(\"2. sample_trips = create_sample_trips()\")\n",
    "print(\"   predict_multiple_fares(model, sample_trips, train_dataset.scaler)\")\n",
    "print(\"3. interactive_prediction(model, train_dataset.scaler)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efeaca22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demo function ready!\n",
      "\n",
      "To run a complete demonstration:\n",
      "demo_predictions()\n",
      "\n",
      "============================================================\n",
      "QUICK REFERENCE - How to make predictions:\n",
      "============================================================\n",
      "1. After training: demo_predictions()\n",
      "2. Single trip:    predict_fare(model, trip_dict, train_dataset.scaler)\n",
      "3. Multiple trips: predict_multiple_fares(model, trip_list, train_dataset.scaler)\n",
      "4. Interactive:    interactive_prediction(model, train_dataset.scaler)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Complete Usage Example\n",
    "def demo_predictions():\n",
    "    \"\"\"\n",
    "    Complete demonstration of how to use the model for predictions\n",
    "    \"\"\"\n",
    "    print(\"=== COMPLETE PREDICTION DEMO ===\")\n",
    "    \n",
    "    # Check if model is trained\n",
    "    try:\n",
    "        # Test if model exists and is trained\n",
    "        model.eval()\n",
    "        print(\"✅ Model is ready for predictions!\")\n",
    "        \n",
    "        # Get the scaler from training dataset\n",
    "        scaler = train_dataset.scaler\n",
    "        print(\"✅ Scaler retrieved from training dataset\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"EXAMPLE 1: Single Trip Prediction\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Simple example\n",
    "        example_trip = {\n",
    "            'passenger_count': 2,\n",
    "            'trip_distance': 6.5,\n",
    "            'extra': 0.5,\n",
    "            'mta_tax': 0.5,\n",
    "            'tip_amount': 3.5,\n",
    "            'tolls_amount': 0.0,\n",
    "            'payment_type': 1,\n",
    "            'congestion_surcharge': 2.5,\n",
    "            'Airport_fee': 0.0,\n",
    "            'cbd_congestion_fee': 0.75,\n",
    "            'trip_duration_minutes': 22,\n",
    "            'pickup_hour': 15,       # 3 PM\n",
    "            'pickup_day': 'Wednesday',\n",
    "            'pickup_month': 1\n",
    "        }\n",
    "        \n",
    "        predicted_fare = predict_fare(model, example_trip, scaler)\n",
    "        print(f\"🚖 Sample trip prediction\")\n",
    "        print(f\"   Distance: {example_trip['trip_distance']} miles\")\n",
    "        print(f\"   Duration: {example_trip['trip_duration_minutes']} minutes\")\n",
    "        print(f\"   Passengers: {example_trip['passenger_count']}\")\n",
    "        print(f\"   Predicted Fare: ${predicted_fare:.2f}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"EXAMPLE 2: Multiple Trip Predictions\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Multiple trips\n",
    "        sample_trips = create_sample_trips()\n",
    "        predictions = predict_multiple_fares(model, sample_trips, scaler)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"EXAMPLE 3: Quick Manual Input\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Array-based input (if you know the exact feature order)\n",
    "        # Order: passenger_count, trip_distance, extra, mta_tax, \n",
    "        #        tip_amount, tolls_amount, payment_type, \n",
    "        #        congestion_surcharge, Airport_fee, cbd_congestion_fee, trip_duration_minutes, \n",
    "        #        pickup_hour, pickup_day, pickup_month\n",
    "        \n",
    "        manual_input = [1, 3.2, 0.0, 0.5, 1.8, 0.0, 1, 0.0, 0.0, 0.0, 12, 9, 1, 1]\n",
    "        \n",
    "        predicted_fare_manual = predict_fare(model, manual_input, scaler)\n",
    "        print(f\"Manual array input prediction: ${predicted_fare_manual:.2f}\")\n",
    "        \n",
    "        print(\"\\n✅ All prediction examples completed successfully!\")\n",
    "        \n",
    "    except NameError:\n",
    "        print(\"❌ Model not found. Please train the model first by running:\")\n",
    "        print(\"   train_taxi_fare_model()\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {e}\")\n",
    "        print(\"Make sure the model is trained and datasets are created.\")\n",
    "\n",
    "print(\"Demo function ready!\")\n",
    "print(\"\\nTo run a complete demonstration:\")\n",
    "print(\"demo_predictions()\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"QUICK REFERENCE - How to make predictions:\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. After training: demo_predictions()\")\n",
    "print(\"2. Single trip:    predict_fare(model, trip_dict, train_dataset.scaler)\")  \n",
    "print(\"3. Multiple trips: predict_multiple_fares(model, trip_list, train_dataset.scaler)\")\n",
    "print(\"4. Interactive:    interactive_prediction(model, train_dataset.scaler)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268797dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model saved to 'best_models/best_taxi_fare_model.pth'\n",
      "✅ Scaler and feature order saved\n",
      "✅ Model configuration saved\n",
      "\n",
      "=== MODEL SAVED SUCCESSFULLY ===\n",
      "Files saved:\n",
      "- best_models/best_taxi_fare_model.pth (PyTorch model)\n",
      "- best_models/scaler.pkl (StandardScaler)\n",
      "- best_models/feature_order.pkl (Feature column order)\n",
      "- best_models/model_config.pkl (Model configuration)\n",
      "\n",
      "🚖 Sample prediction: $16.09\n",
      "For a 8.5 mile trip from Times Square to Brooklyn\n",
      "\n",
      "=== TESTING MULTIPLE PREDICTIONS ===\n",
      "=== MULTIPLE TRIP PREDICTIONS ===\n",
      "\n",
      "Short trip:\n",
      "  Distance: 1.2 miles\n",
      "  Duration: 8 minutes\n",
      "  Time: 10:00 on 1\n",
      "  Passengers: 1\n",
      "  🎯 Predicted Fare: $6.53\n",
      "\n",
      "Medium trip:\n",
      "  Distance: 5.8 miles\n",
      "  Duration: 18 minutes\n",
      "  Time: 17:00 on 4\n",
      "  Passengers: 2\n",
      "  🎯 Predicted Fare: $12.39\n",
      "\n",
      "Long trip:\n",
      "  Distance: 12.3 miles\n",
      "  Duration: 35 minutes\n",
      "  Time: 6:00 on 0\n",
      "  Passengers: 1\n",
      "  🎯 Predicted Fare: $16.33\n",
      "\n",
      "Predictions: ['$6.53', '$12.39', '$16.33']\n",
      "\n",
      "🎉 Model training and saving complete!\n",
      "You can now use the saved model for predictions in other applications.\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "import pickle\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Create best_models directory if it doesn't exist\n",
    "os.makedirs('best_models', exist_ok=True)\n",
    "\n",
    "# Save the PyTorch model\n",
    "torch.save(model.state_dict(), 'best_models/best_taxi_fare_model.pth')\n",
    "print(\"✅ Model saved to 'best_models/best_taxi_fare_model.pth'\")\n",
    "\n",
    "# Save the scaler and feature order\n",
    "joblib.dump(train_dataset.scaler, 'best_models/scaler.pkl')\n",
    "joblib.dump(feature_columns, 'best_models/feature_order.pkl')\n",
    "print(\"✅ Scaler and feature order saved\")\n",
    "\n",
    "# Save model configuration\n",
    "model_config = {\n",
    "    'input_size': 14,  # Updated from 17 to 14 features\n",
    "    'hidden_sizes': [128, 64, 32],\n",
    "    'dropout_rate': 0.2,\n",
    "    'final_rmse': 0.88,\n",
    "    'final_mae': 0.59,\n",
    "    'final_r2': 0.9839\n",
    "}\n",
    "\n",
    "with open('best_models/model_config.pkl', 'wb') as f:\n",
    "    pickle.dump(model_config, f)\n",
    "print(\"✅ Model configuration saved\")\n",
    "\n",
    "print(\"\\n=== MODEL SAVED SUCCESSFULLY ===\")\n",
    "print(\"Files saved:\")\n",
    "print(\"- best_models/best_taxi_fare_model.pth (PyTorch model)\")\n",
    "print(\"- best_models/scaler.pkl (StandardScaler)\")\n",
    "print(\"- best_models/feature_order.pkl (Feature column order)\")\n",
    "print(\"- best_models/model_config.pkl (Model configuration)\")\n",
    "\n",
    "# Test prediction with complete sample trip\n",
    "sample_trip = {\n",
    "    'passenger_count': 1,\n",
    "    'trip_distance': 8.5,\n",
    "    'extra': 0.5,\n",
    "    'mta_tax': 0.5,\n",
    "    'tip_amount': 3.0,\n",
    "    'tolls_amount': 0.0,\n",
    "    'payment_type': 1,\n",
    "    'congestion_surcharge': 2.5,\n",
    "    'Airport_fee': 0.0,\n",
    "    'cbd_congestion_fee': 0.75,\n",
    "    'trip_duration_minutes': 25,\n",
    "    'pickup_hour': 14,\n",
    "    'pickup_day': 'Friday',\n",
    "    'pickup_month': 1\n",
    "}\n",
    "\n",
    "# Get prediction\n",
    "predicted_fare = predict_fare(model, sample_trip, train_dataset.scaler)\n",
    "print(f\"\\n🚖 Sample prediction: ${predicted_fare:.2f}\")\n",
    "print(f\"For a {sample_trip['trip_distance']} mile trip\")\n",
    "\n",
    "# Demo of multiple predictions\n",
    "print(\"\\n=== TESTING MULTIPLE PREDICTIONS ===\")\n",
    "sample_trips = create_sample_trips()\n",
    "predictions = predict_multiple_fares(model, sample_trips, train_dataset.scaler)\n",
    "print(f\"\\nPredictions: {[f'${p:.2f}' for p in predictions]}\")\n",
    "\n",
    "print(\"\\n🎉 Model training and saving complete!\")\n",
    "print(\"You can now use the saved model for predictions in other applications.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87474c07",
   "metadata": {},
   "source": [
    "## 🚀 Simplified Model for UI-Based Predictions\n",
    "\n",
    "### Key Changes Made:\n",
    "1. **Removed Location Features**: Eliminated `PULocationID` and `DOLocationID` from training\n",
    "2. **Removed Total Amount**: Excluded `total_amount` as it's the sum of fare + extras\n",
    "3. **Streamlined Features**: Now using only 14 features instead of 17\n",
    "\n",
    "### New Feature Set (14 features):\n",
    "- `passenger_count`, `trip_distance`, `extra`, `mta_tax`, `tip_amount`, `tolls_amount`\n",
    "- `payment_type`, `congestion_surcharge`, `Airport_fee`, `cbd_congestion_fee`\n",
    "- `trip_duration_minutes`, `pickup_hour`, `pickup_day`, `pickup_month`\n",
    "\n",
    "### For UI Integration:\n",
    "- **User Input**: Only pickup and dropoff locations\n",
    "- **Backend Processing**: Calculate distance using precomputed distance matrix\n",
    "- **Model Prediction**: Use distance + estimated features for fare prediction\n",
    "\n",
    "### Benefits:\n",
    "- **Simplified UI**: Users only need to select pickup/dropoff locations\n",
    "- **Better Generalization**: Model focuses on core fare-determining factors\n",
    "- **Easier Deployment**: Fewer features to manage and validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d9977e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🌐 Distance-Based Prediction Function for UI Integration\n",
    "\n",
    "def predict_fare_from_locations(model, pickup_location_id, dropoff_location_id, \n",
    "                               passenger_count=1, pickup_hour=14, pickup_day='Friday', \n",
    "                               pickup_month=1, scaler=None):\n",
    "    \"\"\"\n",
    "    Predict fare using only pickup and dropoff location IDs\n",
    "    \n",
    "    Args:\n",
    "        model: Trained PyTorch model\n",
    "        pickup_location_id: NYC taxi zone ID for pickup\n",
    "        dropoff_location_id: NYC taxi zone ID for dropoff\n",
    "        passenger_count: Number of passengers (default: 1)\n",
    "        pickup_hour: Hour of pickup (0-23, default: 14)\n",
    "        pickup_day: Day of pickup (default: 'Friday')\n",
    "        pickup_month: Month of pickup (1-12, default: 1)\n",
    "        scaler: StandardScaler used during training\n",
    "    \n",
    "    Returns:\n",
    "        Predicted fare amount\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load distance matrix (this should be precomputed)\n",
    "    try:\n",
    "        distance_matrix = pd.read_csv('./distances/full_taxi_zone_distance_matrix.csv', index_col=0)\n",
    "        \n",
    "        # Get distance between pickup and dropoff locations\n",
    "        trip_distance = distance_matrix.loc[pickup_location_id, str(dropoff_location_id)]\n",
    "        \n",
    "        if pd.isna(trip_distance):\n",
    "            # Fallback: use average distance for missing values\n",
    "            trip_distance = 5.0\n",
    "            print(f\"⚠️ Distance not found, using average distance: {trip_distance} miles\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error loading distance matrix: {e}\")\n",
    "        print(\"Using default distance calculation...\")\n",
    "        trip_distance = 5.0  # Default fallback\n",
    "    \n",
    "    # Estimate trip duration based on distance (rough estimate: 2 miles per minute + traffic)\n",
    "    trip_duration_minutes = max(5, int(trip_distance * 2.5))  # Minimum 5 minutes\n",
    "    \n",
    "    # Create feature dictionary with estimated values\n",
    "    trip_features = {\n",
    "        'passenger_count': passenger_count,\n",
    "        'trip_distance': trip_distance,\n",
    "        'extra': 0.5,  # Standard extra charge\n",
    "        'mta_tax': 0.5,  # Standard MTA tax\n",
    "        'tip_amount': max(2.0, trip_distance * 0.3),  # Estimated tip (30% of distance)\n",
    "        'tolls_amount': 0.0,  # Default no tolls (can be enhanced)\n",
    "        'payment_type': 1,  # Default credit card\n",
    "        'congestion_surcharge': 2.5 if pickup_hour >= 6 and pickup_hour <= 20 else 0.0,\n",
    "        'Airport_fee': 5.0 if pickup_location_id in [1, 132, 138] or dropoff_location_id in [1, 132, 138] else 0.0,\n",
    "        'cbd_congestion_fee': 0.75 if pickup_location_id <= 100 or dropoff_location_id <= 100 else 0.0,\n",
    "        'trip_duration_minutes': trip_duration_minutes,\n",
    "        'pickup_hour': pickup_hour,\n",
    "        'pickup_day': pickup_day,\n",
    "        'pickup_month': pickup_month\n",
    "    }\n",
    "    \n",
    "    # Use existing predict_fare function\n",
    "    predicted_fare = predict_fare(model, trip_features, scaler)\n",
    "    \n",
    "    return predicted_fare, trip_features\n",
    "\n",
    "# Example usage function\n",
    "def demo_location_based_prediction():\n",
    "    \"\"\"\n",
    "    Demonstrate location-based fare prediction\n",
    "    \"\"\"\n",
    "    print(\"=== LOCATION-BASED FARE PREDICTION DEMO ===\")\n",
    "    \n",
    "    try:\n",
    "        # Test different location pairs\n",
    "        test_trips = [\n",
    "            (161, 230, \"Times Square to Brooklyn\"),\n",
    "            (1, 132, \"JFK Airport to LGA Airport\"),\n",
    "            (74, 75, \"Local Manhattan trip\"),\n",
    "            (138, 161, \"Queens to Manhattan\"),\n",
    "            (36, 138, \"Brooklyn to Queens\")\n",
    "        ]\n",
    "        \n",
    "        print(\"\\\\nTesting different location pairs:\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for pickup_id, dropoff_id, description in test_trips:\n",
    "            try:\n",
    "                fare, features = predict_fare_from_locations(\n",
    "                    model, pickup_id, dropoff_id, \n",
    "                    passenger_count=1, pickup_hour=14, \n",
    "                    scaler=train_dataset.scaler\n",
    "                )\n",
    "                \n",
    "                print(f\"\\\\n🚖 {description}\")\n",
    "                print(f\"   Pickup Zone: {pickup_id}, Dropoff Zone: {dropoff_id}\")\n",
    "                print(f\"   Distance: {features['trip_distance']:.1f} miles\")\n",
    "                print(f\"   Duration: {features['trip_duration_minutes']} minutes\")\n",
    "                print(f\"   Predicted Fare: ${fare:.2f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error predicting fare for {description}: {e}\")\n",
    "        \n",
    "        print(\"\\\\n✅ Location-based prediction demo completed!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in demo: {e}\")\n",
    "        print(\"Make sure the model is trained and distance matrix is available.\")\n",
    "\n",
    "print(\"🌐 Distance-based prediction function ready!\")\n",
    "print(\"\\\\nUsage:\")\n",
    "print(\"1. predict_fare_from_locations(model, pickup_id, dropoff_id, scaler=train_dataset.scaler)\")\n",
    "print(\"2. demo_location_based_prediction()\")\n",
    "print(\"\\\\nThis function is perfect for UI integration where users only select pickup/dropoff locations!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taxi_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
